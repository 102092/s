{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decisoin Tree\n",
    "\n",
    "* 문제\n",
    "* 알고리즘 정의\n",
    "* 구현\n",
    "    * 공개된 데이터\n",
    "    * 함수로 구현 -> 라이브러리 (비객체지향)\n",
    "* 성능\n",
    "    * 정확성, 빠르기\n",
    "* 사례\n",
    "    * house-votes-84.data 데이터 이용\n",
    "    * 패키지 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 문제\n",
    "\n",
    "* 의사결정에 영향을 미치는 속성을 값에 따라 분기하여, 최종 결정에 이르도록 한다.\n",
    "* 사다리타기와 같이 최종 결정이 명목 값을 가진다.\n",
    "* 속성 값이 연속적이지 아닌 경우 사용한다.\n",
    "* 몸무게와 같이 연속적인 값이 아닌 결정일 경우 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 알고리즘\n",
    "\n",
    "### 4.2.1 ID3 algorithm\n",
    "\n",
    "* input: Training Data, Attribute List\n",
    "  * 학습데이터\n",
    "  * 속성목록\n",
    "* output: decision tree\n",
    "\n",
    "Generate_decision_tree(Training Data, Attribute List)\n",
    "* create a node N 시작 노드\n",
    "* if samples are all of the same class C then\n",
    "    return N as a leaf node labeled with class C; 모두 같은 클래스인 경우 분기하여 노드\n",
    "* if attribute-list is empty then\n",
    "    return N as a leaft node labeled with majority voting; 분기할 속성이 남아 있지 않으면 최빈으로 노드\n",
    "* from among attributes in the attribute-list,\n",
    "    select test attribute that leads to the highest information gain\n",
    "    label node N with the test-attribute; IG가 가장 높은 속성을 선택하여 노드\n",
    "* for each known value ai of test-attribute //partition the samples\n",
    "    * grow a branch from node N for the condition test-attribute = ai 분기\n",
    "    * let si be the set of samples in the samples for which test-attribute = ai 데이터분할\n",
    "    * if si is empty then\n",
    "        attach a leaf labelled with majority voting; 분할한 데이터가 공집합이면 최빈\n",
    "    * else\n",
    "        attach the node returned by Generate_decision_tree(si, Attribute List - test attribute); 재귀적으로 함수를 실행하고 그 결과 값을 받아서 노드\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Shannon entorpy\n",
    "\n",
    "* 정보이론 (Shannon, 1948)\n",
    "* Entropy H(S)는 데이터에 포함된 불확실성, 엔트로피가 클수록 불확실성이 높음.\n",
    "* log2를 취하면서 그 승수가 정보를 표현하는 bit수를 의미.\n",
    "즉 4의 log2 값은 2 ($2^2$), 4개의 정보 값을 표현하려면 2 bits 필요.\n",
    "\n",
    "* 참고\n",
    "    * scipy.stats.entropy\n",
    "    * Gini impurity\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(S) &= - \\sum_{i=1}^{n}\\ p_i\\ log_2\\ p_i\\\\\n",
    "     &= - p_1 log_2\\ p_1 - p_2 log_2\\ p_2\\ \\ldots\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "위 식에서 $p_i$는 사건i가 발생할 확률을 의미.\n",
    "동전을 던지는 경우, 모두 앞면 또는 모두 뒷면은 entropy가 0 (즉 불확실성이 없는 purity)\n",
    "반대로 반반씩 섞여 있는 경우 entropy는 가장 크다.\n",
    "즉 엔트로피가 클수록 정보가 혼재되어 있다는 의미.\n",
    "가장 유용한 정보는 엔트로피를 가장 많이 감소시키는 것.\n",
    "\n",
    "```\n",
    "H( (0.5,0.5) ) = -0.5 x log_2 0.5 - 0.5 x log_2 0.5 = 1\n",
    "H( (0.5,0.5) ) = -0.7 x log_2 0.7 - 0.3 x log_2 0.3 = 0.88\n",
    "H( (0.5,0.5) ) = -1.0 x log_2 1.0 - 0.0 x log_2 0.0 = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5]\n",
      "[0.3602012209808308, 0.5210896782498619]\n",
      "math domain error\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print [-p*math.log(p,2) for p in [0.5,0.5]]\n",
    "print [-p*math.log(p,2) for p in [0.7,0.3]]\n",
    "try:\n",
    "    [-p*math.log(p,2) for p in [1.0,0.0]]\n",
    "except Exception as e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 엔트로피 계산 - 문자열\n",
    "    * 엔트로피는 부호화에 필요한 문자 당 최소 평균 이진값\n",
    "        ```\n",
    "        str = 'aabcddddefffg'\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'a'\tFreq->2\tProb->0.153846153846\teach entropy->0.415452264329 Entropy0.415452264329\n",
      "'c'\tFreq->1\tProb->0.0769230769231\teach entropy->0.284649209088 Entropy0.700101473417\n",
      "'b'\tFreq->1\tProb->0.0769230769231\teach entropy->0.284649209088 Entropy0.984750682505\n",
      "'e'\tFreq->1\tProb->0.0769230769231\teach entropy->0.284649209088 Entropy1.26939989159\n",
      "'d'\tFreq->4\tProb->0.307692307692\teach entropy->0.523212220966 Entropy1.79261211256\n",
      "'g'\tFreq->1\tProb->0.0769230769231\teach entropy->0.284649209088 Entropy2.07726132165\n",
      "'f'\tFreq->3\tProb->0.230769230769\teach entropy->0.488187050174 Entropy2.56544837182\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "str = 'aabcddddefffg'\n",
    "# 1) counts frequency\n",
    "allChars=list(str)\n",
    "charDict=dict()\n",
    "for token in allChars:\n",
    "    if charDict.has_key(token):\n",
    "        charDict[token]+=1\n",
    "    else:\n",
    "        charDict[token]=1\n",
    "# 2) computes entropy\n",
    "entropy=0\n",
    "allFreq=float(len(allChars))\n",
    "for key in charDict.iterkeys():\n",
    "    freq=charDict[key]\n",
    "    prob=float(freq)/allFreq\n",
    "    ent=-prob * math.log(prob,2)\n",
    "    entropy=entropy+ent\n",
    "    print \"'{0}'\\tFreq->{1}\\tProb->{2}\\teach entropy->{3} Entropy{4}\".format(key,freq,prob,ent,entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 엔트로피 계산\n",
    "    * 5건에서 yes 2건, no 3건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case:\tyes=2/5 no=3/5 (MiA p.42)\n",
      "\tcomputing entropy --> 0.97095059\n",
      "\tentropy=[0.52877123795494485, 0.44217935649972373]\n",
      "\tfunct=0.970950594455\n"
     ]
    }
   ],
   "source": [
    "print \"case:\\tyes=2/5 no=3/5 (MiA p.42)\"\n",
    "print \"\\tcomputing entropy --> 0.97095059\"\n",
    "S=np.array((0.4, 0.6))\n",
    "entropy=[ -p*math.log(p,2) for p in S ]\n",
    "def computeEntropy(data):\n",
    "    entropy=[-p*math.log(p,2) for p in data]\n",
    "    return sum(entropy)\n",
    "print \"\\tentropy={0}\\n\\tfunct={1}\".format(entropy,computeEntropy(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 information gain\n",
    "\n",
    "* 불확실성이 가장 낮은 속성으로 분기하기 위한 방법. 가장 높은 IG를 선택하여 분기함.\n",
    "* 속성선정값 (attribute selection measure) 또는 분기적합도 (measure of the goodness of split)\n",
    "\n",
    "$$\n",
    "IG(A,S) = H(S) - \\sum_{i=1}^m\\ p(i)\\ H(i)\n",
    "$$\n",
    "\n",
    "* 각 속성의 값으로, 그 발생확률을 계산 (위 식 $p(i)$)\n",
    "* 그 속성 값의 엔트로피를 계산 (H(i)\n",
    "* 위 1과 2를 가중하여 해당 속성의 엔트로피를 계산 (위 식의 우측 항)\n",
    "* 기본엔트로피 (H(s))에서 공제하여 Information Gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 구현\n",
    "\n",
    "1. 데이터준비\n",
    "2. create tree\n",
    "    * 엔트로피 계산\n",
    "    * Information Gain 계산\n",
    "    * decision tree 구조 만듦\n",
    "3. classify\n",
    "4. 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 데이터 준비\n",
    "\n",
    "* 사례\n",
    "    * source: Liu, Bing. Web data mining: exploring hyperlinks, contents, and usage data. Springer Science & Business Media, 2007.\n",
    "    * 속성 4개 열, 속성 값은 문자열\n",
    "    * 클래스 마지막 열\n",
    "    * 테이블로 나타내면:\n",
    "\n",
    "| age | has_job | own_house | credit_rating | class |\n",
    "|-----|---------|-----------|---------------|-------|\n",
    "| young | false | false | fair | no |\n",
    "\n",
    "* numpy로 계산 (List로 처리하는 것과 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['young' 'false' 'false' 'fair' 'No']\n",
      " ['young' 'false' 'false' 'good' 'No']\n",
      " ['young' 'true' 'false' 'good' 'Yes']\n",
      " ['young' 'true' 'true' 'fair' 'Yes']\n",
      " ['young' 'false' 'false' 'fair' 'No']\n",
      " ['middle' 'false' 'false' 'fair' 'No']\n",
      " ['middle' 'false' 'false' 'good' 'No']\n",
      " ['middle' 'true' 'true' 'good' 'Yes']\n",
      " ['middle' 'false' 'true' 'excellent' 'Yes']\n",
      " ['middle' 'false' 'true' 'excellent' 'Yes']\n",
      " ['old' 'false' 'true' 'excellent' 'Yes']\n",
      " ['old' 'false' 'true' 'good' 'Yes']\n",
      " ['old' 'true' 'false' 'good' 'Yes']\n",
      " ['old' 'true' 'false' 'excellent' 'Yes']\n",
      " ['old' 'false' 'false' 'fair' 'No']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "myV=np.array([['young', 'false', 'false', 'fair', 'No'],\n",
    "           ['young', 'false', 'false', 'good', 'No'],\n",
    "           ['young', 'true', 'false', 'good', 'Yes'],\n",
    "           ['young', 'true', 'true', 'fair', 'Yes'],\n",
    "           ['young', 'false', 'false', 'fair', 'No'],\n",
    "           ['middle', 'false', 'false', 'fair', 'No'],\n",
    "           ['middle', 'false', 'false', 'good', 'No'],\n",
    "           ['middle', 'true', 'true', 'good', 'Yes'],\n",
    "           ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "           ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "           ['old', 'false', 'true', 'excellent', 'Yes'],\n",
    "           ['old', 'false', 'true', 'good', 'Yes'],\n",
    "           ['old', 'true', 'false', 'good', 'Yes'],\n",
    "           ['old', 'true', 'false', 'excellent', 'Yes'],\n",
    "           ['old', 'false', 'false', 'fair', 'No']], \n",
    "          dtype='|S9')\n",
    "\n",
    "print myV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 엔트로피 계산\n",
    "\n",
    "* 속성의 키를 찾아, 빈도를 계산\n",
    "    * numpy bincount\n",
    "        ```\n",
    "        buildKeyCountVec(data)\n",
    "        ```\n",
    "    * 간편하게 collections을 이용할 수 있슴.\n",
    "        ```\n",
    "        from collections import Counter\n",
    "        kc = Counter(data)\n",
    "        ```\n",
    "* 확률 계산\n",
    "    ```\n",
    "    computeProbVec(kc)\n",
    "    ```\n",
    "\n",
    "* 엔트로피 계산\n",
    "    ```\n",
    "    entropyVec(data)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이['middle' 'old' 'young'] 빈도[5 5 5]\n"
     ]
    }
   ],
   "source": [
    "#학습데이터 첫째 속성(나이)의 엔트로피계산을 위한 확률 구함.\n",
    "#키를 찾아, 빈도를 계산\n",
    "a=myV[:,0]\n",
    "keys=np.unique(a)\n",
    "bins=keys.searchsorted(myV[:,0])\n",
    "freq=np.bincount(bins)\n",
    "print \"나이{0} 빈도{1}\".format(keys,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫째 속성의 키/빈도수\n",
      "[['middle' 'old' 'young']\n",
      " ['5' '5' '5']]\n"
     ]
    }
   ],
   "source": [
    "def buildKeyCountVec(data):\n",
    "    import numpy as np\n",
    "    keys=np.unique(data)\n",
    "    bins=keys.searchsorted(data)\n",
    "    return np.vstack([keys,np.bincount(bins)])\n",
    "\n",
    "myX=[2,2,8,7,5,3,1,1,2,2,8,7,5,3,9,7]\n",
    "buildKeyCountVec(myX)\n",
    "print \"첫째 속성의 키/빈도수\\n{0}\".format(buildKeyCountVec(myV[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마지막 컬럼 클래스의 키/빈도수\n",
      "[['No' 'Yes']\n",
      " ['6' '9']]\n",
      "확률 [ 0.4  0.6] 전체빈도 15\n",
      "엔트로피는 0.970950594455\n"
     ]
    }
   ],
   "source": [
    "#전체 데이터에 대해 엔트로피를 계산하면\n",
    "#1) 확률 계산\n",
    "#Yes: 9/15 No: 6/15\n",
    "print \"마지막 컬럼 클래스의 키/빈도수\\n{0}\".format(findKeyCounts(myV[:,-1]))\n",
    "\n",
    "#전체빈도수 계산.\n",
    "kcV=buildKeyCountVec(myV[:,-1])\n",
    "#array는 문자열이 섞이면 수도 문자열로 casting.\n",
    "allFreq=kcV[1,:].astype('int').sum()\n",
    "#확률계산 - vector연산이므로 for-loop가 필요없슴.\n",
    "prob=kcV[1,:].astype('float')/allFreq\n",
    "print \"확률 {0} 전체빈도 {1}\".format(prob,allFreq)\n",
    "\n",
    "#2) 엔트로피를 계산\n",
    "print \"엔트로피는 {0}\".format(sum([-p*math.log(p,2) for p in prob]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[from computeProbVec] prob [ 0.4  0.6] all freq 15\n",
      "함수를 이용한 엔트로피는 0.970950594455\n"
     ]
    }
   ],
   "source": [
    "def computeProbVec(kc):\n",
    "    allFreq=kc[1,:].astype('int').sum()\n",
    "    prob=kc[1,:].astype('float')/allFreq\n",
    "    print \"[from computeProbVec] prob {0} all freq {1}\".format(prob,allFreq)\n",
    "    return prob\n",
    "\n",
    "def entropyVec(data):\n",
    "    import math\n",
    "    kc=buildKeyCountVec(data)\n",
    "    prob=computeProbVec(kc)\n",
    "    entropy=computeEntropy(prob)\n",
    "    return entropy\n",
    "\n",
    "print \"함수를 이용한 엔트로피는 {0}\".format(entropyVec(myV[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Information Gain 계산\n",
    "\n",
    "* age 속성으로 info gain을 손으로 계산해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['No' 'Yes']\n",
      " ['3' '2']]\n",
      "[['No' 'Yes']\n",
      " ['2' '3']]\n",
      "[['No' 'Yes']\n",
      " ['1' '4']]\n",
      "Young Entropy 0.323650198152 = 0.333333333333  x  0.970950594455\n",
      "Middle Entropy 0.323650198152 = 0.333333333333  x  0.970950594455\n",
      "Old Entropy 0.240642698296 = 0.333333333333  x  0.721928094887\n",
      "Age Entropy: 0.887943094599 = 0.323650198152 + 0.323650198152 + 0.240642698296\n",
      "Class Entropy: 0.970950594455\n",
      "age info gain: 0.0830074998558 = 0.970950594455 - 0.887943094599\n"
     ]
    }
   ],
   "source": [
    "# select best feature\n",
    "# 1) age --> 0.888\n",
    "#H(age)=5/15 * H(age=young) + 5/15 * H(age=middle) + 5/15 * H(age=old)\n",
    "# 1-1) select 'young'\n",
    "#array([['young', 'false', 'false', 'fair', 'No'],\n",
    "#       ['young', 'false', 'false', 'good', 'No'],\n",
    "#       ['young', 'true', 'false', 'good', 'Yes'],\n",
    "#       ['young', 'true', 'true', 'fair', 'Yes'],\n",
    "#       ['young', 'false', 'false', 'fair', 'No']])\n",
    "subV=myV[myV[:,0]=='young']\n",
    "print buildKeyCountVec(subV[:,-1])\n",
    "#array([['No', 'Yes'],['3', '2']])\n",
    "freqYoung=len(subV)\n",
    "entYoung=sum([-p*math.log(p,2) for p in (3/5.,2/5.)])\n",
    "#0.9709505944546686\n",
    "# 1-2) select 'middle'\n",
    "#array([['middle', 'false', 'false', 'fair', 'No'],\n",
    "#       ['middle', 'false', 'false', 'good', 'No'],\n",
    "#       ['middle', 'true', 'true', 'good', 'Yes'],\n",
    "#       ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "#       ['middle', 'false', 'true', 'excellent', 'Yes']]) \n",
    "subV=myV[myV[:,0]=='middle']\n",
    "print buildKeyCountVec(subV[:,-1])\n",
    "#array([['No', 'Yes'],['2', '3']]) \n",
    "freqMiddle=len(subV)\n",
    "entMiddle=sum([-p*math.log(p,2) for p in (2/5.,3/5.)])\n",
    "#0.9709505944546686\n",
    "# 1-3) select 'old'\n",
    "subV=myV[myV[:,0]=='old']\n",
    "print buildKeyCountVec(subV[:,-1])\n",
    "#array([['No', 'Yes'],['1', '4']])\n",
    "freqOld=len(subV)\n",
    "entOld=sum([-p*math.log(p,2) for p in (1/5.,4/5.)])\n",
    "#0.7219280948873623\n",
    "# 2) compute prob and entropy\n",
    "probYoung=float(freqYoung)/len(myV)\n",
    "entPYoung=probYoung*entYoung\n",
    "print \"Young Entropy\",entPYoung,\"=\",probYoung,\" x \",entYoung\n",
    "entAge=entPYoung\n",
    "\n",
    "probMiddle=float(freqMiddle)/len(myV)\n",
    "entPMiddle=probMiddle*entMiddle\n",
    "print \"Middle Entropy\",entPMiddle,\"=\",probMiddle,\" x \",entMiddle\n",
    "entAge+=entPMiddle\n",
    "\n",
    "probOld=float(freqOld)/len(myV)\n",
    "entPOld=probOld*entOld\n",
    "print \"Old Entropy\",entPOld,\"=\",probOld,\" x \",entOld\n",
    "entAge+=entPOld\n",
    "\n",
    "print \"Age Entropy:\",entAge,\"=\",entPYoung,\"+\",entPMiddle,\"+\",entPOld\n",
    "baseEntropy=sum([-p*math.log(p,2) for p in (9/15.,6/15.)])\n",
    "# 3) IG\n",
    "print \"Class Entropy:\",baseEntropy\n",
    "print \"age info gain:\",baseEntropy-entAge,\"=\",baseEntropy,\"-\",entAge\n",
    "# 4) pick the best\n",
    "#IG(D,age) = 0.971 - 0.888 = 0.083\n",
    "#IG(D,own_house) = 0.971 - 0.551 = 0.420\n",
    "#IG(D,has_job) = 0.971 - 0.647 = 0.324\n",
    "#IG(D,credit_rating) = 0.971 - 0.608 = 0.363\n",
    "#pick own_house as the largest IG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gain(D, Age) = 0.971 - 0.888 = 0.083\n",
    "gain(D, Has_job) = 0.971 - 0.647 = 0.324\n",
    "gain(D, Own_house) = 0.971 - 0.551 = 0.420\n",
    "gain(D, Credit_rating) = 0.971 - 0.608 = 0.363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* information gain 함수\n",
    "    * feature별로 계산한다.\n",
    "    * best feature를 고른다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[from computeProbVec] prob [ 0.4  0.6] all freq 15\n",
      "\tage p=[ 0.33333333  0.33333333  0.33333333] allFreq=15\n",
      "[from computeProbVec] prob [ 0.4  0.6] all freq 5\n",
      "keyToSearch=middle Entropy=[ 0.3236502  0.         0.         0.       ]\n",
      "[from computeProbVec] prob [ 0.2  0.8] all freq 5\n",
      "keyToSearch=old Entropy=[ 0.5642929  0.         0.         0.       ]\n",
      "[from computeProbVec] prob [ 0.6  0.4] all freq 5\n",
      "keyToSearch=young Entropy=[ 0.88794309  0.          0.          0.        ]\n",
      "\tage info gain=[ 0.0830075  0.         0.         0.       ]\n"
     ]
    }
   ],
   "source": [
    "baseEntropy=entropyVec(myV[:,-1]) # class labels\n",
    "nFeatures=myV.shape[1]-1 # n of features except the class label\n",
    "igV=np.zeros([nFeatures]) # empty array of features\n",
    "aFeature=0 # age -> the first feature\n",
    "kcV=buildKeyCountVec(myV[:,aFeature])\n",
    "nKeys=kcV.shape[1] # 3 keys -> old,middle,young\n",
    "allFreq=kcV[1,:].astype('int').sum() #n=15\n",
    "prob=kcV[1,:].astype('float')/allFreq # [5/15,5/15,5/15]\n",
    "print \"\\tage p={0} allFreq={1}\".format(prob,allFreq)\n",
    "for aKey in range(nKeys):\n",
    "    keyToSearch=kcV[0][aKey]\n",
    "    subData=myV[myV[:,aFeature]==keyToSearch]\n",
    "    igV[aFeature]+=prob[aKey]*entropyVec(subData[:,-1])\n",
    "    print \"keyToSearch={0} Entropy={1}\".format(keyToSearch,igV)\n",
    "igV[aFeature]=baseEntropy-igV[aFeature]\n",
    "print \"\\tage info gain={0}\".format(igV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[from computeProbVec] prob [ 0.4  0.6] all freq 15\n",
      "[from computeProbVec] prob [ 0.33333333  0.33333333  0.33333333] all freq 15\n",
      "[from computeProbVec] prob [ 0.4  0.6] all freq 5\n",
      "[from computeProbVec] prob [ 0.2  0.8] all freq 5\n",
      "[from computeProbVec] prob [ 0.6  0.4] all freq 5\n",
      "[getInfoGainVec] 0th InfoGain=[ 0.0830075  0.         0.         0.       ]\n",
      "[from computeProbVec] prob [ 0.4  0.6] all freq 15\n",
      "[from computeProbVec] prob [ 0.66666667  0.33333333] all freq 15\n",
      "[from computeProbVec] prob [ 0.6  0.4] all freq 10\n",
      "[from computeProbVec] prob [ 1.] all freq 5\n",
      "[getInfoGainVec] 1th InfoGain=[ 0.0830075  0.3236502  0.         0.       ]\n",
      "[from computeProbVec] prob [ 0.4  0.6] all freq 15\n",
      "[from computeProbVec] prob [ 0.6  0.4] all freq 15\n",
      "[from computeProbVec] prob [ 0.66666667  0.33333333] all freq 9\n",
      "[from computeProbVec] prob [ 1.] all freq 6\n",
      "[getInfoGainVec] 2th InfoGain=[ 0.0830075   0.3236502   0.41997309  0.        ]\n",
      "[from computeProbVec] prob [ 0.4  0.6] all freq 15\n",
      "[from computeProbVec] prob [ 0.26666667  0.33333333  0.4       ] all freq 15\n",
      "[from computeProbVec] prob [ 1.] all freq 4\n",
      "[from computeProbVec] prob [ 0.8  0.2] all freq 5\n",
      "[from computeProbVec] prob [ 0.33333333  0.66666667] all freq 6\n",
      "[getInfoGainVec] 3th InfoGain=[ 0.0830075   0.3236502   0.41997309  0.36298956]\n",
      "[ 0.0830075   0.3236502   0.41997309  0.36298956]\n"
     ]
    }
   ],
   "source": [
    "def getInfoGainVec(data):\n",
    "    import numpy as np\n",
    "    nFeature=data.shape[1]-1 # except the last column\n",
    "    InfoGain=np.zeros([nFeature]) # by feature\n",
    "    for item in range(nFeature):\n",
    "        InfoGain[item]=computeInfoGain(data[:,[item,-1]])\n",
    "        print \"[getInfoGainVec] {0}th InfoGain={1}\".format(item,InfoGain)\n",
    "    return InfoGain\n",
    "\n",
    "def computeInfoGain(data):\n",
    "    di=0 # the first column = data column\n",
    "    classEntropy=entropyVec(data[:,-1]) # class labels\n",
    "    kc=buildKeyCountVec(data[:,di])\n",
    "    nKey=kc.shape[1] #n of unique keys\n",
    "    prob=computeProbVec(kc)\n",
    "    rawInfoGain=0.\n",
    "    for item in range(nKey):\n",
    "        keyToSearch=kc[0][item]\n",
    "        subData=split2DVec(data,di,keyToSearch)\n",
    "        classEntropyByItem=entropyVec(subData[:,-1])\n",
    "        rawInfoGain+=prob[item]*classEntropyByItem\n",
    "    InfoGain=classEntropy-rawInfoGain\n",
    "    return InfoGain\n",
    "\n",
    "def split2DVec(data,col,query):\n",
    "    subData=data[data[:,col]==query]\n",
    "    return subData\n",
    "\n",
    "print getInfoGainVec(myV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['young', 'young', 'young', 'young', 'young', 'middle', 'middle', 'middle', 'middle', 'middle', 'old', 'old', 'old', 'old', 'old'], ['false', 'false', 'true', 'true', 'false', 'false', 'false', 'true', 'false', 'false', 'false', 'false', 'true', 'true', 'false'], ['false', 'false', 'false', 'true', 'false', 'false', 'false', 'true', 'true', 'true', 'true', 'true', 'false', 'false', 'false'], ['fair', 'good', 'good', 'fair', 'fair', 'fair', 'good', 'good', 'excellent', 'excellent', 'excellent', 'good', 'good', 'excellent', 'fair'], ['No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']]\n"
     ]
    }
   ],
   "source": [
    "# %load -r 747-752 src/pystat/sfun.py\n",
    "def swapColRow(data):\n",
    "    dataCol=list()\n",
    "    for c in range(len(data[0])):\n",
    "        col=[eachRow[c] for eachRow in data]\n",
    "        dataCol.append(col)\n",
    "    return dataCol\n",
    "\n",
    "def splitListByRow(data,row,query):\n",
    "    subData=list()\n",
    "    index=list()\n",
    "    for i,v in enumerate(data[row]):\n",
    "        if v==query:\n",
    "            index.append(i)\n",
    "    for j in range(len(data)):\n",
    "        subDataByRow=list()\n",
    "        for i in index:\n",
    "            subDataByRow.append(data[j][i])\n",
    "        subData.append(subDataByRow)\n",
    "    return subData\n",
    "\n",
    "\n",
    "def createTree(keyList,n=0):\n",
    "    from collections import defaultdict\n",
    "    nStop=len(keyList)-1\n",
    "    if n==nStop:\n",
    "        cLables=keyList[-1]\n",
    "        return cLables\n",
    "    # d grows dynamically\n",
    "    d=defaultdict()\n",
    "    parentKeyList=set(keyList[n])\n",
    "    for key in parentKeyList:\n",
    "        subKeyList=splitListByRow(keyList,n,key)\n",
    "        d.update({key:createTree(subKeyList,n+1)})\n",
    "    return d\n",
    "\n",
    "myL=myV.tolist()\n",
    "myLCol=swapColRow(myL)\n",
    "myTree=createTree(myLCol)\n",
    "print myLCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ 1 middle\n",
      "++ 2 false\n",
      "+++ 3 false\n",
      "++++ 4 good: ['No']\n",
      "++++ 4 fair: ['No']\n",
      "+++ 3 true\n",
      "++++ 4 excellent: ['Yes', 'Yes']\n",
      "++ 2 true\n",
      "+++ 3 true\n",
      "++++ 4 good: ['Yes']\n",
      "+ 1 old\n",
      "++ 2 false\n",
      "+++ 3 true\n",
      "++++ 4 good: ['Yes']\n",
      "++++ 4 excellent: ['Yes']\n",
      "+++ 3 false\n",
      "++++ 4 fair: ['No']\n",
      "++ 2 true\n",
      "+++ 3 false\n",
      "++++ 4 good: ['Yes']\n",
      "++++ 4 excellent: ['Yes']\n",
      "+ 1 young\n",
      "++ 2 false\n",
      "+++ 3 false\n",
      "++++ 4 good: ['No']\n",
      "++++ 4 fair: ['No', 'No']\n",
      "++ 2 true\n",
      "+++ 3 false\n",
      "++++ 4 good: ['Yes']\n",
      "+++ 3 true\n",
      "++++ 4 fair: ['Yes']\n"
     ]
    }
   ],
   "source": [
    "# %load -r 755-764 src/pystat/sfun.py\n",
    "def dPrint(d,depth=0):\n",
    "    depth+=1\n",
    "    for k, v in d.iteritems():\n",
    "        print \"+\"*depth,\n",
    "        if isinstance(v, dict):\n",
    "            print \"{0} {1}\".format(depth,k)\n",
    "            dPrint(v,depth)\n",
    "        else:\n",
    "            print \"{0} {1}: {2}\".format(depth, k, v)\n",
    "\n",
    "dPrint(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 분류\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print myTree['middle']['false']['false']['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.4 사례\n",
    "\n",
    "### 4.4.1 scikit ML 패키지를 사용하여:\n",
    "\n",
    "* see\n",
    "http://www.netinstructions.com/machine-learning-with-decision-trees/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 교재\n",
    "\n",
    "* MiA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy= 0.970950594455\n",
      "{'yes': 1}\n",
      "0\n",
      "['flippers']\n",
      "False\n",
      "testVec=[1, 0]을 분류하면 결과 class label=no\n",
      "{'No': {'middle': {'Yes': {'false': {'No': {'good': 'good', 'fair': 'fair'}}, 'true': {'No': {'true': 'good', 'false': 'excellent'}}}}, 'old': {'No': {'false': {'Yes': {'true': 'good', 'false': 'fair'}}, 'true': {'No': {'good': 'good', 'excellent': 'excellent'}}}}, 'young': {'Yes': {'false': {'No': {'false': 'fair', 'true': 'good'}}, 'true': 'fair'}}}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "myhome=os.path.expanduser('~')\n",
    "ch3wd=os.path.join(myhome,'Code/git/else/machinelearninginaction/Ch03')\n",
    "os.chdir(ch3wd)\n",
    "import trees\n",
    "myDat,labels=trees.createDataSet()\n",
    "#myDat [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
    "#labels ['no surfacing', 'flippers']\n",
    "print \"entropy=\",trees.calcShannonEnt(myDat) #0.9709505944546686\n",
    "\n",
    "#inside calcShannonEnt: 학습데이터 1개 행의 클래스의 빈도 계산\n",
    "numEntries=len(myDat)\n",
    "labelCounts={}\n",
    "currentLabel=myDat[0][-1] #[1, 1, 'yes']의 마지막 'yes'\n",
    "if currentLabel not in labelCounts.keys():\n",
    "    labelCounts[currentLabel]=0\n",
    "labelCounts[currentLabel]+=1\n",
    "print labelCounts #{'yes': 1}\n",
    "\n",
    "print trees.chooseBestFeatureToSplit(myDat)\n",
    "\n",
    "# create tree as a dictionary and search\n",
    "#createTree p.48\n",
    "myTree=trees.createTree(myDat,labels)\n",
    "#{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
    "print labels #createTree하면 labels lost ['flippers'], so reload!!!\n",
    "\n",
    "#insdie classify p.56\n",
    "myDat,labels=trees.createDataSet() #now labels ['no surfacing', 'flippers']\n",
    "firstStr=myTree.keys()[0] #'no surfacing'\n",
    "secondDict=myTree[firstStr] #{0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}\n",
    "featIndex=labels.index(firstStr) #0\n",
    "secondDict.keys() #[0, 1]\n",
    "secondDict #{0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}\n",
    "testVec=[1,0]\n",
    "key=secondDict.keys()[0] #0\n",
    "print testVec[featIndex]==key #False\n",
    "print \"testVec={0}을 분류하면 결과 class label={1}\".format(testVec,secondDict[0])\n",
    "\n",
    "# sample data로\n",
    "myV=np.array([['young', 'false', 'false', 'fair', 'No'],\n",
    "       ['young', 'false', 'false', 'good', 'No'],\n",
    "       ['young', 'true', 'false', 'good', 'Yes'],\n",
    "       ['young', 'true', 'true', 'fair', 'Yes'],\n",
    "       ['young', 'false', 'false', 'fair', 'No'],\n",
    "       ['middle', 'false', 'false', 'fair', 'No'],\n",
    "       ['middle', 'false', 'false', 'good', 'No'],\n",
    "       ['middle', 'true', 'true', 'good', 'Yes'],\n",
    "       ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "       ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "       ['old', 'false', 'true', 'excellent', 'Yes'],\n",
    "       ['old', 'false', 'true', 'good', 'Yes'],\n",
    "       ['old', 'true', 'false', 'good', 'Yes'],\n",
    "       ['old', 'true', 'false', 'excellent', 'Yes'],\n",
    "       ['old', 'false', 'false', 'fair', 'No']], \n",
    "      dtype='|S9')\n",
    "dat=myV[:,:-1].tolist()\n",
    "lab=myV[:,-1].tolist()\n",
    "print trees.createTree(dat,lab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
