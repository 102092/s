{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "* Last updated 20161125 20170221\n",
    "\n",
    "## 목적\n",
    "\n",
    "* Spark를 사용하여 빅데이터를 ETL 할 수 있다.\n",
    "* Spark를 사용하여 분석을 할 수 있다.\n",
    "\n",
    "## 문제\n",
    "\n",
    "* 문제 S-1: Spark를 standalone cluster로 구성하기\n",
    "* 문제 S-2: Hello Spark - 환경설정을 읽어 클라이언트 sc를 생성하기.\n",
    "* 문제 S-3: RDD를 사용하여 MLlib의 입력 데이터 word vector 생성하기.\n",
    "* 문제 S-4: RDD를 사용하여 MLLib의 입력 데이터 feature vector 생성하기.\n",
    "* 문제 S-5: 파일에서 Spark SQL로 데이터 읽기\n",
    "* 문제 S-6: spark sql uber csv\n",
    "* 문제 S-7: Kolmogorov-Smirnov 검증\n",
    "* 문제 S-8: 무작위 데이터 생성\n",
    "* 문제 S-9: 정량데이터 분석\n",
    "* 문제 S-10: 텍스트 분석\n",
    "* 문제 S-11: twitter 데이터 분석\n",
    "* 문제 S-12: 그래프 분석\n",
    "* 문제 S-13: spark-submit\n",
    "* 문제 S-14: 시각화 Bokeh\n",
    "http://www.blog.pythonlibrary.org/2016/07/27/python-visualization-with-bokeh/\n",
    "\n",
    "* ref\n",
    "    * introduction to big data with apache spark (Berkley Anthony Joseph)\n",
    "    * [spark-sklearn](https://github.com/databricks/spark-sklearn)\n",
    "        ```\n",
    "        pip install spark-sklearn\n",
    "        ```\n",
    "        \n",
    "        * spark-shell\n",
    "        ```\n",
    "        $SPARK_HOME/bin/spark-shell --packages databricks:spark-sklearn:0.2.0\n",
    "        ```\n",
    "        \n",
    "    * 듀크대학 STA663 Statistical Computing and Computation, Spring 2016\n",
    "        * https://github.com/cliburn/sta-663-2016\n",
    "\n",
    "    * kaggle\n",
    "        * https://www.kaggle.com/kaggle/us-baby-names\n",
    "        * https://github.com/pcsanwald/kaggle-titanic\n",
    "\n",
    "* TODO\n",
    "    * spark를 사용해서 이미지 처리 http://docs.thunder-project.org/spark\n",
    "    * spakr mysql\n",
    "    ```\n",
    "    spark.driver.extraLibraryPath\n",
    "    os.environ['SPARK_CLASSPATH'] =r\"/home/jsl/Code/git/bb/sd/lib/mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar\"\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.1 빅데이터와 Spark\n",
    "\n",
    "* 빅데이터는 여러 출처에서 발생하며, 그 형식이 비구조적이고 다양하다.\n",
    "* 데이터 웨어하우스, 데이터 마이닝과 관련이 있다.\n",
    "* ETL (Extract, Transfrom and Load)\n",
    "    * RapidMiner, SAS 등\n",
    "\n",
    "단계 | 설명\n",
    "-----|-----\n",
    "Extract | 다양한 소스에서 데이터 추출 (Hadoop files, files, json, DB...)\n",
    "Transform | 데이터 변환.\n",
    "Load | 데이터베이스에 저장하여 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.2 Spark 소개\n",
    "\n",
    "### 버전\n",
    "* 2009년 UC Berkeley, Matei Zaharia 박사과정 하면서 개뱔\n",
    "* 2010년 BSD 라이센스 오픈소스로 전환.\n",
    "* 2013년 Apache 2.0 license로 전환\n",
    "* 현재 개발자가 Databricks를 설립해서 관리\n",
    "\n",
    "### 왜 Spark를 배워야 하는가?\n",
    "* REPL (Read Eval Print Looop)이 가능해서 배우기 쉽다. 쉘 환경이 있어 편리하다. Standalone으로 시작할 수 있다.\n",
    "* 빅데이터를 빠르게 map reduce 할 수 있다.\n",
    "* Machine learning 라이브러리를 가지고 있다.\n",
    "\n",
    "### 구조\n",
    "\n",
    "* 분산 클러스터 컴퓨팅 프레임워크로서, API를 사용해서 데이터추출, 변환, 기계학습, 그래프분석을 할 수 있다.\n",
    "* Spark Core가 분산작업에 필요한 바탕이 되고, 그 위 sql, streaming, mllib, graphx를 제공한다.\n",
    "\n",
    "구분 | 구성 | 설명\n",
    "-------|-------|-------\n",
    "Spark engine | Spark Core | 작업배분, 입출력 등 분산작업에 필요한 기능\n",
    "Spark Applicaiton Frameworks | Spark SQL | DataFrames\n",
    "| Spark Streaming | 실시간 처리\n",
    "| MLlib | 머신러닝 (참조 scikit-learn)\n",
    "|GraphX | 그래프 분석\n",
    "\n",
    "\n",
    "* 빅데이터를 처리하기 위해 만들어져 있고, Hadoop과 달리 메모리에서 처리하기 때문에 빠름 (pipeline). Spark는 RDD를 통해 Hadoop을 사용할 수 있다.\n",
    "* Scala로 개발되어 jvm에서 실행. 그러나 Scala, Java, Python, R 여러 언어를 섞어서 할 수 있는 환경을 제공 (polyglot)\n",
    "\n",
    "구분 | Spark | Hadoop\n",
    "-------|-------|-------\n",
    "사용 목적 | 데이터 분석 | 데이터 분산 처리\n",
    "파일 시스템 | 자체 파일 시스템이 없슴. hdfs, db, csv등을 사용 | hdfs\n",
    "속도 | 파이프라인을 사용하므로 빠름 | 보다 느림\n",
    "\n",
    "### Spark의 3가지 데이터 - rdd, dataset, dataframe\n",
    "\n",
    "데이터구조 | 도입된 spark version | 설명\n",
    "---------|---------|---------\n",
    "rdd | Spark 1.0 | \n",
    "dataframe | 1.3 | \n",
    "dataset | 1.6 | Scala and Java에서 사용할 수 있다.\n",
    "\n",
    "* Pyspark는 데이터타잎이 loose하므로 RDD, DataFrame만 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설치\n",
    "\n",
    "* 설치하려는 하둡의 버전을 선택하여, prebuilt distribution을 설치한다.\n",
    "* [Spark 다운로드](https://spark.apache.org/downloads.html)\n",
    "    * spark 1.6 hadoop2.6 \n",
    "        ```\n",
    "        tar xvfz spark-1.x.x-bin-hadoop2.x.tgz\n",
    "        ```\n",
    "\n",
    "    * 또는 최근 버전은 tar -xvzf spark-2.1.0-bin-hadoop2.7.tgz\n",
    "\n",
    "* before Spark 2.0\n",
    "    * SparkContext\n",
    "    * streaming StreamingContext\n",
    "    * SQL sqlContext\n",
    "    * hive HiveContext\n",
    "\n",
    "* Spark 2.0\n",
    "    * SparkSession는 context를 통합해서 제공한다. SQLContext, HiveContext and future StreamingContext.\n",
    "    * No need to create SparkConf, SparkContext or SQLContex\n",
    "    * DataFrame still exists but is just a synonym for a Dataset.\n",
    "    sparkSession.read.text와 RDD가 비슷\n",
    "    _sc = spark.sparkContext라고 하면 sc를 만들 수 있다.\n",
    "    * backward compatibility - SQLContext를 그대로 사용할 수 있다.\n",
    "    * 아래는 sqlContex를 포함\n",
    "\n",
    "* diff 1.6 vs 2.0 -- sparksession\n",
    "    * Spark 1.6\n",
    "    sparkConf = SparkConf()\n",
    "    sc = SparkContext(conf=sparkConf)\n",
    "    sqlContext = SQLContext(sc)\n",
    "    df = sc.read.json(\"data.json\")\n",
    "    tables = sc.tables()\n",
    "\n",
    "    * Spark 2.x\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    df = spark.read.json(\"data.json\")\n",
    "    tables = spark.catalog.listTables()\n",
    "\n",
    "\n",
    "```\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"spark session example\")\\\n",
    "    .getOrCreate()\n",
    "1. 그냥 파일\n",
    "textFile=spark.read.text(\"ds_spark_wiki.txt\")\n",
    "\n",
    "2. dataframe연습\n",
    "fields = [\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "]\n",
    "schema = StructType(fields)\n",
    "\n",
    "test = spark.createDataFrame([\n",
    "    Row(id=1, name=u\"a\", age=34), \n",
    "    Row(id=2, name=u\"b\", age=25)\n",
    "], schema)\n",
    "\n",
    "test.show()\n",
    "\n",
    "쓰기 연습 postgresql로 (안해보았슴)\n",
    "test.write.jdbc(\n",
    "    url=\"jdbc:postgresql://localhost:5432/db\", \n",
    "    table=\"test\", \n",
    "    mode=\"overwrite\", \n",
    "    properties={\n",
    "        \"user\":\"root\", \n",
    "        \"password\":\"12345\", \n",
    "        \"driver\":\"org.postgresql.Driver\", \n",
    "        \"client_encoding\":\"utf8\"\n",
    "   }\n",
    ")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 문제 S-1: Spark를 standalone cluster로 구성하기\n",
    "\n",
    "* 클러스터를 구성하지 않으면 클러스터 없이 운영 - curl로 7077, 8080확인해도 없음 -> NO cluster!\n",
    "\n",
    "* 클러스터의 종류:\n",
    "    * Spark-Standalone – Spark workers are registered with spark master\n",
    "    * Yarn – Spark workers are registered with YARN Cluster manager.\n",
    "    * Mesos – Spark workers are registered with Mesos.\n",
    "\n",
    "* 클러스터 환경 구성\n",
    "    * Client: spark shell, pyspark shell\n",
    "    * Cluster\n",
    "        * Cluster 1:\n",
    "            * Spark master / Spark worker\n",
    "            * hdfs namenode / datanode\n",
    "        * Cluster n:\n",
    "            * Spark worker\n",
    "            * hdfs datanode\n",
    "\n",
    "*  Spark-Standalone\n",
    "    * SPARK_HOME은 /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/\n",
    "    * 단계1 JAVA_HOME을 설정\n",
    "        * JAVA_HOME을 설정한다.\n",
    "            * automatically set symlink to java binary /usr/bin/java\n",
    "            * JAVA_HOME을 설정하려면 /etc/environment에 하는 것이 좋다.\n",
    "            ```\n",
    "            $ echo $JAVA_HOME\n",
    "            $ update-alternatives --config java\n",
    "            ```\n",
    "\n",
    "    * 단계2: master 실행\n",
    "        * $SPARK_HOME/conf/spark-env.sh에 master ip설정\n",
    "            ```\n",
    "            SPARK_MASTER_IP=\n",
    "            ```\n",
    "\n",
    "        * 실행\n",
    "            ```\n",
    "            $ sh $SPARK_HOME/sbin/start-master.sh\n",
    "            ```\n",
    "            * spark://IPADRESS_OF_YOUR_MASTER_SYSTEM:7077\n",
    "            * 기본 port는 7077 (web UI는 localhost:8080)\n",
    "\n",
    "    * 단계3: slave 실행 (worker라고 함)\n",
    "        ```\n",
    "        $ sh $SPARK_HOME/sbin//start-slave.sh spark://IPADRESS_OF_YOUR_MASTER_SYSTEM:7077\n",
    "        ```\n",
    "\n",
    "* 쉘 명령어\n",
    "\n",
    "sbin디렉토리의 shell | 설명\n",
    "----------|----------\n",
    "start-master.sh, stop-master.sh | 마스터를 시작 (종료)\n",
    "start-slaves.sh, stop-slaves.sh | 각 노드의 슬레이브를 시작 (종료)\n",
    "start-all.sh, stop-all.sh | 마스터, 슬레이브를 모두 시작 (종료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-2: Hello Spark: 환경설정을 읽어 클라이언트 sc를 생성하기.\n",
    "\n",
    "* Spark는 batch, streaming, iterative, interactive 4가지 방식으로 실행할 수 있다.\n",
    "* 환경설정을 읽어 sc를 생성하고, 이를 클라이언트와 같이 Spark를 사용한다.\n",
    "\n",
    "### spark-submit\n",
    "* spark 프로그램을 일괄 실행\n",
    "* Python프로그램 (.py)의 일괄 실행\n",
    "\n",
    "### insteractive shell\n",
    "\n",
    "* Scala, Python에서 지원되는 REPL (the Read-Eval-Print-Loop)\n",
    "* scala\n",
    "    ```\n",
    "    ./bin/spark-shell\n",
    "    scala>\n",
    "    ```\n",
    "\n",
    "* python\n",
    "    * pyspark를 실행하면, sc, sqlContext는 제공된다.\n",
    "    ```\n",
    "    spark-1.6.0-bin-hadoop2.6/bin$ pyspark\n",
    "    Welcome to\n",
    "          ____              __\n",
    "         / __/__  ___ _____/ /__\n",
    "        _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "       /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
    "          /_/\n",
    "\n",
    "    Using Python version 2.7.12 (default, Jul  1 2016 15:12:24)\n",
    "    SparkContext available as sc, HiveContext available as sqlContext.\n",
    "    >>> sc.version\n",
    "    u'1.6.0'\n",
    "    >>> text=sc.textFile(\"derby.log\");\n",
    "    >>>\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SparkContext\n",
    "    * Spark서버(클러스터)에 대한 클라이언트와 같은 역할로, 반드시 있어야 한다.\n",
    "    * 클러스터를 어떻게 사용할 것인지 정하는 것 -> cluster manager에서 system resource를 할당받음 (cpu, memory, machine)\n",
    "    * Python의 SparkContext는 jar를 분산환경에서 사용하게 되므로 주의 (Scala, Java와 다름)\n",
    "        * pyFiles에 사용할 (의존적인) 라이브러리를 넣는다 (또는 사용할 라이브러리가 없으면 빈 파일로 둔다).\n",
    "\n",
    "    * Cannot run multiple SparkContexts at once;\n",
    "        * sc가 이미 있는 경우\n",
    "        ```\n",
    "        from pyspark import SparkContext\n",
    "        sc = SparkContext(\"master\",\"my python app\", sparkHome=\"sparkhome\",pyFiles=\"placeholderdeps.zip\")\n",
    "        ```\n",
    "\n",
    "* SparkConf\n",
    "    * spark-defaults.conf와 같은 파일의 값을 읽어서 설정\n",
    "    ```\n",
    "    scala> sc.getConf.getOption(\"spark.local.dir\")\n",
    "    res0: Option[String] = None\n",
    "\n",
    "    scala> sc.getConf.getOption(\"spark.app.name\")\n",
    "    res1: Option[String] = Some(Spark shell)\n",
    "\n",
    "    scala> sc.getConf.get(\"spark.master\")\n",
    "    res2: String = local[*]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark on IPython Notebook\n",
    "\n",
    "* ipython notebook에서 pyspark를 사용\n",
    "* kernel을 만들지 않고, findspark를 사용한다.\n",
    "* SPARK_HOME을 설정해서 사용한다\n",
    "    * 현재 ~/Downloads/spark-1.6.0-bin-hadoop2.6\n",
    "* kernel을 사용하면 아래와 같이 한다.\n",
    "    ```\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    conf = SparkConf().setAppName(\"jsl\").setMaster(\"local\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "home=os.getenv(\"HOME\")\n",
    "spark_home=os.path.join(home,\"Downloads/spark-1.6.0-bin-hadoop2.6\")\n",
    "findspark.init(spark_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SparkConf()에서 설정한 값을 읽어 SparkContext sc를 생성한다.\n",
    "    * spark.master와 spark.app.name 필수적으로 설정해야 한다.\n",
    "* standalone 실행에서의 spark master\n",
    "    * local로 설정되고, 기본은 필요한만큼 쓰레드를 생성한다 (local[*])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x10f05add0>\n"
     ]
    }
   ],
   "source": [
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 설정을 읽어 온다 (conf디렉토리 아래)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.get(\"spark.jars.packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.app.name', u'myAppName'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.jars',\n",
       "  u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'),\n",
       " (u'spark.jars.packages',\n",
       "  u'graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0'),\n",
       " (u'spark.files',\n",
       "  u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.master', u'local[*]'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.submit.pyFiles',\n",
       "  u'/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'),\n",
       " (u'spark.mongodb.input.partitioner', u'MongoPaginateBySizePartitioner')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.3 Hello RDD\n",
    "\n",
    "* RDD (Resilient Distributed Dataset)는 분산 레코드인 데이터 형식이다.\n",
    "    * Resilient - fault tolerent (어느 한 노드에서 작업이 실패하면 다른 노드에서 실행된다.)\n",
    "    * Distributed - multiple nodes in a clusters\n",
    "    * Dataset - 데이터타잎으로 구성된다.\n",
    "* RDD는 내, 외부 자료에서 생성하며, 생성된 자료는 read-only이다.\n",
    "    * HDFS 파일을 처리할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3단계 처리\n",
    "    * 1단계: 읽기 - 2가지 방식: \n",
    "        * 외부에서 읽기 \n",
    "        ```\n",
    "        sc.textFile()\n",
    "        ```\n",
    "        \n",
    "        * 내부에서 읽기 parallelizing a collection\n",
    "        ```\n",
    "        sc.parallelize()\n",
    "        ```\n",
    "        \n",
    "    * 2단계: 변환 transformations - lazy도 가능: RDD => RDD or seq(RDD)\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "map(fn) | 요소별로 fn을 적용해서 결과 rdd 돌려줌 | \n",
    "filter(fn) | 요소별로 선별하여 fn을 적용해서 결과 rdd 돌려줌 | filter(lambda line: \"Spark\" in line)\n",
    "flatMap(fn) | 요소별로 fn을 적용하고, flat해서 결과 rdd 돌려줌 | .flatMap(lambda x: x.split(' '))\n",
    "groupByKey() | key를 그룹해서 iterator를 돌려줌. |\n",
    "\n",
    "\n",
    "    * 3단계: actions: RDD => a value (e.g., python list)\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "reduce(fn) | 요소별로 fn을 사용해서 줄여서 결과 list를 돌려줌 |\n",
    "collect() | 모든 요소를 결과 list로 돌려줌 |\n",
    "count() | 요소의 갯수를 결과 list로 돌려줌 |\n",
    "countByKey() | |\n",
    "foreach(fn) | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark를 사용하기 전, python 함수 사용하기\n",
    "\n",
    "* map, reduce, filter\n",
    "    * 함수의 인자는 2개가 필요하다 (함수, 데이터).\n",
    "\n",
    "함수 | 설명 | 예\n",
    "-------|-------|-------\n",
    "map() | 각 데이터 요소에 함수를 적용해서 list를 반환 | map(fn,data)\n",
    "filter() | 각 데이터 요소에 함수의 결과 True를 선택해서 반환 | filter(fn, data)\n",
    "reduce() | 각 데이터 요소에 함수를 적용해서 list를 반환 | reduce(fn, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python 함수로 처리한다.\n",
    "    * 입출력은 데이터 하나씩이 아니라, list로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python에서 제공하는 map() 함수를 사용한다. map() 함수의 인자:\n",
    "    * (1) 함수명 (함수의 return은 반드시 있어야 한다.)\n",
    "    * (2) 입력인자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, celsius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lambda함수를 사용한다.\n",
    "    * lambda는 무명 함수이다. 처리 결과가 반환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5)*c + 32, celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자열에 map()을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hello World'\n",
    "words = sentence.split()\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자열을 사용하면, 각 단어를 split()한다.\n",
    "* list를 사용하면, 각 요소를 split()한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello World\"\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"Hello World\"]\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter()는 데이터를 선별한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduce()는 2개의 인자를 받는다.\n",
    "* [ func(func(s1, s2),s3), ... , sn ]와 같이 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, range(1,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark RDD 사용하기\n",
    "\n",
    "* Apache spark wiki에서 첫 문단을 복사해 왔다.\n",
    "* 3째줄은 한글, 4째 줄은 같은 단어를 반복해 추가했다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 파일에서 읽기\n",
    "    ```\n",
    "    textFile()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"data/ds_spark_wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wikipedia'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* map()함수로 단어 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=textFile.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lambda아닌 함수로 map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "words=textFile.map(mySplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.'],\n",
       " [u'Originally',\n",
       "  u'developed',\n",
       "  u'at',\n",
       "  u'the',\n",
       "  u'University',\n",
       "  u'of',\n",
       "  u'California,',\n",
       "  u\"Berkeley's\",\n",
       "  u'AMPLab,'],\n",
       " [u'the',\n",
       "  u'Spark',\n",
       "  u'codebase',\n",
       "  u'was',\n",
       "  u'later',\n",
       "  u'donated',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'Apache',\n",
       "  u'Software',\n",
       "  u'Foundation,'],\n",
       " [u'which', u'has', u'maintained', u'it', u'since.'],\n",
       " [u'Spark',\n",
       "  u'provides',\n",
       "  u'an',\n",
       "  u'interface',\n",
       "  u'for',\n",
       "  u'programming',\n",
       "  u'entire',\n",
       "  u'clusters',\n",
       "  u'with'],\n",
       " [u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Wikipedia']\n",
      "[u'Apache', u'Spark', u'is', u'an', u'open', u'source', u'cluster', u'computing', u'framework.']\n",
      "[u'\\uc544\\ud30c\\uce58', u'\\uc2a4\\ud30c\\ud06c\\ub294', u'\\uc624\\ud508', u'\\uc18c\\uc2a4', u'\\ud074\\ub7ec\\uc2a4\\ud130', u'\\ucef4\\ud4e8\\ud305', u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']\n",
      "[u'Apache', u'Spark', u'Apache', u'Spark', u'Apache', u'Spark', u'Apache', u'Spark']\n",
      "[u'Originally', u'developed', u'at', u'the', u'University', u'of', u'California,', u\"Berkeley's\", u'AMPLab,']\n",
      "[u'the', u'Spark', u'codebase', u'was', u'later', u'donated', u'to', u'the', u'Apache', u'Software', u'Foundation,']\n",
      "[u'which', u'has', u'maintained', u'it', u'since.']\n",
      "[u'Spark', u'provides', u'an', u'interface', u'for', u'programming', u'entire', u'clusters', u'with']\n",
      "[u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']\n"
     ]
    }
   ],
   "source": [
    "for i in words.collect():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 문장의 철자 갯수를 센다.\n",
    "    * 첫 문장 'Wiskipedia'는 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_sparkLine=textFile.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print _sparkLine.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한글은 앞에 u를 붙여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_line = textFile.filter(lambda line: u\"스파크\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "print _line.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "* groupByKey()\n",
    "    * groupByKey()는 key를 묶어준다. 따라서 iterator를 반환한다. mapValues(sum)을 하면 key별 합계를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 1),\n",
       " (u'\\uc18c\\uc2a4', 1),\n",
       " (u'is', 1),\n",
       " (u'Wikipedia', 1),\n",
       " (u'AMPLab,', 1),\n",
       " (u'maintained', 1),\n",
       " (u'donated', 1),\n",
       " (u'\\ucef4\\ud4e8\\ud305', 1),\n",
       " (u'open', 1),\n",
       " (u'since.', 1),\n",
       " (u'for', 1),\n",
       " (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1),\n",
       " (u'with', 1),\n",
       " (u'framework.', 1),\n",
       " (u'provides', 1),\n",
       " (u'Apache', 6),\n",
       " (u'Spark', 7),\n",
       " (u'was', 1),\n",
       " (u'Originally', 1),\n",
       " (u'which', 1),\n",
       " (u'fault-tolerance.', 1),\n",
       " (u'University', 1),\n",
       " (u'codebase', 1),\n",
       " (u'interface', 1),\n",
       " (u'data', 1),\n",
       " (u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.', 1),\n",
       " (u'Foundation,', 1),\n",
       " (u'\\uc624\\ud508', 1),\n",
       " (u'programming', 1),\n",
       " (u'\\uc2a4\\ud30c\\ud06c\\ub294', 1),\n",
       " (u'the', 3),\n",
       " (u'entire', 1),\n",
       " (u'has', 1),\n",
       " (u'to', 1),\n",
       " (u'later', 1),\n",
       " (u'computing', 1),\n",
       " (u'Software', 1),\n",
       " (u'developed', 1),\n",
       " (u\"Berkeley's\", 1),\n",
       " (u'it', 1),\n",
       " (u'an', 2),\n",
       " (u'cluster', 1),\n",
       " (u'implicit', 1),\n",
       " (u'at', 1),\n",
       " (u'of', 1),\n",
       " (u'clusters', 1),\n",
       " (u'parallelism', 1),\n",
       " (u'\\uc544\\ud30c\\uce58', 1),\n",
       " (u'California,', 1),\n",
       " (u'source', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parallelize() 사용하기\n",
    "    * list에서 읽어, rdd로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_aList=[1,2,3]\n",
    "rdd = sc.parallelize(_aList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* map(), collect() 사용해서 square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "print squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장 처리하기\n",
    "* 단어를 교체하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "a=[\"this is\",\"a line\"]\n",
    "_rdd=sc.parallelize(a)\n",
    "\n",
    "words=_rdd.map(lambda x:x.split())\n",
    "print words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_upper=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "_upper.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 첫 글자를 대문자로 만들어서 출력해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: x[0].upper())\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'WIKIPEDIA'], [u'APACHE', u'SPARK', u'IS', u'AN', u'OPEN', u'SOURCE', u'CLUSTER', u'COMPUTING', u'FRAMEWORK.'], [u'\\uc544\\ud30c\\uce58', u'\\uc2a4\\ud30c\\ud06c\\ub294', u'\\uc624\\ud508', u'\\uc18c\\uc2a4', u'\\ud074\\ub7ec\\uc2a4\\ud130', u'\\ucef4\\ud4e8\\ud305', u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.'], [u'APACHE', u'SPARK', u'APACHE', u'SPARK', u'APACHE', u'SPARK', u'APACHE', u'SPARK'], [u'ORIGINALLY', u'DEVELOPED', u'AT', u'THE', u'UNIVERSITY', u'OF', u'CALIFORNIA,', u\"BERKELEY'S\", u'AMPLAB,'], [u'THE', u'SPARK', u'CODEBASE', u'WAS', u'LATER', u'DONATED', u'TO', u'THE', u'APACHE', u'SOFTWARE', u'FOUNDATION,'], [u'WHICH', u'HAS', u'MAINTAINED', u'IT', u'SINCE.'], [u'SPARK', u'PROVIDES', u'AN', u'INTERFACE', u'FOR', u'PROGRAMMING', u'ENTIRE', u'CLUSTERS', u'WITH'], [u'IMPLICIT', u'DATA', u'PARALLELISM', u'AND', u'FAULT-TOLERANCE.']]\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: [i.upper() for i in x])\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transformation(map()), action(collect()) 함수를 한꺼번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: [i.upper() for i in x]).collect()\n",
    "print pluralRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = words\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print wordsLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 파일에 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pluralRDD.saveAsTextFile(\"data/ds_spark_wiki1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* create RDD from CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp_file = sc.textFile(\"./data/ds_spark_2cols.csv\")\n",
    "numbers_rdd = inp_file.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1'],\n",
       " [u'14', u' 19'],\n",
       " [u'46', u' 1'],\n",
       " [u'10', u' 34'],\n",
       " [u'28', u' 3'],\n",
       " [u'48', u' 1']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.4 데이터 타잎\n",
    "\n",
    "구분 | 설명\n",
    "----------|----------\n",
    "Vector | dense와 sparse가 있다.\n",
    "Labled Point | 클래스값과 결과를 묶음. supervised learning에 사용.\n",
    "Matrix | 행열로 구성\n",
    "\n",
    "* vectors\n",
    "    * local vector - single machine에 있다.\n",
    "    * pyspark.mllib.linalg.Vectors\n",
    "\n",
    "dense vector | sparse vector\n",
    "----------|----------\n",
    "빈 값이 별로 없는 경우. an array of its values | 빈 값이 많은 경우 사용. 인덱스, 값 배열 별도\n",
    "(160,69,24) | (3,[0,1,2],[160.0,69.0,24.0])\n",
    "입력 NumPy’s array, Python list | MLlib’s SparseVector, SciPy’s csc_matrix with a single column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "dv1=Vectors.dense([0.0, 1.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.1,0.1]\n"
     ]
    }
   ],
   "source": [
    "print dv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* numpy를 사용해서 dense vector를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dv2 = np.array([1.0, 2.1, 3.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.1,0.1]\n"
     ]
    }
   ],
   "source": [
    "print dv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python list를 사용하여 dense vector를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dv2 = [1.0, 2.1, 3.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sparse vector.\n",
    "\n",
    "* SparseVector vs Vectors.sparse 차이?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* toArray()는 1줄씩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.]\n"
     ]
    }
   ],
   "source": [
    "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\n",
    "print sv1.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (2, 0)\t3.0\n"
     ]
    }
   ],
   "source": [
    "sv2 = sps.csc_matrix((np.array([1.0,3.0]), np.array([0,2]), np.array([0,2])), shape = (3,1))\n",
    "sv2.todense()\n",
    "print sv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labeled point\n",
    "\n",
    "* local vector, either dense or sparse\n",
    "* label과 response로 구성된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "LabeledPoint(1, [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "LabeledPoint(1.0, Vectors.dense([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python list에서 dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [[1,[1.0,2.0,3.0]],[1,[1.1,2.1,3.1]],[0,[1.2,2.2,3.3]]]\n",
    "trainDf=sqlCtx.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python list를 LabeledPoint로 생성하면, label과 features로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1,[1.0,2.0,3.0]),\n",
    "     LabeledPoint(1,[1.1,2.1,3.1]),\n",
    "     LabeledPoint(0,[1.2,2.2,3.3])]\n",
    "trainDf=sqlCtx.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = sqlCtx.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* schema를 사용해서 dataframe 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "_rdd = sc.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf=_rdd.toDF(schema)\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maxtrix\n",
    "\n",
    "* local matrix - pyspark.mllib.linalg.Matrix, Matrices\n",
    "* distributed matrix\n",
    "    * pyspark.mllib.linalg.distributed.RowMatrix\n",
    "    * pyspark.mllib.linalg.distributed.IndexedRow, IndexedRowMatrix\n",
    "    * pyspark.mllib.linalg.distributed.BlockMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libsvm format\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "* format\n",
    "    ```\n",
    "    [label] [index1]:[value1] [index2]:[value2] ...\n",
    "    [label] [index1]:[value1] [index2]:[value2] ...\n",
    "    ```\n",
    "    * label - class\n",
    "    * index - integers\n",
    "    * value - real numbers\n",
    "\n",
    "* see - /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/data/mllib/sample_libsvm_data.txt\n",
    "```\n",
    "0 128:51 129:159 130:253 131:159 132:50 155:48 156:238 157:252 158:252 159:252 160:237 182:54 183:227 184:253 185:252 186:239 187:233 188:252 189:57 190:6 208:10 209:60 210:224 211:252 212:253 213:252 214:202 215:84 216:252 217:253 218:122 236:163 237:252 238:252 239:252 240:253 241:252 242:252 243:96 244:189 245:253 246:167 263:51 264:238 265:253 266:253 267:190 268:114 269:253 270:228 271:47 272:79 273:255 274:168 290:48 291:238 292:252 293:252 294:179 295:12 296:75 297:121 298:21 301:253 302:243 303:50 317:38 318:165 319:253 320:233 321:208 322:84 329:253 330:252 331:165 344:7 345:178 346:252 347:240 348:71 349:19 350:28 357:253 358:252 359:195 372:57 373:252 374:252 375:63 385:253 386:252 387:195 400:198 401:253 402:190 413:255 414:253 415:196 427:76 428:246 429:252 430:112 441:253 442:252 443:148 455:85 456:252 457:230 458:25 467:7 468:135 469:253 470:186 471:12 483:85 484:252 485:223 494:7 495:131 496:252 497:225 498:71 511:85 512:252 513:145 521:48 522:165 523:252 524:173 539:86 540:253 541:225 548:114 549:238 550:253 551:162 567:85 568:252 569:249 570:146 571:48 572:29 573:85 574:178 575:225 576:253 577:223 578:167 579:56 595:85 596:252 597:252 598:252 599:229 600:215 601:252 602:252 603:252 604:196 605:130 623:28 624:199 625:252 626:252 627:253 628:252 629:252 630:233 631:145 652:25 653:128 654:252 655:253 656:252 657:141 658:37\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmfn=\"/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/data/mllib/sample_libsvm_data.txt\"\n",
    "svmDf = sqlCtx.read.format(\"libsvm\").load(svmfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(svmDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* csr format (https://www.ncsu.edu/hpc/Courses/6sparse.html)\n",
    "    ```\n",
    "    0 0 0 0\n",
    "    5 8 0 0\n",
    "    0 0 3 0\n",
    "    0 6 0 0\n",
    "    ```\n",
    "    * non-zero 5 8 3 6\n",
    "    * column-index 0 1 2 1 (5(1,0) 8(1,1) 3(2,2) 6(3,1)에서 행값만 추출)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-3: RDD를 사용하여 MLlib의 입력 데이터 word vector생성하기.\n",
    "\n",
    "* RDD API를 사용해서 단어를 셀 수 있다 (map, reduce 등).\n",
    "* mllib 패키지를 사용하여 데이터를 변환할 수 있다.\n",
    "    * TF-IDF, Word2Vec 등을 사용할 수 있다.\n",
    "    * mllib에 없는 변환기능은 ml을 사용한다 (ml은 dataframe을 변환하는 패키지.)\n",
    "        * Tokenizer, StopWordsRemove, n-gram등\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ds_spark_wiki.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/ds_spark_wiki.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 전체 word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"data/ds_spark_wiki.txt\")\n",
    "wc = lines\\\n",
    "    .flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'an',\n",
       " u'open',\n",
       " u'source',\n",
       " u'cluster',\n",
       " u'computing',\n",
       " u'framework.',\n",
       " u'\\uc544\\ud30c\\uce58',\n",
       " u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       " u'\\uc624\\ud508',\n",
       " u'\\uc18c\\uc2a4',\n",
       " u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       " u'\\ucef4\\ud4e8\\ud305',\n",
       " u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Originally',\n",
       " u'developed',\n",
       " u'at',\n",
       " u'the',\n",
       " u'University',\n",
       " u'of',\n",
       " u'California,',\n",
       " u\"Berkeley's\",\n",
       " u'AMPLab,',\n",
       " u'the',\n",
       " u'Spark',\n",
       " u'codebase',\n",
       " u'was',\n",
       " u'later',\n",
       " u'donated',\n",
       " u'to',\n",
       " u'the',\n",
       " u'Apache',\n",
       " u'Software',\n",
       " u'Foundation,',\n",
       " u'which',\n",
       " u'has',\n",
       " u'maintained',\n",
       " u'it',\n",
       " u'since.',\n",
       " u'Spark',\n",
       " u'provides',\n",
       " u'an',\n",
       " u'interface',\n",
       " u'for',\n",
       " u'programming',\n",
       " u'entire',\n",
       " u'clusters',\n",
       " u'with',\n",
       " u'implicit',\n",
       " u'data',\n",
       " u'parallelism',\n",
       " u'and',\n",
       " u'fault-tolerance.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어를 세어서 tuple로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wc = sc.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .flatMap(lambda x: x.split(' '))\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "    .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'and', 1)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라인 별 word count\n",
    "\n",
    "* dataframe으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wc = sc.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n",
    "    .map(lambda x:x.split())\\\n",
    "    .map(lambda x:[(i,1) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'wikipedia', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'is', 1), (u'an', 1), (u'open', 1), (u'source', 1), (u'cluster', 1), (u'computing', 1), (u'framework', 1)]\n",
      "[(u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c\\ub294', 1), (u'\\uc624\\ud508', 1), (u'\\uc18c\\uc2a4', 1), (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1), (u'\\ucef4\\ud4e8\\ud305', 1), (u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4', 1)]\n",
      "[(u'originally', 1), (u'developed', 1), (u'at', 1), (u'the', 1), (u'university', 1), (u'of', 1), (u'california', 1), (u\"berkeley's\", 1), (u'amplab', 1)]\n",
      "[(u'the', 1), (u'spark', 1), (u'codebase', 1), (u'was', 1), (u'later', 1), (u'donated', 1), (u'to', 1), (u'the', 1), (u'apache', 1), (u'software', 1), (u'foundation', 1)]\n",
      "[(u'which', 1), (u'has', 1), (u'maintained', 1), (u'it', 1), (u'since', 1)]\n",
      "[(u'spark', 1), (u'provides', 1), (u'an', 1), (u'interface', 1), (u'for', 1), (u'programming', 1), (u'entire', 1), (u'clusters', 1), (u'with', 1)]\n",
      "[(u'implicit', 1), (u'data', 1), (u'parallelism', 1), (u'and', 1), (u'fault', 1), (u'tolerance', 1)]\n"
     ]
    }
   ],
   "source": [
    "for e in wc.collect():\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF (Term Frequency)\n",
    "    * HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = sc.textFile(\"data/ds_spark_wiki.txt\").map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {253068: 1.0}),\n",
       " SparseVector(1048576, {36751: 1.0, 50570: 1.0, 68380: 1.0, 415281: 1.0, 511377: 1.0, 728364: 1.0, 862087: 1.0, 938426: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {63234: 1.0, 340190: 1.0, 357478: 1.0, 375592: 1.0, 458138: 1.0, 486171: 1.0, 598772: 1.0}),\n",
       " SparseVector(1048576, {938426: 4.0, 999480: 4.0}),\n",
       " SparseVector(1048576, {36757: 1.0, 225801: 1.0, 323305: 1.0, 453405: 1.0, 498679: 1.0, 518030: 1.0, 688842: 1.0, 762570: 1.0, 959994: 1.0}),\n",
       " SparseVector(1048576, {420843: 1.0, 550676: 1.0, 725041: 1.0, 782544: 1.0, 938426: 1.0, 959994: 2.0, 991590: 1.0, 993084: 1.0, 996703: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {50573: 1.0, 263739: 1.0, 892834: 1.0, 1014710: 1.0, 1035538: 1.0}),\n",
       " SparseVector(1048576, {3932: 1.0, 36751: 1.0, 192182: 1.0, 358969: 1.0, 363244: 1.0, 496856: 1.0, 546913: 1.0, 938426: 1.0, 951974: 1.0}),\n",
       " SparseVector(1048576, {69621: 1.0, 157580: 1.0, 219357: 1.0, 297436: 1.0, 715648: 1.0})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countPartitions(id,iterator): \n",
    "         c = 0 \n",
    "         for _ in iterator: \n",
    "              c += 1 \n",
    "         yield (id,c) \n",
    "_wc=wc.mapPartitions(countPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "trainRdd = trainDf.map(lambda row: LabeledPoint(row.label,row.features))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.5 Spark SQL\n",
    "\n",
    "* Spark SQL은 \n",
    "    * 데이터를 구조화해서 sql을 사용할 수 있다. RDD는 비구조적인 경우에 사용한다.\n",
    "\n",
    "구분 | Spark SQL | RDD\n",
    "-----|-----|-----\n",
    "데이터 | 구조적 | 비구조적\n",
    "* Spark SQL 구성\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "Language API | Python, Java, Scala, Hive QL API를 제공\n",
    "Schema RDD | RDD에 Schema를 적용해 임시 테이블로 변환한다. dataframe.\n",
    "Data Sources | 다양한 형식 지원 - HDFS, Cassandra, HBase, and relational databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dataframe 생성\n",
    "    * 이름, 키 정보 파일을 읽어서 키가 170이상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = [{'name': 'kim', 'height': 170}]\n",
    "sqlCtx.createDataFrame(p).collect()\n",
    "\n",
    "type(p)\n",
    "\n",
    "from pyspark.sql import *\n",
    "pRow=list(Row(name=\"kim\", height=1961))\n",
    "\n",
    "df=sqlCtx.createDataFrame([pRow])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-4: 파일을 읽어서 feature vector 생성하기.\n",
    "\n",
    "* rdd에서 dataframe을 생성하고, sql 사용한다.\n",
    "* 네트워크 침입\n",
    "    * https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "* attack 종류 구분 (41번째 열)\n",
    "\n",
    "침입구분 | 건수\n",
    "-------|-------\n",
    "normal | 97278\n",
    "attack | 396743\n",
    "전체 | 494021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')\n",
    "if(not os.path.exists(_fname)):\n",
    "    print \"%s data does not exist! retrieving..\" % _fname\n",
    "    _f=urllib.urlretrieve(_url,_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_rdd = sc.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터에 'normal.'이 포함된 건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n"
     ]
    }
   ],
   "source": [
    "_normal = _rdd.filter(lambda x: 'normal.' in x)\n",
    "print _normal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_csvRdd=_rdd.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']]\n"
     ]
    }
   ],
   "source": [
    "print _csvRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 분류\n",
    "    * reduceByKey()를 사용해 각 경우의 건 수를 센다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_kv = _csvRdd.map(lambda x: (x[41], 1))\n",
    "_attack = _kv.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 231),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 280790),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 107201),\n",
       " (u'normal.', 97278),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 1040),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 1247),\n",
       " (u'satan.', 1589)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attack.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_normalRdd=_csvRdd.filter(lambda x: x[41]==\"normal.\")\n",
    "_attackRdd=_csvRdd.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n",
      "396743\n"
     ]
    }
   ],
   "source": [
    "print _normalRdd.count()\n",
    "print _attackRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* combineByKey(x, y, z)\n",
    "    * Combiner function: x\n",
    "        * key-value에서 value로 combine하려면 (value,1)\n",
    "    * Merge value function: y\n",
    "    * Merge combiners function: z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3.0, 1: 10.0}\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize( [(0, 2.), (0, 4.), (1, 0.), (1, 10.), (1, 20.)] )\n",
    "sumCount = data.combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "averageByKey = sumCount.map(lambda (label, (value_sum, count)): (label, value_sum / count))\n",
    "\n",
    "print averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([(2.0, 1), (4.0, 1)], 2)), (1, ([(10.0, 1), (0.0, 1), (20.0, 1)], 3))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "aggregated_counts = (data\n",
    "    .map(lambda kv: (kv, 1))\n",
    "    .reduceByKey(add)\n",
    "    .map(lambda kv: (kv[0][0], (kv[0][1], kv[1])))\n",
    "    .groupByKey()\n",
    "    .mapValues(lambda xs: (list(xs), sum(x[1] for x in xs))))\n",
    "\n",
    "aggregated_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'back.': (2203, 2203),\n",
       " u'buffer_overflow.': (30, 30),\n",
       " u'ftp_write.': (8, 8),\n",
       " u'guess_passwd.': (53, 53),\n",
       " u'imap.': (12, 12),\n",
       " u'ipsweep.': (1247, 1247),\n",
       " u'land.': (21, 21),\n",
       " u'loadmodule.': (9, 9),\n",
       " u'multihop.': (7, 7),\n",
       " u'neptune.': (107201, 107201),\n",
       " u'nmap.': (231, 231),\n",
       " u'normal.': (97278, 97278),\n",
       " u'perl.': (3, 3),\n",
       " u'phf.': (4, 4),\n",
       " u'pod.': (264, 264),\n",
       " u'portsweep.': (1040, 1040),\n",
       " u'rootkit.': (10, 10),\n",
       " u'satan.': (1589, 1589),\n",
       " u'smurf.': (280790, 280790),\n",
       " u'spy.': (2, 2),\n",
       " u'teardrop.': (979, 979),\n",
       " u'warezclient.': (1020, 1020),\n",
       " u'warezmaster.': (20, 20)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts = _kv.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "sum_counts.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rdd to sql, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _data.map(lambda l: l.split(\",\"))\n",
    "_rdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "_df=sqlCtx.createDataFrame(_rdd)\n",
    "_df.registerTempTable(\"_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|protocol| count|\n",
      "+--------+------+\n",
      "|     udp| 20354|\n",
      "|     tcp|190065|\n",
      "|    icmp|283602|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.select(\"protocol\", \"duration\", \"dst_bytes\").groupBy(\"protocol\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|protocol|count|\n",
      "+--------+-----+\n",
      "|     tcp|  139|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.select(\"protocol\", \"duration\", \"dst_bytes\")\\\n",
    "    .filter(_df.duration>1000)\\\n",
    "    .filter(_df.dst_bytes==0)\\\n",
    "    .groupBy(\"protocol\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcp_interactions = sqlCtx.sql(\n",
    "\"\"\"\n",
    "    SELECT duration, dst_bytes FROM _tab\n",
    "    WHERE protocol = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcp_interactions_out = tcp_interactions\\\n",
    "    .map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5043, Dest. bytes: 0\n",
      "Duration: 5046, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 42448, Dest. bytes: 0\n",
      "Duration: 40121, Dest. bytes: 0\n",
      "Duration: 31709, Dest. bytes: 0\n",
      "Duration: 30619, Dest. bytes: 0\n",
      "Duration: 22616, Dest. bytes: 0\n",
      "Duration: 21455, Dest. bytes: 0\n",
      "Duration: 13998, Dest. bytes: 0\n",
      "Duration: 12933, Dest. bytes: 0\n"
     ]
    }
   ],
   "source": [
    "for i,ti_out in enumerate(tcp_interactions_out.collect()):\n",
    "    if(i%10==0):\n",
    "        print ti_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-5: 파일에서 Spark SQL로 데이터 읽기\n",
    "\n",
    "* 1. json파일에서 읽기\n",
    "    * 주의: format이 건별로 저장되어 있슴???\n",
    "* 2. twitter json\n",
    "* 3. url에서 json 읽어오기\n",
    "* 4. csv파일에서 일기\n",
    "* 5. com.databricks.spark.csv\n",
    "    * vim conf/spark-defaults.conf\n",
    "        ```\n",
    "        spark.jars.packages=com.databricks:spark-csv_2.10:1.3.0\n",
    "        ```\n",
    "\n",
    "* sqlContext.jsonRDD()\n",
    "* sqlContext.jsonFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json 파일 읽기\n",
    "\n",
    "* json파일을 읽어서, sql을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.json\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pDF= sqlCtx.read.json(\"/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pDF.filter(pDF['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pDF.registerTempTable(\"people\")\n",
    "sqlCtx.sql(\"select name from people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### twitter json을 읽을 경우\n",
    "\n",
    "구분 | 예\n",
    "-------|-------\n",
    "unicode를 사용하면 backslash | \"{\\\"created_at\\\":\\\"Sun Nov 13 00:05:19 +0000 2016\\\"\n",
    "보통 | {\"created_at\":\"Sun Nov 13 00:05:19 +0000 2016\"\n",
    "\n",
    "\n",
    "    * allowBackslashEscapingAnyCharacter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitterDF= sqlCtx.read.json(\"src/ds_twitter_1_noquote.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- symbols: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- urls: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- favorited: boolean (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |-- in_reply_to_user_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- is_quote_status: boolean (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- default_profile: boolean (nullable = true)\n",
      " |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- favourites_count: long (nullable = true)\n",
      " |    |-- follow_request_sent: boolean (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following: boolean (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |-- has_extended_profile: boolean (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- is_translation_enabled: boolean (nullable = true)\n",
      " |    |-- is_translator: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- notifications: boolean (nullable = true)\n",
      " |    |-- profile_background_color: string (nullable = true)\n",
      " |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |-- profile_image_url: string (nullable = true)\n",
      " |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |-- profile_link_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |-- profile_text_color: string (nullable = true)\n",
      " |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- time_zone: string (nullable = true)\n",
      " |    |-- translator_type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- utc_offset: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           text|\n",
      "+---------------+\n",
      "|Hello 21 160924|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.select('text').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           text|\n",
      "+---------------+\n",
      "|Hello 21 160924|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.registerTempTable(\"twitter\")\n",
    "sqlCtx.sql(\"select text from twitter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json frm url\n",
    "\n",
    "* url에서 데이터 읽으면 string (예: r.iter_lines()하면 문자 1개씩 가져옴)\n",
    "* response를 json으로 읽으면 ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Row로 만들어주어야?\n",
    "    ```\n",
    "    df = sqlContext.createDataFrame([json.loads(line) for line in r.iter_lines()])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wcDF=sqlCtx.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.registerTempTable(\"wc\")\n",
    "sqlCtx.sql(\"select Club,Team,Year from wc\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* baby names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "_url=\"https://health.data.ny.gov/api/views/jxy9-yhdk/rows.json?accessType=DOWNLOAD\"\n",
    "_json=requests.get(_url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* json데이터는 meta, data로 구분해서 만들어져 있슴\n",
    "* data는 52252건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'meta', u'data']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52252\n"
     ]
    }
   ],
   "source": [
    "_jsonList=_json['data']\n",
    "print len(_jsonList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " u'5DC7F285-052B-4739-8DC3-62827014A4CD',\n",
       " 1,\n",
       " 1425450997,\n",
       " u'714909',\n",
       " 1425450997,\n",
       " u'714909',\n",
       " u'{\\n}',\n",
       " u'2013',\n",
       " u'GAVIN',\n",
       " u'ST LAWRENCE',\n",
       " u'M',\n",
       " u'9']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_json['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* list to spark dataFrame\n",
    "    * schema를 정하지 않으면 없이 생성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52252"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df=sqlCtx.createDataFrame(_json['data'])\n",
    "_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* schema를 정하지 않았으므로 임의로 생성된 속성을 사용하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      " |-- _4: long (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: long (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: string (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      " |-- _13: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+----------+------+----------+------+---+----+-----+-----------+---+---+\n",
      "| _1|                  _2| _3|        _4|    _5|        _6|    _7| _8|  _9|  _10|        _11|_12|_13|\n",
      "+---+--------------------+---+----------+------+----------+------+---+----+-----+-----------+---+---+\n",
      "|  1|5DC7F285-052B-473...|  1|1425450997|714909|1425450997|714909|{\n",
      "}|2013|GAVIN|ST LAWRENCE|  M|  9|\n",
      "| 82|43E9414D-9BE0-456...| 82|1425450997|714909|1425450997|714909|{\n",
      "}|2013|GAVIN|    SUFFOLK|  M| 54|\n",
      "+---+--------------------+---+----------+------+----------+------+---+----+-----+-----------+---+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.filter(_df['_10'] == u'GAVIN').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* select를 사용해보자?\n",
    "    * pivotTable??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    _10|\n",
      "+-------+\n",
      "|  HENRY|\n",
      "| LESLIE|\n",
      "|  ALICE|\n",
      "|MIRANDA|\n",
      "|    EVA|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.registerTempTable(\"babyNames\")\n",
    "sqlCtx.sql(\"select distinct(_10) from babyNames\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.txt\n",
    "Michael, 29\n",
    "Andy, 30\n",
    "Justin, 19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "lines = sc.textFile(\"/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "schemaPeople = sqlCtx.createDataFrame(people)\n",
    "schemaPeople.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = sqlCtx.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "teenNames = teenagers.map(lambda p: \"Name: \" + p.name)\n",
    "for teenName in teenNames.collect():\n",
    "  print(teenName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "sqlCtx.createDataFrame(people,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.withColumnRenamed('1','label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-6: spark sql uber csv\n",
    "\n",
    "https://github.com/tmcgrath/spark-with-python-course/blob/master/Spark-SQL-CSV-with-Python.ipynb\n",
    "\n",
    "\n",
    "\n",
    "* fivethirtyeight\n",
    "    * git clone https://github.com/fivethirtyeight/uber-tlc-foil-response.git\n",
    "        daily Uber trip statistics in January and February 2015\n",
    "        ```\n",
    "        dispatching_base_number\tdate\tactive_vehicles\ttrips\n",
    "        B02512\t1/1/2015\t190\t1132\n",
    "        B02765\t1/1/2015\t225\t1765\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_home=os.path.join(home,\"Code/git/else/uber-tlc-foil-response\")\n",
    "filePath=os.path.join(data_home,\"Uber-Jan-Feb-FOIL.csv\")\n",
    "\n",
    "_fub = sc.textFile(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_fub)\n",
    "_fub.count()\n",
    "_fub.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* csv는 comma seperated 형식이므로, ','로 분리\n",
    "* 첫번째 열에서 key값을 추출한다 (header값 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_dub = _fub.map(lambda line: line.split(\",\"))\n",
    "\n",
    "type(_dub)\n",
    "\n",
    "_row0keys=_dub.map(lambda row: row[0]).distinct().collect()\n",
    "\n",
    "print _row0keys\n",
    "\n",
    "_dub.filter(lambda row: \"B02512\" in row).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* B02512인 경우, trips가 2000보다 큰 레코드 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_dub.filter(lambda row: \"B02512\" in row).filter(lambda row: int(row[3])>2000).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* header는 속성 명을 가지고 있다. 이를 제외하면 전체 갯수에서 1개를 뺀 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_noheader = _fub.filter(lambda line: \"base\" not in line).map(lambda line:line.split(\",\"))\n",
    "_noheader.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduceByKey - key별로 value를 합쳐서 결과 -> 아래는 a,3 b,2\n",
    "```\n",
    "(\"a\", 1)\n",
    "(\"b\", 1)\n",
    "(\"a\", 1)\n",
    "(\"a\", 1)\n",
    "(\"b\", 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_noheader.map(lambda x: (x[0], int(x[3]))).reduceByKey(lambda k,v: k + v).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* saving\n",
    "    ```\n",
    "    rddOfStrings.saveAsTextFile(\"out.txt\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.6: DataFrame\n",
    "\n",
    "http://www.cs.sfu.ca/CourseCentral/732/ggbaker/spark-sql.html\n",
    "\n",
    "\n",
    "* Data Frame은 DB 테이블\n",
    "    * MLib의 입력 데이터로 사용할 수 있다.\n",
    "        * 입력 데이터는 1) Spark RDDs 또는 2) DataFrame을 사용할 수 있다.\n",
    "        * 기본은 Data Frames (Pandas dataframe) (Spark 3.0 이후 DataFrame API)\n",
    "\n",
    "Pipeline | 설명 | 예\n",
    "----------|----------|----------\n",
    "DataFrame | text, feature vectors, true labels, and predictions.\n",
    "Transformer | DataFrame into another DataFrame | Transformer.transform()\n",
    "Estimator | fit on a DataFrame to produce a TransformerPipeline | Estimator.fit()\n",
    "Pipeline | chains multiple Transformers and Estimators together\n",
    "Parameter | a common API for specifying parameters. | ParamMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 기능\n",
    "\n",
    "기능 | 예제\n",
    "-------|-------\n",
    "json 읽기 | sqlContext.read.json(\"employee.json\")\n",
    "data 보기 | dfs.show()\n",
    "schema | dfs.printSchema()\n",
    "select | dfs.select(\"name\").show()\n",
    "filter | dfs.filter(dfs(\"age\") > 23).show()\n",
    "groupBy | dfs.groupBy(\"age\").count().show()\n",
    "select | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spar DataFrame vs Pandas 의 비교\n",
    "\n",
    "DataFrame | Spark | Pandas\n",
    "-------|-------|-------\n",
    "csv file | map split(',') | read_csv()\n",
    "| show() | head(), tail()\n",
    "data types | 맞게 추정 | 모두 strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('name', 'height')\n",
    "rows = [Person('kim', 170), Person('lee', 175), Person('lim', 180),]\n",
    "#rowsRdd = sc.parallelize(rows)\n",
    "#rowsDf = sqlCtx.createDataFrame(rowsRdd)\n",
    "\n",
    "rowsDF=sqlCtx.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rowsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rowsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|height|\n",
      "+----+------+\n",
      "| kim|   170|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowsDF.where(rowsDF.height < 175)\\\n",
    "    .select([rowsDF.name, rowsDF.height]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|height|max(height)|\n",
      "+------+-----------+\n",
      "|   170|        170|\n",
      "|   175|        175|\n",
      "|   180|        180|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowsDF.groupby(rowsDF.height).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.7 Hello Statistics\n",
    "\n",
    "* mllib.stat.Statistics\n",
    "* 기본 통계\n",
    "* 가설 검증\n",
    "* 상관관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-7:  Kolmogorov-Smirnov 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.841344746068543 \n",
      "pValue = 5.06089025353873E-6 \n",
      "Very strong presumption against null hypothesis: Sample follows theoretical distribution.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "parallelData = sc.parallelize([1.0, 2.0, 5.0, 4.0, 3.0, 3.3, 5.5])\n",
    "\n",
    "# run a KS test for the sample versus a standard normal distribution\n",
    "testResult = Statistics.kolmogorovSmirnovTest(parallelData, \"norm\", 0, 1)\n",
    "print(testResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-8:  무작위 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, randn\n",
    " # Create a DataFrame with one int column and 10 rows.\n",
    "df = sqlCtx.range(0, 10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0|0.41371264720975787|  0.5888539012978773|\n",
      "|  1| 0.1982919638208397| 0.06157382353970104|\n",
      "|  2|0.12030715258495939|  1.0854146699817222|\n",
      "|  3|0.44292918521277047| -0.4798519469521663|\n",
      "|  4| 0.8898784253886249| -0.8820294772950535|\n",
      "|  5| 0.2731073068483362|-0.15116027592854422|\n",
      "|  6|   0.87079354700073|-0.27674189870783683|\n",
      "|  7|0.27149331793166864|-0.18575112254167045|\n",
      "|  8| 0.6037143578435027|   0.734722467897308|\n",
      "|  9| 0.1435668838975337|-0.30123700668427145|\n",
      "+---+-------------------+--------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|                id|\n",
      "+-------+------------------+\n",
      "|  count|                10|\n",
      "|   mean|               4.5|\n",
      "| stddev|3.0276503540974917|\n",
      "|    min|                 0|\n",
      "|    max|                 9|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", rand(seed=10).alias(\"uniform\"), randn(seed=27).alias(\"normal\")).show()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.15104231217\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "df = sqlCtx.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "print df.stat.corr('rand1', 'rand2')\n",
    "print df.stat.corr('id', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| name|   item|\n",
      "+-----+-------+\n",
      "|Alice|   milk|\n",
      "|  Bob|  bread|\n",
      "| Mike| butter|\n",
      "|Alice| apples|\n",
      "|  Bob|oranges|\n",
      "| Mike|   milk|\n",
      "|Alice|  bread|\n",
      "|  Bob| butter|\n",
      "| Mike| apples|\n",
      "|Alice|oranges|\n",
      "+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"Alice\", \"Bob\", \"Mike\"]\n",
    "items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\n",
    "df = sqlCtx.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = sqlCtx.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\n",
    "print df.show(10)\n",
    "freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 문제 S-9: 정량데이터 분석\n",
    "\n",
    "* 정량데이터 처리\n",
    "* 머신러닝 입력데이터 형식\n",
    "\n",
    "api | dataframe api | rdd api\n",
    "----------|----------|----------\n",
    "데이터 타잎 | label, feature vectors | Labeled Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### train 데이터 만들기\n",
    "\n",
    "* category 데이터로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlCtx.createDataFrame(\n",
    "    [\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'true', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'good'],\n",
    "        ['Yes','middle', 'true', 'true', 'good'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'excellent'],\n",
    "        ['No','old', 'false', 'false', 'fair'],\n",
    "    ],\n",
    "    ['cls','age','f1','f2','f3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 컬럼명 변경을 위해 일부러 labels로 한다. 뒤에 label로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"labels\")\n",
    "model=labelIndexer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----+---------+------+\n",
      "|cls|   age|   f1|   f2|       f3|labels|\n",
      "+---+------+-----+-----+---------+------+\n",
      "| No| young|false|false|     fair|   1.0|\n",
      "| No| young|false|false|     good|   1.0|\n",
      "|Yes| young| true|false|     good|   0.0|\n",
      "|Yes| young| true| true|     fair|   0.0|\n",
      "| No| young|false|false|     fair|   1.0|\n",
      "| No|middle|false|false|     fair|   1.0|\n",
      "| No|middle|false|false|     good|   1.0|\n",
      "|Yes|middle| true| true|     good|   0.0|\n",
      "|Yes|middle|false| true|excellent|   0.0|\n",
      "|Yes|middle|false| true|excellent|   0.0|\n",
      "|Yes|   old|false| true|excellent|   0.0|\n",
      "|Yes|   old|false| true|     good|   0.0|\n",
      "|Yes|   old| true|false|     good|   0.0|\n",
      "|Yes|   old| true|false|excellent|   0.0|\n",
      "| No|   old|false|false|     fair|   1.0|\n",
      "+---+------+-----+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"age\", outputCol=\"att1\")\n",
    "model=labelIndexer.fit(df1)\n",
    "df2=model.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"f1\", outputCol=\"att2\")\n",
    "model=labelIndexer.fit(df2)\n",
    "df3=model.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"f2\", outputCol=\"att3\")\n",
    "model=labelIndexer.fit(df3)\n",
    "df4=model.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"f3\", outputCol=\"att4\")\n",
    "model=labelIndexer.fit(df4)\n",
    "df5=model.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      " |-- att1: double (nullable = true)\n",
      " |-- att2: double (nullable = true)\n",
      " |-- att3: double (nullable = true)\n",
      " |-- att4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----+---------+------+----+----+----+----+\n",
      "|cls|   age|   f1|   f2|       f3|labels|att1|att2|att3|att4|\n",
      "+---+------+-----+-----+---------+------+----+----+----+----+\n",
      "| No| young|false|false|     fair|   1.0| 0.0| 0.0| 0.0| 1.0|\n",
      "| No| young|false|false|     good|   1.0| 0.0| 0.0| 0.0| 0.0|\n",
      "|Yes| young| true|false|     good|   0.0| 0.0| 1.0| 0.0| 0.0|\n",
      "|Yes| young| true| true|     fair|   0.0| 0.0| 1.0| 1.0| 1.0|\n",
      "| No| young|false|false|     fair|   1.0| 0.0| 0.0| 0.0| 1.0|\n",
      "| No|middle|false|false|     fair|   1.0| 1.0| 0.0| 0.0| 1.0|\n",
      "| No|middle|false|false|     good|   1.0| 1.0| 0.0| 0.0| 0.0|\n",
      "|Yes|middle| true| true|     good|   0.0| 1.0| 1.0| 1.0| 0.0|\n",
      "|Yes|middle|false| true|excellent|   0.0| 1.0| 0.0| 1.0| 2.0|\n",
      "|Yes|middle|false| true|excellent|   0.0| 1.0| 0.0| 1.0| 2.0|\n",
      "|Yes|   old|false| true|excellent|   0.0| 2.0| 0.0| 1.0| 2.0|\n",
      "|Yes|   old|false| true|     good|   0.0| 2.0| 0.0| 1.0| 0.0|\n",
      "|Yes|   old| true|false|     good|   0.0| 2.0| 1.0| 0.0| 0.0|\n",
      "|Yes|   old| true|false|excellent|   0.0| 2.0| 1.0| 0.0| 2.0|\n",
      "| No|   old|false|false|     fair|   1.0| 2.0| 0.0| 0.0| 1.0|\n",
      "+---+------+-----+-----+---------+------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"att1\",\"att2\",\"att3\",\"att4\"],outputCol=\"features\")\n",
    "df6 = va.transform(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7=df6.withColumnRenamed('labels','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- att1: double (nullable = true)\n",
      " |-- att2: double (nullable = true)\n",
      " |-- att3: double (nullable = true)\n",
      " |-- att4: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDf=df7.select('label','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|        (4,[],[])|\n",
      "|  0.0|    (4,[1],[1.0])|\n",
      "|  0.0|[0.0,1.0,1.0,1.0]|\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|[1.0,0.0,0.0,1.0]|\n",
      "|  1.0|    (4,[0],[1.0])|\n",
      "|  0.0|[1.0,1.0,1.0,0.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,2.0]|\n",
      "|  1.0|[2.0,0.0,0.0,1.0]|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD LabelPoint는 df에서 컬럼을 선택해서 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "trainRdd = trainDf.map(lambda row: LabeledPoint(row.label,row.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, (4,[3],[1.0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* udf\n",
    "    * 함수, 반환타잎 (없으면 StringType을 기본으로)\n",
    "* ?결과 show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.linalg import Vectors,VectorUDT\n",
    "myudf=udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "_trainDf2=trainDf.withColumn('dvf',myudf(trainDf.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- dvf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_trainDf2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### machine learning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|        (4,[],[])|\n",
      "|  0.0|    (4,[1],[1.0])|\n",
      "|  0.0|[0.0,1.0,1.0,1.0]|\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|[1.0,0.0,0.0,1.0]|\n",
      "|  1.0|    (4,[0],[1.0])|\n",
      "|  0.0|[1.0,1.0,1.0,0.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,2.0]|\n",
      "|  1.0|[2.0,0.0,0.0,1.0]|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pyspark.ml.classification.LogisticRegression\n",
    "    * 2진 분류만 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.50705810019,-5.31916107407,-5.04694958332,-0.351455356638]\n",
      "3.2822908185\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "model1 = lr.fit(trainDf)\n",
    "print model1.coefficients\n",
    "print model1.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "test0 = sc.parallelize([Row(features=Vectors.dense(2,0,0,1))]).toDF()\n",
    "result = model1.transform(test0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mllib - lr test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "lrm = LogisticRegressionWithSGD.train(trainRdd, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict([1.0,0.0,1.1,1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* mllib - svm (ml은 없다!?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "svm = SVMWithSGD.train(trainRdd, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.predict([1.0,0.0,1.1,1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ml - Data frame pipeline으로 decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------------+\n",
      "|prediction|_label|        _features|\n",
      "+----------+------+-----------------+\n",
      "|       1.0|   1.0|    (4,[3],[1.0])|\n",
      "|       1.0|   1.0|        (4,[],[])|\n",
      "|       0.0|   0.0|    (4,[1],[1.0])|\n",
      "|       0.0|   0.0|[0.0,1.0,1.0,1.0]|\n",
      "|       1.0|   1.0|    (4,[3],[1.0])|\n",
      "|       1.0|   1.0|[1.0,0.0,0.0,1.0]|\n",
      "|       1.0|   1.0|    (4,[0],[1.0])|\n",
      "|       0.0|   0.0|[1.0,1.0,1.0,0.0]|\n",
      "|       0.0|   0.0|[1.0,0.0,1.0,2.0]|\n",
      "|       0.0|   0.0|[1.0,0.0,1.0,2.0]|\n",
      "|       0.0|   0.0|[2.0,0.0,1.0,2.0]|\n",
      "|       0.0|   0.0|[2.0,0.0,1.0,0.0]|\n",
      "|       0.0|   0.0|[2.0,1.0,0.0,0.0]|\n",
      "|       0.0|   0.0|[2.0,1.0,0.0,2.0]|\n",
      "|       1.0|   1.0|[2.0,0.0,0.0,1.0]|\n",
      "+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "li1 = StringIndexer(inputCol=\"cls\", outputCol=\"_label\")\n",
    "li2 = StringIndexer(inputCol=\"age\", outputCol=\"att1\")\n",
    "li3 = StringIndexer(inputCol=\"f1\", outputCol=\"att2\")\n",
    "li4 = StringIndexer(inputCol=\"f2\", outputCol=\"att3\")\n",
    "li5 = StringIndexer(inputCol=\"f3\", outputCol=\"att4\")\n",
    "va = VectorAssembler(inputCols=[\"att1\",\"att2\",\"att3\",\"att4\"],outputCol=\"_features\")\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"_label\", featuresCol=\"_features\")\n",
    "pipeline = Pipeline(stages=[li1,li2,li3,li4,li5,va,dt])\n",
    "model = pipeline.fit(df)\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"_label\", \"_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ml - bayesian\n",
    "    * labeled point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.530628251062,-0.887303195001]\n",
      "DenseMatrix([[-1.07044141, -1.76358859, -1.60943791, -1.25276297],\n",
      "             [-0.87546874, -2.48490665, -2.48490665, -0.87546874]])\n"
     ]
    }
   ],
   "source": [
    "print model.pi\n",
    "print model.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r=model.transform(test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "test0 = sc.parallelize([Row(features=Vectors.dense([1.0,0.0,1.1,1.2]))]).toDF()\n",
    "result = model.transform(test0).head()\n",
    "result.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* mllib - bayesian (spark 제공 자료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parseLine(line):\n",
    "    parts = line.split(',')\n",
    "    label = float(parts[0])\n",
    "    features = Vectors.dense([float(x) for x in parts[1].split(' ')])\n",
    "    print features\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "data = sc.textFile('/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/data/mllib/sample_naive_bayes_data.txt').map(parseLine)\n",
    "\n",
    "# Split data aproximately into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NaiveBayes.train(training, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "* 위에서 내가 가공한 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* trainRDD에 마이너스 값이 있으면 nok?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-10: 텍스트 분석\n",
    "\n",
    "* bag of words 표현\n",
    "    * 문서는 단어로 구성된다.\n",
    "    * 단어의 순서는 의미를 가지지 않는다.\n",
    "    \n",
    "구분 | 설명 | 예\n",
    "----------|----------|----------|\n",
    "corpus | 문서 집합 | \"why she had to go\", \"where she have to go\"\n",
    "document | 레코드 | \"why she had to go\"\n",
    "vocabularay | 중복없는 단어 집합 | \"why\",\"she\",\"had\",\"to\",\"go\",\"where\",\"have\"\n",
    "word vector | 이진수로 단어 유무를 나타낸다 | [1,1,1,1,1,0,0],[0,1,0,1,1,1,1]\n",
    "\n",
    "* word vector의 정보화\n",
    "    * Tokenizer\n",
    "    * HashingTF\n",
    "    ```\n",
    "    hashingTF=HashingTF()\n",
    "    tf=hashingTF.transform(_rdd)\n",
    "    ```\n",
    "\n",
    "    * TF-IDF Term Frequency-Inverse Document Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train 데이터 만들기\n",
    "\n",
    "* 입력은 string으로 한다.\n",
    "    * Array 입력은 맞지 않는다.\n",
    "        ```\n",
    "        [0,['my','dog','has','flea','problems','help','please']]\n",
    "        또는\n",
    "        [0,['my dog has flea problems. help please.']]\n",
    "        ```\n",
    "    * 한글은 unicode로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.createDataFrame(\n",
    "    [\n",
    "        [0,'my dog has flea problems. help please.'],\n",
    "        [1,'maybe not take him to dog park stupid'],\n",
    "        [0,'my dalmation is so cute. I love him'],\n",
    "        [1,'stop posting stupid worthless garbage'],\n",
    "        [0,'mr licks ate my steak how to stop him'],\n",
    "        [1,'quit buying worthless dog food stupid'],\n",
    "        [0,u'우리 강아지 벌레 있어요 도와주세요'],\n",
    "        [0,u'우리 강아지 귀여워 너 사랑해'],\n",
    "        [1,u'강아지 공원 가지마 바보같이'],\n",
    "        [1,u'강아지 음식 구매 마세요 바보같이']\n",
    "    ],\n",
    "    ['cls','sent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lab: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer\n",
    "\n",
    "* transform으로 새로운 컬럼을 추가해서 dataframe을 추가한다.\n",
    "    * label, feature + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokDf = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* words는 array로 만들어졌다. 요소가 string이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(cls=0, sent=u'my dog has flea problems. help please.')\n",
      "Row(cls=1, sent=u'maybe not take him to dog park stupid')\n",
      "Row(cls=0, sent=u'my dalmation is so cute. I love him')\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"cls\", \"sent\").take(3):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "|cls|                sent|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|my dog has flea p...|[my, dog, has, fl...|\n",
      "|  1|maybe not take hi...|[maybe, not, take...|\n",
      "|  0|my dalmation is s...|[my, dalmation, i...|\n",
      "|  1|stop posting stup...|[stop, posting, s...|\n",
      "|  0|mr licks ate my s...|[mr, licks, ate, ...|\n",
      "|  1|quit buying worth...|[quit, buying, wo...|\n",
      "|  0| 우리 강아지 벌레 있어요 도와주세요|[우리, 강아지, 벌레, 있어요...|\n",
      "|  0|    우리 강아지 귀여워 너 사랑해|[우리, 강아지, 귀여워, 너,...|\n",
      "|  1|     강아지 공원 가지마 바보같이|[강아지, 공원, 가지마, 바보같이]|\n",
      "|  1|  강아지 음식 구매 마세요 바보같이|[강아지, 음식, 구매, 마세요...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexTokenizer\n",
    "\n",
    "* 한글에는 \\w 패턴이 적용되지 않는다.\n",
    "* 공백 \\s 패턴을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "|cls|                sent|            wordsReg|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|my dog has flea p...|[my, dog, has, fl...|\n",
      "|  1|maybe not take hi...|[maybe, not, take...|\n",
      "|  0|my dalmation is s...|[my, dalmation, i...|\n",
      "|  1|stop posting stup...|[stop, posting, s...|\n",
      "|  0|mr licks ate my s...|[mr, licks, ate, ...|\n",
      "|  1|quit buying worth...|[quit, buying, wo...|\n",
      "|  0| 우리 강아지 벌레 있어요 도와주세요|[우리, 강아지, 벌레, 있어요...|\n",
      "|  0|    우리 강아지 귀여워 너 사랑해|[우리, 강아지, 귀여워, 너,...|\n",
      "|  1|     강아지 공원 가지마 바보같이|[강아지, 공원, 가지마, 바보같이]|\n",
      "|  1|  강아지 음식 구매 마세요 바보같이|[강아지, 음식, 구매, 마세요...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")\n",
    "regDf=re.transform(df)\n",
    "regDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"words\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords=list()\n",
    "\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_4a9da39097e84c36b177"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can cannot cant co con could couldnt cry de describe detail do done down due during each eg eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen fify fill find fire first five for former formerly forty found four from front full further get give go had has hasnt have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred i ie if in inc indeed interest into is it its itself keep last latter latterly least less ltd made many may me meanwhile might mill mine more moreover most mostly move much must my myself name namely neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off often on once one only onto or other others otherwise our ours ourselves out over own part per perhaps please put rather re same see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under until up upon us very via was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you your yours yourself yourselves 나 너 우리\n"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한글의 stop words '너','우리'가 제거되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "|cls|                sent|               words|             nostops|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|my dog has flea p...|[my, dog, has, fl...|[dog, flea, probl...|\n",
      "|  1|maybe not take hi...|[maybe, not, take...|[maybe, dog, park...|\n",
      "|  0|my dalmation is s...|[my, dalmation, i...|[dalmation, cute....|\n",
      "|  1|stop posting stup...|[stop, posting, s...|[stop, posting, s...|\n",
      "|  0|mr licks ate my s...|[mr, licks, ate, ...|[mr, licks, ate, ...|\n",
      "|  1|quit buying worth...|[quit, buying, wo...|[quit, buying, wo...|\n",
      "|  0| 우리 강아지 벌레 있어요 도와주세요|[우리, 강아지, 벌레, 있어요...|[강아지, 벌레, 있어요, 도와...|\n",
      "|  0|    우리 강아지 귀여워 너 사랑해|[우리, 강아지, 귀여워, 너,...|     [강아지, 귀여워, 사랑해]|\n",
      "|  1|     강아지 공원 가지마 바보같이|[강아지, 공원, 가지마, 바보같이]|[강아지, 공원, 가지마, 바보같이]|\n",
      "|  1|  강아지 음식 구매 마세요 바보같이|[강아지, 음식, 구매, 마세요...|[강아지, 음식, 구매, 마세요...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(tokDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "* Term frequency-inverse document frequency (TF-IDF)\n",
    "* tokenizer하고 나서 사용해야 함.\n",
    "* tfidf를 계산하는 방법은 1) hashingTF, 2) IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vocabulary = \"a list of words I want to look for in the documents\".split()\n",
    "vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', \n",
    "                 stop_words='english')\n",
    "vect.fit(vocabulary)\n",
    "tfidf=vect.transform(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashTF = HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=50)\n",
    "hashDf = hashTF.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "|cls|                sent|               words|                hash|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|my dog has flea p...|[my, dog, has, fl...|(50,[0,16,34,35,4...|\n",
      "|  1|maybe not take hi...|[maybe, not, take...|(50,[0,7,8,17,36,...|\n",
      "|  0|my dalmation is s...|[my, dalmation, i...|(50,[0,5,8,20,26,...|\n",
      "|  1|stop posting stup...|[stop, posting, s...|(50,[1,17,18,39,4...|\n",
      "|  0|mr licks ate my s...|[mr, licks, ate, ...|(50,[0,4,7,8,14,4...|\n",
      "|  1|quit buying worth...|[quit, buying, wo...|(50,[17,38,39,41,...|\n",
      "|  0| 우리 강아지 벌레 있어요 도와주세요|[우리, 강아지, 벌레, 있어요...|(50,[16,20,31,34,...|\n",
      "|  0|    우리 강아지 귀여워 너 사랑해|[우리, 강아지, 귀여워, 너,...|(50,[16,20,30,31,...|\n",
      "|  1|     강아지 공원 가지마 바보같이|[강아지, 공원, 가지마, 바보같이]|(50,[0,31,33,37],...|\n",
      "|  1|  강아지 음식 구매 마세요 바보같이|[강아지, 음식, 구매, 마세요...|(50,[5,14,30,31,3...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                hash|                 idf|\n",
      "+--------------------+--------------------+\n",
      "|(50,[0,16,34,35,4...|(50,[0,16,34,35,4...|\n",
      "|(50,[0,7,8,17,36,...|(50,[0,7,8,17,36,...|\n",
      "|(50,[0,5,8,20,26,...|(50,[0,5,8,20,26,...|\n",
      "|(50,[1,17,18,39,4...|(50,[1,17,18,39,4...|\n",
      "|(50,[0,4,7,8,14,4...|(50,[0,4,7,8,14,4...|\n",
      "|(50,[17,38,39,41,...|(50,[17,38,39,41,...|\n",
      "|(50,[16,20,31,34,...|(50,[16,20,31,34,...|\n",
      "|(50,[16,20,30,31,...|(50,[16,20,30,31,...|\n",
      "|(50,[0,31,33,37],...|(50,[0,31,33,37],...|\n",
      "|(50,[5,14,30,31,3...|(50,[5,14,30,31,3...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfDf.select('hash','idf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- hash: vector (nullable = true)\n",
      " |-- idf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(cls=0, hash=SparseVector(50, {0: 1.0, 16: 2.0, 34: 1.0, 35: 1.0, 44: 1.0, 48: 1.0}))\n",
      "Row(cls=1, hash=SparseVector(50, {0: 1.0, 7: 1.0, 8: 1.0, 17: 1.0, 36: 1.0, 39: 1.0, 41: 1.0, 44: 1.0}))\n",
      "Row(cls=0, hash=SparseVector(50, {0: 1.0, 5: 1.0, 8: 2.0, 20: 1.0, 26: 1.0, 29: 1.0, 31: 1.0}))\n",
      "Row(cls=1, hash=SparseVector(50, {1: 1.0, 17: 1.0, 18: 1.0, 39: 1.0, 44: 1.0}))\n",
      "Row(cls=0, hash=SparseVector(50, {0: 1.0, 4: 1.0, 7: 1.0, 8: 1.0, 14: 1.0, 43: 1.0, 44: 2.0, 46: 1.0}))\n",
      "Row(cls=1, hash=SparseVector(50, {17: 1.0, 38: 1.0, 39: 1.0, 41: 1.0, 44: 2.0}))\n",
      "Row(cls=0, hash=SparseVector(50, {16: 1.0, 20: 1.0, 31: 1.0, 34: 1.0, 38: 1.0}))\n",
      "Row(cls=0, hash=SparseVector(50, {16: 1.0, 20: 1.0, 30: 1.0, 31: 1.0, 41: 1.0}))\n",
      "Row(cls=1, hash=SparseVector(50, {0: 1.0, 31: 1.0, 33: 1.0, 37: 1.0}))\n",
      "Row(cls=1, hash=SparseVector(50, {5: 1.0, 14: 1.0, 30: 1.0, 31: 1.0, 37: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "for e in idfDf.select(\"cls\",\"hash\").take(12):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "\n",
    "* Word2Vec\n",
    "    * see wikipedia https://en.wikipedia.org/wiki/Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([-0.0514, 0.0162, 0.0288]))\n",
      "Row(w2v=DenseVector([0.0043, -0.0537, 0.0231]))\n",
      "Row(w2v=DenseVector([-0.0225, 0.0084, -0.0055]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"w2v\")\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)\n",
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "* word vectors를 생성한다.\n",
    "\n",
    "* tokenize하고 나서 사용\n",
    "* 결과는 sparse vector\n",
    "* minDF\n",
    "    * 소수점은 비율, 사용된 문서 수/전체 문서 수)\n",
    "    * 정수는 사용된 문서 수, 단어가 몇 개의 문서에 사용되어야 하는지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sklearn\n",
    "    * fit_transform -> bag of words\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{u'duke': 1, u'basketball': 0, u'lost': 4, u'played': 5, u'game': 2, u'unc': 7, u'in': 3, u'the': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game'\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "print vectorizer.fit_transform(corpus).todense()\n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               words|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[my, dog, has, fl...|[dog, flea, probl...|(30,[1,11,16,17,2...|\n",
      "|[maybe, not, take...|[maybe, dog, park...|(30,[1,2,8,13],[1...|\n",
      "|[my, dalmation, i...|[dalmation, cute....|(30,[14,19],[1.0,...|\n",
      "|[stop, posting, s...|[stop, posting, s...|(30,[2,3,5,18,26]...|\n",
      "|[mr, licks, ate, ...|[mr, licks, ate, ...|(30,[3,15,27,28],...|\n",
      "|[quit, buying, wo...|[quit, buying, wo...|(30,[1,2,5,22,29]...|\n",
      "|[우리, 강아지, 벌레, 있어요...|[강아지, 벌레, 있어요, 도와...|(30,[0,10,24,25],...|\n",
      "|[우리, 강아지, 귀여워, 너,...|     [강아지, 귀여워, 사랑해]|(30,[0,7],[1.0,1.0])|\n",
      "|[강아지, 공원, 가지마, 바보같이]|[강아지, 공원, 가지마, 바보같이]|(30,[0,4,9,20],[1...|\n",
      "|[강아지, 음식, 구매, 마세요...|[강아지, 음식, 구매, 마세요...|(30,[0,4,6,12,21]...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30,minDF=1.0)\n",
    "cvModel = cv.fit(stopDf)\n",
    "cvDf = cvModel.transform(stopDf)\n",
    "\n",
    "cvDf.collect()\n",
    "cvDf.select('words','nostops','cv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cv=SparseVector(30, {1: 1.0, 11: 1.0, 16: 1.0, 17: 1.0, 23: 1.0})),\n",
       " Row(cv=SparseVector(30, {1: 1.0, 2: 1.0, 8: 1.0, 13: 1.0})),\n",
       " Row(cv=SparseVector(30, {14: 1.0, 19: 1.0})),\n",
       " Row(cv=SparseVector(30, {2: 1.0, 3: 1.0, 5: 1.0, 18: 1.0, 26: 1.0})),\n",
       " Row(cv=SparseVector(30, {3: 1.0, 15: 1.0, 27: 1.0, 28: 1.0})),\n",
       " Row(cv=SparseVector(30, {1: 1.0, 2: 1.0, 5: 1.0, 22: 1.0, 29: 1.0})),\n",
       " Row(cv=SparseVector(30, {0: 1.0, 10: 1.0, 24: 1.0, 25: 1.0})),\n",
       " Row(cv=SparseVector(30, {0: 1.0, 7: 1.0})),\n",
       " Row(cv=SparseVector(30, {0: 1.0, 4: 1.0, 9: 1.0, 20: 1.0})),\n",
       " Row(cv=SparseVector(30, {0: 1.0, 4: 1.0, 6: 1.0, 12: 1.0, 21: 1.0}))]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvDf.select('cv').take(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강아지 dog stupid stop 바보같이 worthless 구매 귀여워 maybe 가지마 있어요 help 음식 park love ate problems. flea posting cute. 공원 마세요 buying please. 도와주세요 벌레 garbage mr steak food\n"
     ]
    }
   ],
   "source": [
    "for v in cvModel.vocabulary:\n",
    "    print v,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼을 선택해서 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rdd로 변환\n",
    "    * label과 테스트용 1,1을 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'It is calm and sunny.'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd=df.map(lambda x:x.sent)\n",
    "_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, [1.0,1.0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "_rdd=df.map(lambda x:LabeledPoint(x.class,[1,1]))\n",
    "_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler\n",
    "\n",
    "* string할 수 없슴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(va=SparseVector(8, {0: 1.0, 3: 1.0, 7: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"label2\", \"cv\"],\n",
    "    outputCol=\"va\")\n",
    "\n",
    "output = assembler.transform(cvDf)\n",
    "print(output.select(\"va\").first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram\n",
    "\n",
    "* input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|label2|                sent|               words|              ngrams|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|     1|It is calm and su...|[it, is, calm, an...|[it is, is calm, ...|\n",
      "|     0|         feel gloomy|      [feel, gloomy]|       [feel gloomy]|\n",
      "|     1|           I am calm|       [i, am, calm]|     [i am, am calm]|\n",
      "|     0|      gloomy weather|   [gloomy, weather]|    [gloomy weather]|\n",
      "|     1|               나 행복해|            [나, 행복해]|             [나 행복해]|\n",
      "|     0|               너 불행해|            [너, 불행해]|             [너 불행해]|\n",
      "|     1|              우리 행복해|           [우리, 행복해]|            [우리 행복해]|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "\n",
      "Row(ngrams=[u'it is', u'is calm', u'calm and', u'and sunny.'], label2=1)\n",
      "Row(ngrams=[u'feel gloomy'], label2=0)\n",
      "Row(ngrams=[u'i am', u'am calm'], label2=1)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDf = ngram.transform(tokDf)\n",
    "ngramDf.show()\n",
    "for e in ngramDf.select(\"ngrams\", \"label2\").take(3):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "trainDf = sqlCtx.createDataFrame([\n",
    "    (0L, \"a b c d e spark\", 1.0),\n",
    "    (1L, \"b d\", 0.0),\n",
    "    (2L, \"spark f g h\", 1.0),\n",
    "    (3L, \"hadoop mapreduce\", 0.0)], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(trainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### udf\n",
    "\n",
    "* df의 행을 처리하는 함수\n",
    "* 함수명과 반환 값을 미리 정의함\n",
    "* withColumn과 같이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cv: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, sent=u'When I find myself in times of trouble', words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'], cv=SparseVector(30, {3: 1.0, 4: 1.0, 17: 1.0, 18: 1.0, 20: 1.0, 23: 1.0, 24: 1.0, 27: 1.0}))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장을 대문자로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    " \n",
    "def uppercase(s):\n",
    "    return s.upper()\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())\n",
    "newDF = cvDf.withColumn(\"upperSent\", upperUdf(cvDf.sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(upperSent=u'IT IS CALM AND SUNNY.')]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_a=newDF.select('upperSent')\n",
    "__a=_a.rdd.map(lambda x:x.split(' '))\n",
    "_a.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 벡터변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string, maturity: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "maturity_udf = udf(lambda age: \"adult\" if age >=18 else \"child\", StringType())\n",
    "\n",
    "# use Row\n",
    "df = sqlCtx.createDataFrame([{'name': 'Alice', 'age': 1}])\n",
    "df.withColumn(\"maturity\", maturity_udf(df.age))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ml test\n",
    "\n",
    "```\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "```\n",
    "\n",
    "```\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ml.decisiontree\n",
    "    * cvDf의 label은 IllegalArgumentException, StringIndexer로 변환해서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndexer\n",
    "\n",
    "* double로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"labels\")\n",
    "model=labelIndexer.fit(cvDf)\n",
    "trainDf2=model.transform(cvDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- nostops: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cv: vector (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [nok] udf를 사용할 수 있다. double로 안만들어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "toDoublefunc = udf(lambda x: x.DoubleType())\n",
    "trainDf3 = trainDf2.withColumn(\"_label\",toDoublefunc(trainDf2.cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- nostops: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cv: vector (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      " |-- _label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train df는 사전에 정해 놓은 컬럼명, 데이터타잎으로\n",
    "\n",
    "\n",
    "열 | 데이터타잎 | 예\n",
    "-----|-----|-----\n",
    "label | string | \n",
    "features | sparse or dense vectors | (7,[2,6],[1.0,1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일부를 선택할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_trainDf=trainDf2.select('labels','cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDf=trainDf2.withColumnRenamed('labels','label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* df 이름 변경하지 않고 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDf=trainDf.withColumnRenamed('cv','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- nostops: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "|cls|label|            features|\n",
      "+---+-----+--------------------+\n",
      "|  0|  0.0|(30,[1,11,16,17,2...|\n",
      "|  1|  1.0|(30,[1,2,8,13],[1...|\n",
      "|  0|  0.0|(30,[14,19],[1.0,...|\n",
      "|  1|  1.0|(30,[2,3,5,18,26]...|\n",
      "|  0|  0.0|(30,[3,15,27,28],...|\n",
      "|  1|  1.0|(30,[1,2,5,22,29]...|\n",
      "|  0|  0.0|(30,[0,10,24,25],...|\n",
      "|  0|  0.0|(30,[0,7],[1.0,1.0])|\n",
      "|  1|  1.0|(30,[0,4,9,20],[1...|\n",
      "|  1|  1.0|(30,[0,4,6,12,21]...|\n",
      "+---+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.select('cls','label','features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### machine learning test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = lr.fit(trainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lr pipeline\n",
    "    * 처음 df를 가지고 pipeline 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"label\")\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[labelIndexer,tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "model=dt.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.numNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(trainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mllib\n",
    "    * 위에 있는 trainDf를 사용해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "trainRdd = trainDf.map(lambda row: LabeledPoint(row.label,row.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.classification.NaiveBayesModel at 0x10b4fed90>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "NaiveBayes.train(trainRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weights=[0.176731539981,0.215696205714,0.404460423896,0.102108218995,0.36888244587,0.219717639591,0.151150601987,-0.131736079465,0.184742784305,0.217731843883,-0.0604148264235,-0.0534905795486,0.151150601987,0.184742784305,0.0,-0.0331654196387,-0.0534905795486,-0.0534905795486,0.135273638634,0.0,0.217731843883,0.151150601987,0.0844440009573,-0.0534905795486,-0.0604148264235,-0.0604148264235,0.135273638634,-0.0331654196387,-0.0331654196387,0.0844440009573], intercept=0.0)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "LinearRegressionWithSGD.train(trainRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "lrm = LogisticRegressionWithSGD.train(trainRdd, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "svm = SVMWithSGD.train(trainRdd, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-11: twitter 데이터 분석\n",
    "\n",
    "* 1-1 sklearn\n",
    "* 1-2 spark\n",
    "\n",
    "    * tf-idf\n",
    "    * KMeans\n",
    "    * MDS Multi-Dimensional Scaling (MDS)\n",
    "    * visualize\n",
    "\n",
    "* ibm직원이 tweet을 변환해서 mlib한 거 https://github.com/castanan/w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [nok] 현재 디렉토리 _tweet.json\n",
    "    * src/ds_twitter_3.py로 변경 (ds_twitter_3.json으로 저장)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#_jfname='_tweet.json'\n",
    "_jfname='src/ds_twitter_seoul_3.json'\n",
    "# read the entire file into a python array\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# remove the trailing \"\\n\" from each line\n",
    "data = map(lambda x: x.rstrip(), data)\n",
    "\n",
    "# each element of 'data' is an individual JSON object.\n",
    "# i want to convert it into an *array* of JSON objects\n",
    "# which, in and of itself, is one large JSON object\n",
    "# basically... add square brackets to the beginning\n",
    "# and end, and have all the individual business JSON objects\n",
    "# separated by a comma\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "# now, load it into pandas\n",
    "data_df = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contributors                    0\n",
       "coordinates                    14\n",
       "created_at                   2000\n",
       "entities                     2000\n",
       "extended_entities             343\n",
       "favorite_count               2000\n",
       "favorited                    2000\n",
       "geo                            14\n",
       "id                           2000\n",
       "id_str                       2000\n",
       "in_reply_to_screen_name        66\n",
       "in_reply_to_status_id          63\n",
       "in_reply_to_status_id_str      63\n",
       "in_reply_to_user_id            66\n",
       "in_reply_to_user_id_str        66\n",
       "is_quote_status              2000\n",
       "lang                         2000\n",
       "metadata                     2000\n",
       "place                          21\n",
       "possibly_sensitive            846\n",
       "quoted_status                   6\n",
       "quoted_status_id               14\n",
       "quoted_status_id_str           14\n",
       "retweet_count                2000\n",
       "retweeted                    2000\n",
       "retweeted_status             1586\n",
       "source                       2000\n",
       "text                         2000\n",
       "truncated                    2000\n",
       "user                         2000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>lang</th>\n",
       "      <th>metadata</th>\n",
       "      <th>place</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td> None</td>\n",
       "      <td>2016-11-25 01:09:18</td>\n",
       "      <td> {u'symbols': [], u'user_mentions': [{u'indices...</td>\n",
       "      <td> {u'media': [{u'source_user_id': 74629927357097...</td>\n",
       "      <td> 0</td>\n",
       "      <td> False</td>\n",
       "      <td> None</td>\n",
       "      <td> 801955891956236288</td>\n",
       "      <td> 801955891956236288</td>\n",
       "      <td> None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> False</td>\n",
       "      <td> en</td>\n",
       "      <td> {u'iso_language_code': u'en', u'result_type': ...</td>\n",
       "      <td> None</td>\n",
       "      <td>  0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td> None</td>\n",
       "      <td>2016-11-25 01:09:14</td>\n",
       "      <td> {u'symbols': [], u'user_mentions': [{u'indices...</td>\n",
       "      <td> {u'media': [{u'source_user_id': 3131144039, u'...</td>\n",
       "      <td> 0</td>\n",
       "      <td> False</td>\n",
       "      <td> None</td>\n",
       "      <td> 801955872410697728</td>\n",
       "      <td> 801955872410697728</td>\n",
       "      <td> None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> False</td>\n",
       "      <td> en</td>\n",
       "      <td> {u'iso_language_code': u'en', u'result_type': ...</td>\n",
       "      <td> None</td>\n",
       "      <td>  0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td> None</td>\n",
       "      <td>2016-11-25 01:09:09</td>\n",
       "      <td> {u'symbols': [], u'user_mentions': [], u'hasht...</td>\n",
       "      <td>                                               NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> False</td>\n",
       "      <td> None</td>\n",
       "      <td> 801955852798267393</td>\n",
       "      <td> 801955852798267392</td>\n",
       "      <td> None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> False</td>\n",
       "      <td> en</td>\n",
       "      <td> {u'iso_language_code': u'en', u'result_type': ...</td>\n",
       "      <td> None</td>\n",
       "      <td>  0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td> None</td>\n",
       "      <td>2016-11-25 01:09:06</td>\n",
       "      <td> {u'symbols': [], u'user_mentions': [], u'hasht...</td>\n",
       "      <td> {u'media': [{u'expanded_url': u'https://twitte...</td>\n",
       "      <td> 0</td>\n",
       "      <td> False</td>\n",
       "      <td> None</td>\n",
       "      <td> 801955840051781633</td>\n",
       "      <td> 801955840051781632</td>\n",
       "      <td> None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> False</td>\n",
       "      <td> en</td>\n",
       "      <td> {u'iso_language_code': u'en', u'result_type': ...</td>\n",
       "      <td> None</td>\n",
       "      <td>  0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td> None</td>\n",
       "      <td>2016-11-25 01:09:04</td>\n",
       "      <td> {u'symbols': [], u'user_mentions': [{u'indices...</td>\n",
       "      <td>                                               NaN</td>\n",
       "      <td> 0</td>\n",
       "      <td> False</td>\n",
       "      <td> None</td>\n",
       "      <td> 801955833424642048</td>\n",
       "      <td> 801955833424642048</td>\n",
       "      <td> None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td> False</td>\n",
       "      <td> th</td>\n",
       "      <td> {u'iso_language_code': u'th', u'result_type': ...</td>\n",
       "      <td> None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   contributors coordinates          created_at  \\\n",
       "0           NaN        None 2016-11-25 01:09:18   \n",
       "1           NaN        None 2016-11-25 01:09:14   \n",
       "2           NaN        None 2016-11-25 01:09:09   \n",
       "3           NaN        None 2016-11-25 01:09:06   \n",
       "4           NaN        None 2016-11-25 01:09:04   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {u'symbols': [], u'user_mentions': [{u'indices...   \n",
       "1  {u'symbols': [], u'user_mentions': [{u'indices...   \n",
       "2  {u'symbols': [], u'user_mentions': [], u'hasht...   \n",
       "3  {u'symbols': [], u'user_mentions': [], u'hasht...   \n",
       "4  {u'symbols': [], u'user_mentions': [{u'indices...   \n",
       "\n",
       "                                   extended_entities  favorite_count  \\\n",
       "0  {u'media': [{u'source_user_id': 74629927357097...               0   \n",
       "1  {u'media': [{u'source_user_id': 3131144039, u'...               0   \n",
       "2                                                NaN               0   \n",
       "3  {u'media': [{u'expanded_url': u'https://twitte...               0   \n",
       "4                                                NaN               0   \n",
       "\n",
       "  favorited   geo                  id              id_str  \\\n",
       "0     False  None  801955891956236288  801955891956236288   \n",
       "1     False  None  801955872410697728  801955872410697728   \n",
       "2     False  None  801955852798267393  801955852798267392   \n",
       "3     False  None  801955840051781633  801955840051781632   \n",
       "4     False  None  801955833424642048  801955833424642048   \n",
       "\n",
       "  in_reply_to_screen_name  in_reply_to_status_id  in_reply_to_status_id_str  \\\n",
       "0                    None                    NaN                        NaN   \n",
       "1                    None                    NaN                        NaN   \n",
       "2                    None                    NaN                        NaN   \n",
       "3                    None                    NaN                        NaN   \n",
       "4                    None                    NaN                        NaN   \n",
       "\n",
       "   in_reply_to_user_id  in_reply_to_user_id_str is_quote_status lang  \\\n",
       "0                  NaN                      NaN           False   en   \n",
       "1                  NaN                      NaN           False   en   \n",
       "2                  NaN                      NaN           False   en   \n",
       "3                  NaN                      NaN           False   en   \n",
       "4                  NaN                      NaN           False   th   \n",
       "\n",
       "                                            metadata place  \\\n",
       "0  {u'iso_language_code': u'en', u'result_type': ...  None   \n",
       "1  {u'iso_language_code': u'en', u'result_type': ...  None   \n",
       "2  {u'iso_language_code': u'en', u'result_type': ...  None   \n",
       "3  {u'iso_language_code': u'en', u'result_type': ...  None   \n",
       "4  {u'iso_language_code': u'th', u'result_type': ...  None   \n",
       "\n",
       "   possibly_sensitive      \n",
       "0                   0 ...  \n",
       "1                   0 ...  \n",
       "2                   0 ...  \n",
       "3                   0 ...  \n",
       "4                 NaN ...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     801955891956236288\n",
       "1     801955872410697728\n",
       "2     801955852798267393\n",
       "3     801955840051781633\n",
       "4     801955833424642048\n",
       "5     801955813715804160\n",
       "6     801955812302127104\n",
       "7     801955794631598080\n",
       "8     801955792853168128\n",
       "9     801955787476070404\n",
       "10    801955750666899456\n",
       "11    801955740197986304\n",
       "12    801955739140964352\n",
       "13    801955736448339968\n",
       "14    801955733025816576\n",
       "...\n",
       "1985    801931097713975296\n",
       "1986    801931077199818752\n",
       "1987    801931065329876996\n",
       "1988    801931056123260928\n",
       "1989    801931044148506624\n",
       "1990    801931008454967296\n",
       "1991    801931005762289664\n",
       "1992    801931001018486784\n",
       "1993    801930990893617152\n",
       "1994    801930973013311488\n",
       "1995    801930944290504704\n",
       "1996    801930935772086274\n",
       "1997    801930895523512320\n",
       "1998    801930887239634944\n",
       "1999    801930876875481088\n",
       "Name: id, Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf-idf\n",
    "    * 단계1: tf-idf model\n",
    "    * 단계2: fit\n",
    "\n",
    "* 아래는 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vocabulary = \"a list of words I want to look for in the documents\".split()\n",
    "\n",
    "vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', \n",
    "           stop_words='english', vocabulary=vocabulary)\n",
    "\n",
    "doc = \"some string I want to get tf-idf vector for\"\n",
    "doc_tfidf = vect.fit_transform([doc])\n",
    "print doc_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 10개만 연습으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                                    min_df=2, stop_words='english',\n",
    "                                    use_idf=True)\n",
    "_text=data_df['text'][20:30]\n",
    "X = vectorizer.fit_transform(_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 0)\t0.353553390593\n",
      "  (1, 3)\t0.353553390593\n",
      "  (1, 9)\t0.353553390593\n",
      "  (1, 13)\t0.353553390593\n",
      "  (1, 11)\t0.353553390593\n",
      "  (1, 10)\t0.353553390593\n",
      "  (1, 12)\t0.353553390593\n",
      "  (1, 8)\t0.353553390593\n",
      "  (2, 15)\t0.449404150908\n",
      "  (2, 7)\t0.449404150908\n",
      "  (2, 14)\t0.349561218439\n",
      "  (2, 6)\t0.349561218439\n",
      "  (2, 4)\t0.313925733382\n",
      "  (2, 5)\t0.313925733382\n",
      "  (2, 2)\t0.393175527292\n",
      "  (3, 0)\t0.301511344578\n",
      "  (3, 3)\t0.301511344578\n",
      "  (3, 9)\t0.301511344578\n",
      "  (3, 13)\t0.301511344578\n",
      "  (3, 11)\t0.301511344578\n",
      "  (3, 10)\t0.301511344578\n",
      "  (3, 12)\t0.301511344578\n",
      "  (3, 8)\t0.603022689156\n",
      "  (5, 16)\t0.439122351338\n",
      "  (5, 17)\t0.439122351338\n",
      "  (5, 1)\t0.439122351338\n",
      "  (5, 14)\t0.341563699106\n",
      "  (5, 6)\t0.341563699106\n",
      "  (5, 4)\t0.306743508955\n",
      "  (5, 5)\t0.306743508955\n",
      "  (6, 14)\t0.507796399128\n",
      "  (6, 4)\t0.456029870009\n",
      "  (6, 5)\t0.456029870009\n",
      "  (6, 2)\t0.571153510321\n",
      "  (8, 1)\t0.439122351338\n",
      "  (8, 15)\t0.439122351338\n",
      "  (8, 7)\t0.439122351338\n",
      "  (8, 14)\t0.341563699106\n",
      "  (8, 6)\t0.341563699106\n",
      "  (8, 4)\t0.306743508955\n",
      "  (8, 5)\t0.306743508955\n",
      "  (9, 16)\t0.479664492613\n",
      "  (9, 17)\t0.479664492613\n",
      "  (9, 6)\t0.373098700004\n",
      "  (9, 4)\t0.335063722302\n",
      "  (9, 5)\t0.335063722302\n",
      "  (9, 2)\t0.419649750509\n"
     ]
    }
   ],
   "source": [
    "print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 0.081\n",
      "Iteration  1, inertia 0.041\n",
      "Converged at iteration 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=7, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=7, init='k-means++', max_iter=100, n_init=1,verbose=1)\n",
    "\n",
    "#import sklearn.cluster.KMeans\n",
    "#km = sklearn.cluster.KMeans(n_clusters=7, init='k-means++', max_iter=100, n_init=1,verbose=1)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 위에서 읽은 데이터\n",
    "    * _jfname -> \\n제거 -> data\n",
    "    * data는 string이라서 parallelize해서 dataframe으로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_df=sqlCtx.jsonRDD(sc.parallelize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_my=sc.parallelize(\"I am not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[91] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_my.map(lambda x: x.replace(\"not\",\"NN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-12: 그래프 분석\n",
    "\n",
    "* todo\n",
    "    * theme\n",
    "        * link prediction - similarities\n",
    "        * communities - clustering\n",
    "        * opinion leader - PageRank\n",
    "        * graph partitioning\n",
    "    * google search\n",
    "    \"social graph analysis spark\"\n",
    "    to read - Social big data: Recent achievements and new challenges와 이 논문 cite한 논문들\n",
    "    \n",
    "* [ok] spark-defaults.conf\n",
    "    ```\n",
    "    spark.jars.packages=graphframes:graphframes:0.1.0-spark1.6\n",
    "    ```\n",
    "    \n",
    "* [ok in python; nok in ipython notebook] to use a Spark package 'GraphFrame'\n",
    "    * download graphframes-0.1.0-spark1.6.jar\n",
    "    * copy to lib/ and symlink\n",
    "    ```\n",
    "    cd ~/Downloads/spark-1.6.0-bin-hadoop2.6/lib\n",
    "    ln -s graphframes-0.1.0-spark1.6.jar graphframes.jar\n",
    "    ```\n",
    "\n",
    "    * run\n",
    "    ```\n",
    "    cd ~/Downloads/spark-1.6.0-bin-hadoop2.6/bin\n",
    "    ./pyspark --py-files ../lib/graphframes.jar --jars ../lib/graphframes.jar\n",
    "    ```\n",
    "    \n",
    "    * tutorial\n",
    "    ```\n",
    "    http://graphframes.github.io/quick-start.html\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  b|       2|\n",
      "|  c|       1|\n",
      "+---+--------+\n",
      "\n",
      "+---+-------------------+\n",
      "| id|           pagerank|\n",
      "+---+-------------------+\n",
      "|  a|               0.01|\n",
      "|  b| 0.2808611427228327|\n",
      "|  c|0.27995525261339177|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "home=os.getenv(\"HOME\")\n",
    "spark_home=os.path.join(home,\"Downloads/spark-1.6.0-bin-hadoop2.6\")\n",
    "findspark.init(spark_home)\n",
    "\n",
    "import pyspark\n",
    "#conf=pyspark.SparkConf()\n",
    "#conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "#conf.set(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway?readPreference=primaryPreferred\")\n",
    "#conf.set(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway\")\n",
    "#sc = pyspark.SparkContext(conf=conf)\n",
    "sc = pyspark.SparkContext()\n",
    "print sc._conf.get(\"spark.jars.packages\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = pyspark.sql.SQLContext(sc)\n",
    "\n",
    "v = sqlContext.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "e = sqlContext.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "# Create a GraphFrame\n",
    "from graphframes import *\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# Query: Get in-degree of each vertex.\n",
    "g.inDegrees.show()\n",
    "\n",
    "# Query: Count the number of \"follow\" connections in the graph.\n",
    "g.edges.filter(\"relationship = 'follow'\").count()\n",
    "\n",
    "# Run PageRank algorithm, and show results.\n",
    "results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
    "results.vertices.select(\"id\", \"pagerank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-13: spark-submit\n",
    "\n",
    "* spark-defaults.conf\n",
    "    * packages 여러개를 넣을 경우 컴마로 분리\n",
    "\n",
    "* spark-submit (self-contained app in quick-start 참조)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/jsl/Code/git/bb/jsl/pyds'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_hello.py\n",
    "print \"---------BEGIN-----------\"\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName1\")\n",
    "sc   = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print \"---------RESULT-----------\"\n",
    "print sc\n",
    "rdd = sc.parallelize(range(1000), 10)\n",
    "print \"mean=\",rdd.mean()\n",
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "for num in squared:\n",
    "    print \"%i \" % (num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.mongodb.spark#mongo-spark-connector_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.11;1.1.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      ":: resolution report :: resolve 128ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.11;1.1.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "---------BEGIN-----------\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/10/29 11:46:16 INFO SparkContext: Running Spark version 1.6.0\n",
      "16/10/29 11:46:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/10/29 11:46:16 WARN Utils: Your hostname, jsl-smu resolves to a loopback address: 127.0.1.1; using 117.16.44.45 instead (on interface eth0)\n",
      "16/10/29 11:46:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "16/10/29 11:46:16 INFO SecurityManager: Changing view acls to: jsl\n",
      "16/10/29 11:46:16 INFO SecurityManager: Changing modify acls to: jsl\n",
      "16/10/29 11:46:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jsl); users with modify permissions: Set(jsl)\n",
      "16/10/29 11:46:17 INFO Utils: Successfully started service 'sparkDriver' on port 46233.\n",
      "16/10/29 11:46:17 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/10/29 11:46:17 INFO Remoting: Starting remoting\n",
      "16/10/29 11:46:17 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@117.16.44.45:38397]\n",
      "16/10/29 11:46:17 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 38397.\n",
      "16/10/29 11:46:17 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/10/29 11:46:17 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/10/29 11:46:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cf4a59b4-7cdb-49dd-b05f-82da700a8e2b\n",
      "16/10/29 11:46:17 INFO MemoryStore: MemoryStore started with capacity 511.1 MB\n",
      "16/10/29 11:46:17 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/10/29 11:46:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/10/29 11:46:17 INFO SparkUI: Started SparkUI at http://117.16.44.45:4040\n",
      "16/10/29 11:46:17 INFO HttpFileServer: HTTP File server directory is /tmp/spark-6bce0fdb-924e-481b-ad54-384c75e58942/httpd-7c98e2fe-3479-4ba9-8d8b-9deacd2d1ee6\n",
      "16/10/29 11:46:17 INFO HttpServer: Starting HTTP Server\n",
      "16/10/29 11:46:17 INFO Utils: Successfully started service 'HTTP file server' on port 41725.\n",
      "16/10/29 11:46:17 INFO SparkContext: Added JAR file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar at http://117.16.44.45:41725/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar with timestamp 1477709177667\n",
      "16/10/29 11:46:17 INFO SparkContext: Added JAR file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar at http://117.16.44.45:41725/jars/org.mongodb_mongo-java-driver-3.2.2.jar with timestamp 1477709177669\n",
      "16/10/29 11:46:17 INFO Utils: Copying /home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_hello.py to /tmp/spark-6bce0fdb-924e-481b-ad54-384c75e58942/userFiles-9ec85e42-4963-4f49-a5bc-021ede112f09/ds_spark_hello.py\n",
      "16/10/29 11:46:17 INFO SparkContext: Added file file:/home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_hello.py at file:/home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_hello.py with timestamp 1477709177729\n",
      "16/10/29 11:46:17 INFO Utils: Copying /home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar to /tmp/spark-6bce0fdb-924e-481b-ad54-384c75e58942/userFiles-9ec85e42-4963-4f49-a5bc-021ede112f09/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar\n",
      "16/10/29 11:46:17 INFO SparkContext: Added file file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar at file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar with timestamp 1477709177740\n",
      "16/10/29 11:46:17 INFO Utils: Copying /home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar to /tmp/spark-6bce0fdb-924e-481b-ad54-384c75e58942/userFiles-9ec85e42-4963-4f49-a5bc-021ede112f09/org.mongodb_mongo-java-driver-3.2.2.jar\n",
      "16/10/29 11:46:17 INFO SparkContext: Added file file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar at file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar with timestamp 1477709177743\n",
      "16/10/29 11:46:17 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/10/29 11:46:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37687.\n",
      "16/10/29 11:46:17 INFO NettyBlockTransferService: Server created on 37687\n",
      "16/10/29 11:46:17 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/10/29 11:46:17 INFO BlockManagerMasterEndpoint: Registering block manager localhost:37687 with 511.1 MB RAM, BlockManagerId(driver, localhost, 37687)\n",
      "16/10/29 11:46:17 INFO BlockManagerMaster: Registered BlockManager\n",
      "---------RESULT-----------\n",
      "<pyspark.context.SparkContext object at 0x7fbd651bfa50>\n",
      "mean= 499.5\n",
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/bin/spark-submit src/ds_spark_hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sql, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds_spark_sql.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_sql.py\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print \"---------RESULT-----------\"\n",
    "from operator import add\n",
    "lines = sc.textFile(\"README.md\")\n",
    "word_count_bo = lines\\\n",
    "    .flatMap(lambda x: x.split(' '))\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "    .reduceByKey(add)\n",
    "print word_count_bo.count()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "d = [{'name': 'Alice', 'age': 1}]\n",
    "print sqlCtx.createDataFrame(d).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.mongodb.spark#mongo-spark-connector_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.11;1.1.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      ":: resolution report :: resolve 129ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.11;1.1.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/10/29 11:47:19 INFO SparkContext: Running Spark version 1.6.0\n",
      "16/10/29 11:47:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/10/29 11:47:19 WARN Utils: Your hostname, jsl-smu resolves to a loopback address: 127.0.1.1; using 117.16.44.45 instead (on interface eth0)\n",
      "16/10/29 11:47:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "16/10/29 11:47:19 INFO SecurityManager: Changing view acls to: jsl\n",
      "16/10/29 11:47:19 INFO SecurityManager: Changing modify acls to: jsl\n",
      "16/10/29 11:47:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jsl); users with modify permissions: Set(jsl)\n",
      "16/10/29 11:47:19 INFO Utils: Successfully started service 'sparkDriver' on port 40211.\n",
      "16/10/29 11:47:20 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/10/29 11:47:20 INFO Remoting: Starting remoting\n",
      "16/10/29 11:47:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@117.16.44.45:34169]\n",
      "16/10/29 11:47:20 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 34169.\n",
      "16/10/29 11:47:20 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/10/29 11:47:20 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/10/29 11:47:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-112db501-6d0f-4500-b31b-71bc546527d4\n",
      "16/10/29 11:47:20 INFO MemoryStore: MemoryStore started with capacity 511.1 MB\n",
      "16/10/29 11:47:20 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/10/29 11:47:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/10/29 11:47:20 INFO SparkUI: Started SparkUI at http://117.16.44.45:4040\n",
      "16/10/29 11:47:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-5643d451-ae62-4124-9313-8298273d85e0/httpd-edb64837-6597-4f22-9587-2e615caa0047\n",
      "16/10/29 11:47:20 INFO HttpServer: Starting HTTP Server\n",
      "16/10/29 11:47:20 INFO Utils: Successfully started service 'HTTP file server' on port 36525.\n",
      "16/10/29 11:47:20 INFO SparkContext: Added JAR file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar at http://117.16.44.45:36525/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar with timestamp 1477709240315\n",
      "16/10/29 11:47:20 INFO SparkContext: Added JAR file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar at http://117.16.44.45:36525/jars/org.mongodb_mongo-java-driver-3.2.2.jar with timestamp 1477709240318\n",
      "16/10/29 11:47:20 INFO Utils: Copying /home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_sql.py to /tmp/spark-5643d451-ae62-4124-9313-8298273d85e0/userFiles-0a184b38-4898-435a-acb8-e46b9ed61877/ds_spark_sql.py\n",
      "16/10/29 11:47:20 INFO SparkContext: Added file file:/home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_sql.py at file:/home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_sql.py with timestamp 1477709240381\n",
      "16/10/29 11:47:20 INFO Utils: Copying /home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar to /tmp/spark-5643d451-ae62-4124-9313-8298273d85e0/userFiles-0a184b38-4898-435a-acb8-e46b9ed61877/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar\n",
      "16/10/29 11:47:20 INFO SparkContext: Added file file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar at file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.11-1.1.0.jar with timestamp 1477709240389\n",
      "16/10/29 11:47:20 INFO Utils: Copying /home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar to /tmp/spark-5643d451-ae62-4124-9313-8298273d85e0/userFiles-0a184b38-4898-435a-acb8-e46b9ed61877/org.mongodb_mongo-java-driver-3.2.2.jar\n",
      "16/10/29 11:47:20 INFO SparkContext: Added file file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar at file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar with timestamp 1477709240393\n",
      "16/10/29 11:47:20 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/10/29 11:47:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40273.\n",
      "16/10/29 11:47:20 INFO NettyBlockTransferService: Server created on 40273\n",
      "16/10/29 11:47:20 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/10/29 11:47:20 INFO BlockManagerMasterEndpoint: Registering block manager localhost:40273 with 511.1 MB RAM, BlockManagerId(driver, localhost, 40273)\n",
      "16/10/29 11:47:20 INFO BlockManagerMaster: Registered BlockManager\n",
      "---------RESULT-----------\n",
      "161\n",
      "/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/context.py:237: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "[Row(age=1, name=u'Alice')]\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/bin/spark-submit src/ds_spark_sql.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graphframes\n",
    "\n",
    "* spark-defaults.conf\n",
    "    ```\n",
    "    spark.jars.packages=graphframes:graphframes:0.1.0-spark1.6\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds_spark_graphframe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_graphframe.py\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print \"=====ds_spark_dataframe=====\"\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "v = sqlContext.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "e = sqlContext.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "# Create a GraphFrame\n",
    "from graphframes import *\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# Query: Get in-degree of each vertex.\n",
    "g.inDegrees.show()\n",
    "\n",
    "# Query: Count the number of \"follow\" connections in the graph.\n",
    "g.edges.filter(\"relationship = 'follow'\").count()\n",
    "\n",
    "# Run PageRank algorithm, and show results.\n",
    "results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
    "results.vertices.select(\"id\", \"pagerank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.1.0-spark1.6 in spark-packages\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;1.1.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      ":: resolution report :: resolve 143ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.1.0-spark1.6 from spark-packages in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;1.1.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "16/10/29 21:10:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/10/29 21:10:33 WARN Utils: Your hostname, jsl-smu resolves to a loopback address: 127.0.1.1; using 117.16.44.45 instead (on interface eth0)\n",
      "16/10/29 21:10:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "=====ds_spark_dataframe=====\n",
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  b|       2|\n",
      "|  c|       1|\n",
      "+---+--------+\n",
      "\n",
      "+---+-------------------+\n",
      "| id|           pagerank|\n",
      "+---+-------------------+\n",
      "|  a|               0.01|\n",
      "|  b| 0.2808611427228327|\n",
      "|  c|0.27995525261339177|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/bin/spark-submit src/ds_spark_graphframe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mongodb spark connector\n",
    "\n",
    "### 설치\n",
    "\n",
    "* 참조 https://docs.mongodb.com/spark-connector/\n",
    "* 사전 설치\n",
    "    * Running MongoDB instance (version 2.6 or later).\n",
    "    * Spark 1.6.x.\n",
    "    * Scala 2.10.x if using the mongo-spark-connector_2.10 package\n",
    "\n",
    "* [ok] submit-spark\n",
    "    ```\n",
    "    $ ./bin/spark-submit /home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_mongo.py\n",
    "    ```\n",
    "    \n",
    "    * submit-spark실행 전, 설정에 packages를 넣음 (scala -version이 2.11이라 mongo-spark-connector_2.11을 넣어야 하지만, 2.10으로 ok\n",
    "    ```\n",
    "    $vim conf/spark-defaults.conf \n",
    "    spark.jars.packages=org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "    ```\n",
    "    \n",
    "    * scala version 확인\n",
    "    ```\n",
    "    jsl@jsl-smu:~$ scala -version\n",
    "    Scala code runner version 2.11.6 -- Copyright 2002-2013, LAMP/EPFL\n",
    "    ```\n",
    "    \n",
    "* [ok] pyspark\n",
    "\n",
    "```\n",
    "./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n",
    "              --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n",
    "              --packages org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "```\n",
    "\n",
    "* ERROR DefaultMongoPartitioner: MongoDB version < 3.2 detected. 설정에 추가\n",
    "    ```\n",
    "    spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB Python API Basics\n",
    "\n",
    "* MongoDB에 쓰기\n",
    "    * DataFrame을 생성\n",
    "        ```\n",
    "        SQLContext.createDataFrame()\n",
    "        ```\n",
    "    * DataFrame을 MongoDB로 저장\n",
    "        ```\n",
    "        write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()\n",
    "        ```\n",
    "\n",
    "* MongoDB을 읽기 (collection을 DataFrame으로)\n",
    "    * database, collection은 spark.mongodb.input.uri로 설정해 놓음\n",
    "    * format은 \"com.mongodb.spark.sql.DefaultSource\"로 정해놓음.\n",
    "        ```\n",
    "        sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_mongo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_mongo.py\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "conf.set(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/test.my2?readPreference=primaryPreferred\")\n",
    "conf.set(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/test.my2\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "#sc = pyspark.SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print sc._conf.getAll()\n",
    "sqlContext = pyspark.sql.SQLContext(sc)\n",
    "print \"---------write-----------\"\n",
    "myRdd = sc.parallelize([\n",
    "        (\"js\", 150),\n",
    "        (\"Gandalf\", 1000),\n",
    "        (\"Thorin\", 195),\n",
    "        (\"Balin\", 178),\n",
    "        (\"Kili\", 77),\n",
    "        (\"Dwalin\", 169),\n",
    "        (\"Oin\", 167),\n",
    "        (\"Gloin\", 158),\n",
    "        (\"Fili\", 82),\n",
    "        (\"Bombur\", None)\n",
    "    ])\n",
    "myDf = sqlContext.createDataFrame(myRdd, [\"name\", \"age\"])\n",
    "print myDf\n",
    "myDf.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()\n",
    "print \"---------read-----------\"\n",
    "df = sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "print df.printSchema()\n",
    "df.registerTempTable(\"myTable\")\n",
    "myTab = sqlContext.sql(\"SELECT name, age FROM myTable WHERE age >= 100\")\n",
    "myTab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.1.0-spark1.6 in spark-packages\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;1.1.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      ":: resolution report :: resolve 146ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.1.0-spark1.6 from spark-packages in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;1.1.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "16/10/30 21:26:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/10/30 21:26:16 WARN Utils: Your hostname, jsl-smu resolves to a loopback address: 127.0.1.1; using 117.16.44.45 instead (on interface eth0)\n",
      "16/10/30 21:26:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "[(u'spark.app.name', u'myAppName'), (u'spark.mongodb.input.uri', u'mongodb://127.0.0.1/test.my2?readPreference=primaryPreferred'), (u'spark.files', u'file:/home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_mongo.py,file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'), (u'spark.rdd.compress', u'True'), (u'spark.jars', u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'), (u'spark.jars.packages', u'graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0'), (u'spark.serializer.objectStreamReset', u'100'), (u'spark.master', u'local[*]'), (u'spark.submit.deployMode', u'client'), (u'spark.submit.pyFiles', u'/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'), (u'spark.mongodb.output.uri', u'mongodb://127.0.0.1/test.my2'), (u'spark.mongodb.input.partitioner', u'MongoPaginateBySizePartitioner')]\n",
      "---------write-----------\n",
      "DataFrame[name: string, age: bigint]\n",
      "---------read-----------\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|     js| 150|\n",
      "| Thorin| 195|\n",
      "|  Balin| 178|\n",
      "|Gandalf|1000|\n",
      "| Dwalin| 169|\n",
      "|    Oin| 167|\n",
      "|  Gloin| 158|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/bin/spark-submit src/ds_spark_mongo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 내 사례 mongodb twitter\n",
    "\n",
    "json을 읽을 경우, \n",
    "CardSubwayStatisticsService.row.RIDE_PASGR_NUM\n",
    "```\n",
    "$ mongo\n",
    "> use ds_rest_subwayPassengers_mongo_db\n",
    "switched to db ds_rest_subwayPassengers_mongo_db\n",
    "> show tables\n",
    "db_rest_subway\n",
    "system.indexes\n",
    "> db.db_rest_subway.find().limit(1)\n",
    "{ \"_id\" : ObjectId(\"57fa386ff5e6e94359c033e9\"), \"CardSubwayStatisticsService\" : { \"row\" : [ { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 111275, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"용문\", \"ALIGHT_PASGR_NUM\" : 108878, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 11495, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"원덕\", \"ALIGHT_PASGR_NUM\" : 10964, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 118103, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"양평\", \"ALIGHT_PASGR_NUM\" : 116604, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 10590, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"오빈\", \"ALIGHT_PASGR_NUM\" : 10020, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 26304, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"아신\", \"ALIGHT_PASGR_NUM\" : 26358, \"USE_MON\" : \"201306\" } ], \"RESULT\" : { \"MESSAGE\" : \"정상 처리되었습니다\", \"CODE\" : \"INFO-000\" }, \"list_total_count\" : 530 } }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_twitter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_twitter.py\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "conf.set(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway?readPreference=primaryPreferred\")\n",
    "conf.set(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "#sc = pyspark.SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print sc._conf.getAll()\n",
    "sqlContext = pyspark.sql.SQLContext(sc)\n",
    "print \"---------read-----------\"\n",
    "df = sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "print df.printSchema()\n",
    "df.registerTempTable(\"myTwitter\")\n",
    "myTab = sqlContext.sql(\"SELECT CardSubwayStatisticsService.row.RIDE_PASGR_NUM FROM myTwitter\")\n",
    "print type(myTab)\n",
    "myTab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.1.0-spark1.6 in spark-packages\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;1.1.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      ":: resolution report :: resolve 144ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.1.0-spark1.6 from spark-packages in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;1.1.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "16/10/30 21:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/10/30 21:45:04 WARN Utils: Your hostname, jsl-smu resolves to a loopback address: 127.0.1.1; using 117.16.44.45 instead (on interface eth0)\n",
      "16/10/30 21:45:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "[(u'spark.app.name', u'myAppName'), (u'spark.mongodb.output.uri', u'mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway'), (u'spark.rdd.compress', u'True'), (u'spark.jars', u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'), (u'spark.jars.packages', u'graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0'), (u'spark.serializer.objectStreamReset', u'100'), (u'spark.master', u'local[*]'), (u'spark.submit.deployMode', u'client'), (u'spark.files', u'file:/home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_twitter.py,file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'), (u'spark.submit.pyFiles', u'/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'), (u'spark.mongodb.input.uri', u'mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway?readPreference=primaryPreferred'), (u'spark.mongodb.input.partitioner', u'MongoPaginateBySizePartitioner')]\n",
      "---------read-----------\n",
      "root\n",
      " |-- CardSubwayStatisticsService: struct (nullable = true)\n",
      " |    |-- row: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- COMMT: string (nullable = true)\n",
      " |    |    |    |-- RIDE_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- WORK_DT: string (nullable = true)\n",
      " |    |    |    |-- LINE_NUM: string (nullable = true)\n",
      " |    |    |    |-- SUB_STA_NM: string (nullable = true)\n",
      " |    |    |    |-- ALIGHT_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- USE_MON: string (nullable = true)\n",
      " |    |-- RESULT: struct (nullable = true)\n",
      " |    |    |-- MESSAGE: string (nullable = true)\n",
      " |    |    |-- CODE: string (nullable = true)\n",
      " |    |-- list_total_count: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n",
      "None\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+\n",
      "|      RIDE_PASGR_NUM|\n",
      "+--------------------+\n",
      "|[111275.0, 11495....|\n",
      "|[30281.0, 10832.0...|\n",
      "|[75553.0, 189783....|\n",
      "|[62789.0, 220544....|\n",
      "|[74486.0, 152381....|\n",
      "|[43418.0, 413533....|\n",
      "|[301856.0, 37978....|\n",
      "|[146909.0, 227066...|\n",
      "|[152275.0, 285263...|\n",
      "|[161298.0, 117720...|\n",
      "|[95996.0, 39150.0...|\n",
      "|[59900.0, 201035....|\n",
      "|[228737.0, 108938...|\n",
      "|[164574.0, 481998...|\n",
      "|[748205.0, 817657...|\n",
      "|[206631.0, 188076...|\n",
      "|[112991.0, 111791...|\n",
      "|[225105.0, 938296...|\n",
      "|[175909.0, 271844...|\n",
      "|[45047.0, 126837....|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/bin/spark-submit src/ds_spark_twitter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* all-in-one 위를 spark-submit아닌 것으로 풀기\n",
    "* sc가 또 생성되지 않도록 주의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'spark.app.name', u'myAppName'), (u'spark.mongodb.input.uri', u'mongodb://127.0.0.1/ds_rest_subwayPassengersDb.db_rest_subwayTable?readPreference=primaryPreferred'), (u'spark.submit.pyFiles', u'/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,/home/jsl/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar,/home/jsl/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,/home/jsl/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'), (u'spark.rdd.compress', u'True'), (u'spark.serializer.objectStreamReset', u'100'), (u'spark.master', u'local[*]'), (u'spark.mongodb.output.uri', u'mongodb://127.0.0.1/ds_rest_subwayPassengersDb.db_rest_subwayTable'), (u'spark.submit.deployMode', u'client'), (u'spark.jars', u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar,file:/home/jsl/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/jsl/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'), (u'spark.files', u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar,file:/home/jsl/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/jsl/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'), (u'spark.jars.packages', u'graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0,com.databricks:spark-csv_2.10:1.3.0'), (u'spark.mongodb.input.partitioner', u'MongoPaginateBySizePartitioner')]\n",
      "graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0,com.databricks:spark-csv_2.10:1.3.0\n",
      "---------read-----------\n",
      "root\n",
      " |-- CardSubwayStatisticsService: struct (nullable = true)\n",
      " |    |-- row: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- COMMT: string (nullable = true)\n",
      " |    |    |    |-- RIDE_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- WORK_DT: string (nullable = true)\n",
      " |    |    |    |-- LINE_NUM: string (nullable = true)\n",
      " |    |    |    |-- SUB_STA_NM: string (nullable = true)\n",
      " |    |    |    |-- ALIGHT_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- USE_MON: string (nullable = true)\n",
      " |    |-- RESULT: struct (nullable = true)\n",
      " |    |    |-- MESSAGE: string (nullable = true)\n",
      " |    |    |-- CODE: string (nullable = true)\n",
      " |    |-- list_total_count: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n",
      "None\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+\n",
      "|      RIDE_PASGR_NUM|\n",
      "+--------------------+\n",
      "|[111275.0, 11495....|\n",
      "|[30281.0, 10832.0...|\n",
      "|[75553.0, 189783....|\n",
      "|[62789.0, 220544....|\n",
      "|[74486.0, 152381....|\n",
      "|[43418.0, 413533....|\n",
      "|[301856.0, 37978....|\n",
      "|[146909.0, 227066...|\n",
      "|[152275.0, 285263...|\n",
      "|[161298.0, 117720...|\n",
      "|[95996.0, 39150.0...|\n",
      "|[59900.0, 201035....|\n",
      "|[228737.0, 108938...|\n",
      "|[164574.0, 481998...|\n",
      "|[748205.0, 817657...|\n",
      "|[206631.0, 188076...|\n",
      "|[112991.0, 111791...|\n",
      "|[225105.0, 938296...|\n",
      "|[175909.0, 271844...|\n",
      "|[45047.0, 126837....|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "home=os.getenv(\"HOME\")\n",
    "spark_home=os.path.join(home,\"Downloads/spark-1.6.0-bin-hadoop2.6\")\n",
    "findspark.init(spark_home)\n",
    "\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "conf.set(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengersDb.db_rest_subwayTable?readPreference=primaryPreferred\")\n",
    "conf.set(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengersDb.db_rest_subwayTable\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "#sc = pyspark.SparkContext()\n",
    "print sc._conf.getAll()\n",
    "print sc._conf.get(\"spark.jars.packages\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = pyspark.sql.SQLContext(sc)\n",
    "print \"---------read-----------\"\n",
    "df = sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "print df.printSchema()\n",
    "df.registerTempTable(\"myTwitter\")\n",
    "myTab = sqlContext.sql(\"SELECT CardSubwayStatisticsService.row.RIDE_PASGR_NUM FROM myTwitter\")\n",
    "print type(myTab)\n",
    "myTab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(RIDE_PASGR_NUM=[111275.0, 11495.0, 118103.0, 10590.0, 26304.0])\n",
      "Row(RIDE_PASGR_NUM=[111275.0, 11495.0, 118103.0, 10590.0, 26304.0])\n"
     ]
    }
   ],
   "source": [
    "print myTab.first()\n",
    "print myTab.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
