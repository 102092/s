{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark RDD\n",
    "\n",
    "* Last updated 20161125 20170221\n",
    "* spark.read.text vs spark.sparkContext.textFile\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark RDD를 사용할 수 있다.\n",
    "\n",
    "### S.1.2 목차\n",
    "\n",
    "* S.2 Data를 사용하는 API\n",
    "* S.3 SparkSession\n",
    "* S.3.1 Spark 2.0 vs 1.6\n",
    "* S.3.2 SparkContext\n",
    "* S.3.3 IPython Notebook에서 SparkSession 생성하기\n",
    "* S.4 RDD\n",
    "* S.5 Spark를 사용하기 전, Python 함수 사용하기\n",
    "* S.6 Spark RDD 사용하기\n",
    "* S.7 spark-submit\n",
    "\n",
    "### S.1.3 문제\n",
    "\n",
    "* 문제 S-1: Hello Spark - 환경설정을 읽어 클라이언트 sc를 생성하기.\n",
    "* 문제 S-2: RDD를 사용하여 MLlib의 입력 데이터 word vector 생성하기.\n",
    "* 문제 S-3: RDD를 사용하여 MLLib의 입력 데이터 feature vector 생성하기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 Data를 사용하는 API\n",
    "\n",
    "* 3가지 데이터\n",
    "    * RDD, DataSet, DataFrame\n",
    "* Spark의 RDD, DataFrame 모두 immutable\n",
    "* Spark의 데이터는 모두 lazy (실제 transformation을 action까지 연기)\n",
    "* Spark에서 모든 transformations이 연기된다는 lazy의 의미는:\n",
    "    * 변환을 하여 메모리에 가지고 있는 비효율성\n",
    "    * 실제 action이 실행되는 경우, 계산이 이루어짐.\n",
    "    * RDD의 경우, action이 실행될 때마다 재계산이 이루어지는 것을 막기 위해 persist (or cache)함수를 사용할 수 있다.\n",
    "\n",
    "데이터구조 | 도입된 spark version | 설명\n",
    "---------|---------|---------\n",
    "rdd | Spark 1.0 | unstructured, schema free, low-level\n",
    "dataframe | 1.3 | semi 또는 structured, schema를 가진다. Dataset[Row]와 같은 의미로, 타잎을 강제하지 않는다.\n",
    "dataset | 1.6 | 자바의 Generic과 같이 Dataset[T]으로 '타잎'을 강제하는 형식이다. Scala and Java에서 사용한다. Python loosely-typed이므로 사용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 SparkSession\n",
    "\n",
    "### S.3.1 Spark 2.0 vs 1.6\n",
    "\n",
    "* Spark 2.0부터는 SparkSession으로 시작점을 통합해서, 개별적인 Context를 모두 통합해서 'pyspark.sql.SparkSession'를 사용한다.\n",
    "\n",
    "```\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "Context 구분 | 설명 | 2.0의 사용 예 | 1.x의 사용 예\n",
    "----------|----------|----------|----------\n",
    "SparkContext | RDD를 사용하는 Context | spark.SparkContext | SparkContext()\n",
    "StreamingContext | 향후 제공 | |\n",
    "SQLContext | Spark SQL, DataFrame | spark.sql | SQLContext(sarkContext)\n",
    "HiveContext | HiveQL, DataFrame | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 이전 1.x 버전에서는 SparkContext를 통해 다른 Context를 사용했고, 호환성을 제공하므로 그대로 사용할 수도 있다.\n",
    "* Spark 1.6에서는 다음과 같이 사용한다.\n",
    "\n",
    "```\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "sc = pyspark.SparkContext(conf=conf) #SparkContext를 직접 생성한다.\n",
    "sqlContext = SQLContext(sc)   #SparkContext를 넣어서 SQLContext를 생성\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.3.2 SparkContext\n",
    "\n",
    "* Spark 서버(클러스터)에 대한 클라이언트와 같은 역할을 한다.\n",
    "* 클러스터를 어떻게 사용할 것인지 정하는 것 -> cluster manager에서 system resource를 할당받음 (cpu, memory, machine)\n",
    "* Python의 SparkContext는 jar를 분산환경에서 사용하게 되므로 주의 (Scala, Java와 다름)\n",
    "    * pyFiles에 사용할 (의존적인) 라이브러리를 넣는다 (또는 사용할 라이브러리가 없으면 빈 파일로 둔다).\n",
    "\n",
    "* Cannot run multiple SparkContexts at once;\n",
    "    * sc가 이미 있는 경우\n",
    "```\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"master\",\"my python app\", sparkHome=\"sparkhome\",pyFiles=\"placeholderdeps.zip\")\n",
    "```\n",
    "\n",
    "* SparkConf\n",
    "    * spark-defaults.conf와 같은 파일의 값을 읽어서 설정\n",
    "```\n",
    "scala> sc.getConf.getOption(\"spark.local.dir\")\n",
    "res0: Option[String] = None\n",
    "\n",
    "scala> sc.getConf.getOption(\"spark.app.name\")\n",
    "res1: Option[String] = Some(Spark shell)\n",
    "\n",
    "scala> sc.getConf.get(\"spark.master\")\n",
    "res2: String = local[*]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.3.3 IPython Notebook에서 SparkSession 생성하기\n",
    "\n",
    "PySpark on IPython Notebook\n",
    "\n",
    "* ipython notebook에서 pyspark를 사용\n",
    "* kernel을 만들지 않고, findspark를 사용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-1: Hello Spark: 환경설정을 읽어 클라이언트 sc를 생성하기.\n",
    "\n",
    "* 설정을 변경한다.\n",
    "\n",
    "설정 항목 | 설명\n",
    "----------|----------\n",
    "SPARK_HOME | Spark를 설치한 자신의 경로로 수정한다.\n",
    "PYTHONPATH | sys.path.insert()를 사용하여 PYTHONPATH를 수정한다. pyspark.zip, py4j-0.10.1-src.zip를 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip\n",
      "\n",
      "/usr/lib/python2.7\n",
      "/usr/lib/python2.7/plat-x86_64-linux-gnu\n",
      "/usr/lib/python2.7/lib-tk\n",
      "/usr/lib/python2.7/lib-old\n",
      "/usr/lib/python2.7/lib-dynload\n",
      "/home/jsl/.local/lib/python2.7/site-packages\n",
      "/usr/local/lib/python2.7/dist-packages\n",
      "/usr/lib/python2.7/dist-packages\n",
      "/usr/lib/python2.7/dist-packages/PILcompat\n",
      "/usr/lib/python2.7/dist-packages/gtk-2.0\n",
      "/usr/lib/python2.7/dist-packages/ubuntu-sso-client\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/extensions\n",
      "/home/jsl/.ipython\n"
     ]
    }
   ],
   "source": [
    "for i in sys.path:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 설정을 읽어 온다 (conf디렉토리 아래)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "pyspark-shell\n",
      "local[*]\n",
      "117.16.44.45\n"
     ]
    }
   ],
   "source": [
    "print spark.version\n",
    "print spark.conf.get('spark.app.name')\n",
    "print spark.conf.get('spark.master')\n",
    "print spark.conf.get('spark.driver.host')\n",
    "#print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.4 RDD 소개\n",
    "\n",
    "* RDD (Resilient Distributed Dataset)는 분산 레코드인 데이터 형식이다.\n",
    "    * Resilient - fault tolerent (어느 한 노드에서 작업이 실패하면 다른 노드에서 실행된다.)\n",
    "    * Distributed - multiple nodes in a clusters\n",
    "    * Dataset - 데이터타잎으로 구성된다.\n",
    "* RDD는 내, 외부 자료에서 생성하며, 생성된 자료는 read-only이다.\n",
    "    * HDFS 파일을 처리할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3단계 처리\n",
    "    * 1단계: 읽기 - 2가지 방식: \n",
    "        * 외부에서 읽기 \n",
    "```\n",
    "sparkContext.textFile()\n",
    "```\n",
    "        \n",
    "        * 내부에서 읽기 parallelizing a collection\n",
    "```\n",
    "sparkContext.parallelize()\n",
    "```\n",
    "        \n",
    "    * 2단계: 변환 transformations - lazy도 가능: RDD => RDD or seq(RDD)\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "map(fn) | 요소별로 fn을 적용해서 결과 rdd 돌려줌 | \n",
    "filter(fn) | 요소별로 선별하여 fn을 적용해서 결과 rdd 돌려줌 | filter(lambda line: \"Spark\" in line)\n",
    "flatMap(fn) | 요소별로 fn을 적용하고, flat해서 결과 rdd 돌려줌 | .flatMap(lambda x: x.split(' '))\n",
    "groupByKey() | key를 그룹해서 iterator를 돌려줌. |\n",
    "\n",
    "\n",
    "    * 3단계: actions: RDD => a value (e.g., python list)\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "reduce(fn) | 요소별로 fn을 사용해서 줄여서 결과 list를 돌려줌 |\n",
    "collect() | 모든 요소를 결과 list로 돌려줌 |\n",
    "count() | 요소의 갯수를 결과 list로 돌려줌 |\n",
    "countByKey() | |\n",
    "foreach(fn) | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.5 Spark를 사용하기 전, Python 함수 사용하기\n",
    "\n",
    "* map, reduce, filter\n",
    "    * 함수의 인자는 2개가 필요하다 (함수, 데이터).\n",
    "\n",
    "함수 | 설명 | 예\n",
    "-------|-------|-------\n",
    "map() | 각 데이터 요소에 함수를 적용해서 list를 반환 | map(fn,data)\n",
    "filter() | 각 데이터 요소에 함수의 결과 True를 선택해서 반환 | filter(fn, data)\n",
    "reduce() | 각 데이터 요소에 함수를 적용해서 list를 반환 | reduce(fn, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python 함수로 처리한다.\n",
    "    * 입출력은 데이터 하나씩이 아니라, list로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python에서 제공하는 map() 함수를 사용한다. map() 함수의 인자:\n",
    "    * (1) 함수명 (함수의 return은 반드시 있어야 한다.)\n",
    "    * (2) 입력인자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, celsius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lambda함수를 사용한다.\n",
    "    * lambda는 무명 함수이다. 처리 결과가 반환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5)*c + 32, celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자열에 map()을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hello World'\n",
    "words = sentence.split()\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자열을 사용하면, 각 단어를 split()한다.\n",
    "* list를 사용하면, 각 요소를 split()한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello World\"\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"Hello World\"]\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter()는 데이터를 선별한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduce()는 2개의 인자를 받는다.\n",
    "* [ func(func(s1, s2),s3), ... , sn ]와 같이 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, range(1,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.6 Spark RDD 사용하기\n",
    "\n",
    "* Apache spark wiki에서 첫 문단을 복사해 왔다.\n",
    "* 3째줄은 한글, 4째 줄은 같은 단어를 반복해 추가했다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 파일에서 읽기\n",
    "    ```\n",
    "    textFile()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile=spark.read.text(\"ds_spark_wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#textFile = sc.textFile(\"data/ds_spark_wiki.txt\")\n",
    "textFile = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wikipedia'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* map()함수로 단어 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words=textFile.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lambda아닌 함수로 map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "words=textFile.map(mySplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark'],\n",
       " [u'Originally',\n",
       "  u'developed',\n",
       "  u'at',\n",
       "  u'the',\n",
       "  u'University',\n",
       "  u'of',\n",
       "  u'California,',\n",
       "  u\"Berkeley's\",\n",
       "  u'AMPLab,'],\n",
       " [u'the',\n",
       "  u'Spark',\n",
       "  u'codebase',\n",
       "  u'was',\n",
       "  u'later',\n",
       "  u'donated',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'Apache',\n",
       "  u'Software',\n",
       "  u'Foundation,'],\n",
       " [u'which', u'has', u'maintained', u'it', u'since.'],\n",
       " [u'Spark',\n",
       "  u'provides',\n",
       "  u'an',\n",
       "  u'interface',\n",
       "  u'for',\n",
       "  u'programming',\n",
       "  u'entire',\n",
       "  u'clusters',\n",
       "  u'with'],\n",
       " [u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Wikipedia']\n",
      "[u'Apache', u'Spark', u'is', u'an', u'open', u'source', u'cluster', u'computing', u'framework.']\n",
      "[u'\\uc544\\ud30c\\uce58', u'\\uc2a4\\ud30c\\ud06c\\ub294', u'\\uc624\\ud508', u'\\uc18c\\uc2a4', u'\\ud074\\ub7ec\\uc2a4\\ud130', u'\\ucef4\\ud4e8\\ud305', u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']\n",
      "[u'Apache', u'Spark', u'Apache', u'Spark', u'Apache', u'Spark', u'Apache', u'Spark']\n",
      "[u'Originally', u'developed', u'at', u'the', u'University', u'of', u'California,', u\"Berkeley's\", u'AMPLab,']\n",
      "[u'the', u'Spark', u'codebase', u'was', u'later', u'donated', u'to', u'the', u'Apache', u'Software', u'Foundation,']\n",
      "[u'which', u'has', u'maintained', u'it', u'since.']\n",
      "[u'Spark', u'provides', u'an', u'interface', u'for', u'programming', u'entire', u'clusters', u'with']\n",
      "[u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']\n"
     ]
    }
   ],
   "source": [
    "for i in words.collect():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 문장의 철자 갯수를 센다.\n",
    "    * 첫 문장 'Wiskipedia'는 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_sparkLine=textFile.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print _sparkLine.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한글은 앞에 u를 붙여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_line = textFile.filter(lambda line: u\"스파크\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "print _line.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "* groupByKey()\n",
    "    * groupByKey()는 key를 묶어준다. 따라서 iterator를 반환한다. mapValues(sum)을 하면 key별 합계를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 1),\n",
       " (u'\\uc18c\\uc2a4', 1),\n",
       " (u'is', 1),\n",
       " (u'Wikipedia', 1),\n",
       " (u'AMPLab,', 1),\n",
       " (u'maintained', 1),\n",
       " (u'donated', 1),\n",
       " (u'\\ucef4\\ud4e8\\ud305', 1),\n",
       " (u'open', 1),\n",
       " (u'since.', 1),\n",
       " (u'for', 1),\n",
       " (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1),\n",
       " (u'with', 1),\n",
       " (u'framework.', 1),\n",
       " (u'provides', 1),\n",
       " (u'Apache', 6),\n",
       " (u'Spark', 7),\n",
       " (u'was', 1),\n",
       " (u'Originally', 1),\n",
       " (u'which', 1),\n",
       " (u'fault-tolerance.', 1),\n",
       " (u'University', 1),\n",
       " (u'codebase', 1),\n",
       " (u'interface', 1),\n",
       " (u'data', 1),\n",
       " (u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.', 1),\n",
       " (u'Foundation,', 1),\n",
       " (u'\\uc624\\ud508', 1),\n",
       " (u'programming', 1),\n",
       " (u'\\uc2a4\\ud30c\\ud06c\\ub294', 1),\n",
       " (u'the', 3),\n",
       " (u'entire', 1),\n",
       " (u'has', 1),\n",
       " (u'to', 1),\n",
       " (u'later', 1),\n",
       " (u'computing', 1),\n",
       " (u'Software', 1),\n",
       " (u'developed', 1),\n",
       " (u\"Berkeley's\", 1),\n",
       " (u'it', 1),\n",
       " (u'an', 2),\n",
       " (u'cluster', 1),\n",
       " (u'implicit', 1),\n",
       " (u'at', 1),\n",
       " (u'of', 1),\n",
       " (u'clusters', 1),\n",
       " (u'parallelism', 1),\n",
       " (u'\\uc544\\ud30c\\uce58', 1),\n",
       " (u'California,', 1),\n",
       " (u'source', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parallelize() 사용하기\n",
    "    * list에서 읽어, rdd로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_aList=[1,2,3]\n",
    "rdd = spark.sparkContext.parallelize(_aList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* map(), collect() 사용해서 square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "print squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장 처리하기\n",
    "* 단어를 교체하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "a=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(a)\n",
    "\n",
    "words=_rdd.map(lambda x:x.split())\n",
    "print words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_upper=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "_upper.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 첫 글자를 대문자로 만들어서 출력해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: x[0].upper())\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: [i.upper() for i in x])\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transformation(map()), action(collect()) 함수를 한꺼번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: [i.upper() for i in x]).collect()\n",
    "print pluralRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = words\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print wordsLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 파일에 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pluralRDD.saveAsTextFile(\"data/ds_spark_wiki1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* create RDD from CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp_file = spark.sparkContext.textFile(\"./data/ds_spark_2cols.csv\")\n",
    "numbers_rdd = inp_file.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1'],\n",
       " [u'14', u' 19'],\n",
       " [u'46', u' 1'],\n",
       " [u'10', u' 34'],\n",
       " [u'28', u' 3'],\n",
       " [u'48', u' 1']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.7 spark-submit\n",
    "\n",
    "* sys.path 설정은 하지 않아도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_rdd_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_hello.py\n",
    "import pyspark\n",
    "#conf = pyspark.SparkConf().setAppName(\"myAppName1\")\n",
    "#sc   = pyspark.SparkContext(conf=conf)\n",
    "#sc.setLogLevel(\"ERROR\")\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* spark-submit을 실행하기 전, 'conf/log4j.properties'를 수정 log level을 ERROR로 설정하였다.\n",
    "```\n",
    "log4j.rootCategory=ERROR, console\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------BEGIN-----------\n",
      "---------RESULT-----------\n",
      "2.0.0\n",
      " 499.5\n",
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_rdd_hello.py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
