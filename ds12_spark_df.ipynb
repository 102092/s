{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark DataFrame\n",
    "\n",
    "* Last updated 20161125 20170221\n",
    "* TODO: Spark MySql, postgresql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark 머신러닝에 필요한 데이터타잎을 정의하고, 데이터프레임을 사용할 수 있다.\n",
    "* Spark SQL을 사용하여 데이터를 추출할 수 있다.\n",
    "* Spark 데이터를 MongoDB에 쓰고, 읽을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "\n",
    "* S.2 IPython Notebook에서 SparkSession 생성하기  \n",
    "* S.3 데이터 타잎\n",
    "* S.3.1 dense vector\n",
    "* S.3.2 sparse vector.\n",
    "* S.3.3 labeled point\n",
    "* S.3.4 maxtrix\n",
    "* S.3.5 libsvm format\n",
    "* S.4 Spark SQL\n",
    "* S.5 Dataframe\n",
    "* S.6 MonogoDB\n",
    "* S.7 spark-submit\n",
    "\n",
    "### S.1.3 문제\n",
    "\n",
    "* 문제 S-1: RDD를 사용하여 MLlib의 입력 데이터 word vector생성하기. \n",
    "* 문제 S-2: 파일을 읽어서 feature vector 생성하기. \n",
    "* 문제 S-3: 파일에서 Spark SQL로 데이터 읽기\n",
    "* 문제 S-4: spark sql uber csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## S.2 IPython Notebook에서 SparkSession 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"mySpark\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 데이터 타잎\n",
    "\n",
    "* 데이터 타잎은 다음과 같다.\n",
    "\n",
    "구분 | 설명\n",
    "----------|----------\n",
    "Vector | dense와 sparse가 있다.\n",
    "Labled Point | 클래스값과 결과를 묶음. supervised learning에 사용.\n",
    "Matrix | 행열로 구성\n",
    "\n",
    "* 이러한 데이터 타잎이 Spark의 ml, mllib 패키지에 따라 차이가 있다는 것을 주의한다.\n",
    "\n",
    "구분 | 설명\n",
    "-------|-------\n",
    "mllib | RDD API\n",
    "ml | DataFrame API, Pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* vectors\n",
    "    * local vector - single machine에 있다.\n",
    "    * pyspark.mllib.linalg.Vectors\n",
    "\n",
    "dense vector | sparse vector\n",
    "----------|----------\n",
    "빈 값이 별로 없는 경우. an array of its values | 빈 값이 많은 경우 사용. 인덱스, 값 배열 별도\n",
    "(160,69,24) | (3,[0,1,2],[160.0,69.0,24.0])\n",
    "입력 NumPy’s array, Python list | MLlib’s SparseVector, SciPy’s csc_matrix with a single column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.1 dense vector\n",
    "\n",
    "* 주의: ml, mllib에서 제공하는지 식별하면서 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "dv1ml=Vectors.dense([0.0, 1.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "dv1mllib=Vectors.dense([0.0, 1.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.1,0.1] [0.0,1.1,0.1]\n"
     ]
    }
   ],
   "source": [
    "print dv1ml, dv1mllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* numpy를 사용해서 dense vector를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dv2 = np.array([1.0, 2.1, 3.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.1,0.1]\n"
     ]
    }
   ],
   "source": [
    "print dv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list를 사용하여 dense vector를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dv2 = [1.0, 2.1, 3.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.2 sparse vector\n",
    "\n",
    "* SparseVector vs Vectors.sparse 차이?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* toArray()는 1줄씩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.]\n"
     ]
    }
   ],
   "source": [
    "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\n",
    "print sv1.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (2, 0)\t3.0\n"
     ]
    }
   ],
   "source": [
    "sv2 = sps.csc_matrix((np.array([1.0,3.0]), np.array([0,2]), np.array([0,2])), shape = (3,1))\n",
    "sv2.todense()\n",
    "print sv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.3 labeled point\n",
    "\n",
    "* local vector, either dense or sparse\n",
    "* label과 response로 구성된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "LabeledPoint(1, [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "LabeledPoint(1.0, Vectors.dense([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* mllib의 LabeledPoint를 사용하는 경우, ml의 Vectors를 혼용하면 오류\n",
    "\n",
    "```\n",
    "Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "```\n",
    "* 이러한 오류는 Vectors.fromML() 함수를 사용해서 혼용하지 않게 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f0a04d955da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdv1ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, label, features)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\u001b[0m in \u001b[0;36m_convert_to_vector\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot convert type %s into Vector\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector"
     ]
    }
   ],
   "source": [
    "LabeledPoint(1.0, dv1ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [0.0,1.1,0.1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabeledPoint(1.0, dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [0.0,1.1,0.1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabeledPoint(1.0, Vectors.fromML(dv1ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list에서 dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [[1,[1.0,2.0,3.0]],[1,[1.1,2.1,3.1]],[0,[1.2,2.2,3.3]]]\n",
    "#trainDf=sqlCtx.createDataFrame(p)\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list를 LabeledPoint로 생성하면, label과 features로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1,[1.0,2.0,3.0]),\n",
    "     LabeledPoint(1,[1.1,2.1,3.1]),\n",
    "     LabeledPoint(0,[1.2,2.2,3.3])]\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* schema를 사용해서 dataframe 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf=_rdd.toDF(schema)\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.4 maxtrix\n",
    "\n",
    "* local matrix - pyspark.mllib.linalg.Matrix, Matrices\n",
    "* distributed matrix\n",
    "    * pyspark.mllib.linalg.distributed.RowMatrix\n",
    "    * pyspark.mllib.linalg.distributed.IndexedRow, IndexedRowMatrix\n",
    "    * pyspark.mllib.linalg.distributed.BlockMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.5 libsvm format\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "* format\n",
    "    ```\n",
    "    [label] [index1]:[value1] [index2]:[value2] ...\n",
    "    [label] [index1]:[value1] [index2]:[value2] ...\n",
    "    ```\n",
    "    * label - class\n",
    "    * index - integers\n",
    "    * value - real numbers\n",
    "\n",
    "* see - /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/data/mllib/sample_libsvm_data.txt\n",
    "```\n",
    "0 128:51 129:159 130:253 131:159 132:50 155:48 156:238 157:252 158:252 159:252 160:237 182:54 183:227 184:253 185:252 186:239 187:233 188:252 189:57 190:6 208:10 209:60 210:224 211:252 212:253 213:252 214:202 215:84 216:252 217:253 218:122 236:163 237:252 238:252 239:252 240:253 241:252 242:252 243:96 244:189 245:253 246:167 263:51 264:238 265:253 266:253 267:190 268:114 269:253 270:228 271:47 272:79 273:255 274:168 290:48 291:238 292:252 293:252 294:179 295:12 296:75 297:121 298:21 301:253 302:243 303:50 317:38 318:165 319:253 320:233 321:208 322:84 329:253 330:252 331:165 344:7 345:178 346:252 347:240 348:71 349:19 350:28 357:253 358:252 359:195 372:57 373:252 374:252 375:63 385:253 386:252 387:195 400:198 401:253 402:190 413:255 414:253 415:196 427:76 428:246 429:252 430:112 441:253 442:252 443:148 455:85 456:252 457:230 458:25 467:7 468:135 469:253 470:186 471:12 483:85 484:252 485:223 494:7 495:131 496:252 497:225 498:71 511:85 512:252 513:145 521:48 522:165 523:252 524:173 539:86 540:253 541:225 548:114 549:238 550:253 551:162 567:85 568:252 569:249 570:146 571:48 572:29 573:85 574:178 575:225 576:253 577:223 578:167 579:56 595:85 596:252 597:252 598:252 599:229 600:215 601:252 602:252 603:252 604:196 605:130 623:28 624:199 625:252 626:252 627:253 628:252 629:252 630:233 631:145 652:25 653:128 654:252 655:253 656:252 657:141 658:37\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```OLD\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "svmfn=\"/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/data/mllib/sample_libsvm_data.txt\"\n",
    "svmDf = sqlCtx.read.format(\"libsvm\").load(svmfn)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "svmfn=\"/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/data/mllib/sample_libsvm_data.txt\"\n",
    "svmDf = spark.read.format(\"libsvm\").load(svmfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(svmDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* csr format (https://www.ncsu.edu/hpc/Courses/6sparse.html)\n",
    "    ```\n",
    "    0 0 0 0\n",
    "    5 8 0 0\n",
    "    0 0 3 0\n",
    "    0 6 0 0\n",
    "    ```\n",
    "    * non-zero 5 8 3 6\n",
    "    * column-index 0 1 2 1 (5(1,0) 8(1,1) 3(2,2) 6(3,1)에서 행값만 추출)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: RDD를 사용하여 MLlib의 입력 데이터 word vector생성하기.\n",
    "\n",
    "* RDD API를 사용해서 단어를 셀 수 있다 (map, reduce 등).\n",
    "* mllib 패키지를 사용하여 데이터를 변환할 수 있다.\n",
    "    * TF-IDF, Word2Vec 등을 사용할 수 있다.\n",
    "    * mllib에 없는 변환기능은 ml을 사용한다 (ml은 dataframe을 변환하는 패키지.)\n",
    "        * Tokenizer, StopWordsRemove, n-gram등\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ds_spark_wiki.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/ds_spark_wiki.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 파일 전체 word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\n",
    "wc=lines\\\n",
    "    .flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'an',\n",
       " u'open',\n",
       " u'source',\n",
       " u'cluster',\n",
       " u'computing',\n",
       " u'framework.',\n",
       " u'\\uc544\\ud30c\\uce58',\n",
       " u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       " u'\\uc624\\ud508',\n",
       " u'\\uc18c\\uc2a4',\n",
       " u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       " u'\\ucef4\\ud4e8\\ud305',\n",
       " u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Originally',\n",
       " u'developed',\n",
       " u'at',\n",
       " u'the',\n",
       " u'University',\n",
       " u'of',\n",
       " u'California,',\n",
       " u\"Berkeley's\",\n",
       " u'AMPLab,',\n",
       " u'the',\n",
       " u'Spark',\n",
       " u'codebase',\n",
       " u'was',\n",
       " u'later',\n",
       " u'donated',\n",
       " u'to',\n",
       " u'the',\n",
       " u'Apache',\n",
       " u'Software',\n",
       " u'Foundation,',\n",
       " u'which',\n",
       " u'has',\n",
       " u'maintained',\n",
       " u'it',\n",
       " u'since.',\n",
       " u'Spark',\n",
       " u'provides',\n",
       " u'an',\n",
       " u'interface',\n",
       " u'for',\n",
       " u'programming',\n",
       " u'entire',\n",
       " u'clusters',\n",
       " u'with',\n",
       " u'implicit',\n",
       " u'data',\n",
       " u'parallelism',\n",
       " u'and',\n",
       " u'fault-tolerance.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단어를 세어서 tuple로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wc = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .flatMap(lambda x: x.split(' '))\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "    .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'and', 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 라인 별 word count\n",
    "\n",
    "* dataframe으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wc = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n",
    "    .map(lambda x:x.split())\\\n",
    "    .map(lambda x:[(i,1) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'wikipedia', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'is', 1), (u'an', 1), (u'open', 1), (u'source', 1), (u'cluster', 1), (u'computing', 1), (u'framework', 1)]\n",
      "[(u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c\\ub294', 1), (u'\\uc624\\ud508', 1), (u'\\uc18c\\uc2a4', 1), (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1), (u'\\ucef4\\ud4e8\\ud305', 1), (u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1)]\n",
      "[(u'originally', 1), (u'developed', 1), (u'at', 1), (u'the', 1), (u'university', 1), (u'of', 1), (u'california', 1), (u\"berkeley's\", 1), (u'amplab', 1)]\n",
      "[(u'the', 1), (u'spark', 1), (u'codebase', 1), (u'was', 1), (u'later', 1), (u'donated', 1), (u'to', 1), (u'the', 1), (u'apache', 1), (u'software', 1), (u'foundation', 1)]\n",
      "[(u'which', 1), (u'has', 1), (u'maintained', 1), (u'it', 1), (u'since', 1)]\n",
      "[(u'spark', 1), (u'provides', 1), (u'an', 1), (u'interface', 1), (u'for', 1), (u'programming', 1), (u'entire', 1), (u'clusters', 1), (u'with', 1)]\n",
      "[(u'implicit', 1), (u'data', 1), (u'parallelism', 1), (u'and', 1), (u'fault', 1), (u'tolerance', 1)]\n"
     ]
    }
   ],
   "source": [
    "for e in wc.collect():\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* TF (Term Frequency)\n",
    "    * HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\").map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {253068: 1.0}),\n",
       " SparseVector(1048576, {36751: 1.0, 50570: 1.0, 68380: 1.0, 415281: 1.0, 511377: 1.0, 728364: 1.0, 862087: 1.0, 938426: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {63234: 1.0, 340190: 1.0, 357478: 1.0, 375592: 1.0, 458138: 1.0, 486171: 1.0, 598772: 1.0}),\n",
       " SparseVector(1048576, {938426: 4.0, 999480: 4.0}),\n",
       " SparseVector(1048576, {36757: 1.0, 225801: 1.0, 323305: 1.0, 453405: 1.0, 498679: 1.0, 518030: 1.0, 688842: 1.0, 762570: 1.0, 959994: 1.0}),\n",
       " SparseVector(1048576, {420843: 1.0, 550676: 1.0, 725041: 1.0, 782544: 1.0, 938426: 1.0, 959994: 2.0, 991590: 1.0, 993084: 1.0, 996703: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {50573: 1.0, 263739: 1.0, 892834: 1.0, 1014710: 1.0, 1035538: 1.0}),\n",
       " SparseVector(1048576, {3932: 1.0, 36751: 1.0, 192182: 1.0, 358969: 1.0, 363244: 1.0, 496856: 1.0, 546913: 1.0, 938426: 1.0, 951974: 1.0}),\n",
       " SparseVector(1048576, {69621: 1.0, 157580: 1.0, 219357: 1.0, 297436: 1.0, 715648: 1.0})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def countPartitions(id,iterator): \n",
    "         c = 0 \n",
    "         for _ in iterator: \n",
    "              c += 1 \n",
    "         yield (id,c) \n",
    "_wc=wc.mapPartitions(countPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "trainRdd = trainDf.map(lambda row: LabeledPoint(row.label,row.features))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 Spark SQL\n",
    "\n",
    "* Spark SQL은 \n",
    "    * 데이터를 구조화해서 sql을 사용할 수 있다. RDD는 비구조적인 경우에 사용한다.\n",
    "\n",
    "구분 | Spark SQL | RDD\n",
    "-----|-----|-----\n",
    "데이터 | 구조적 | 비구조적\n",
    "* Spark SQL 구성\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "Language API | Python, Java, Scala, Hive QL API를 제공\n",
    "Schema RDD | RDD에 Schema를 적용해 임시 테이블로 변환한다. dataframe.\n",
    "Data Sources | 다양한 형식 지원 - HDFS, Cassandra, HBase, and relational databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* dataframe 생성\n",
    "    * 이름, 키 정보 파일을 읽어서 키가 170이상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(height=170, name=u'kim')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [{'name': 'kim', 'height': 170}]\n",
    "spark.createDataFrame(p).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print type(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  _1| _2|\n",
      "+----+---+\n",
      "|1961|kim|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "pRow=list(Row(name=\"kim\", height=1961))\n",
    "\n",
    "df=spark.createDataFrame([pRow])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: 파일을 읽어서 feature vector 생성하기.\n",
    "\n",
    "* rdd에서 dataframe을 생성하고, sql 사용한다.\n",
    "* 네트워크 침입\n",
    "    * https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "* attack 종류 구분 (41번째 열)\n",
    "\n",
    "침입구분 | 건수\n",
    "-------|-------\n",
    "normal | 97278\n",
    "attack | 396743\n",
    "전체 | 494021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')\n",
    "if(not os.path.exists(_fname)):\n",
    "    print \"%s data does not exist! retrieving..\" % _fname\n",
    "    _f=urllib.urlretrieve(_url,_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_rdd = spark.sparkContext.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 데이터에 'normal.'이 포함된 건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n"
     ]
    }
   ],
   "source": [
    "_normal = _rdd.filter(lambda x: 'normal.' in x)\n",
    "print _normal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_csvRdd=_rdd.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']]\n"
     ]
    }
   ],
   "source": [
    "print _csvRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 데이터 분류\n",
    "    * reduceByKey()를 사용해 각 경우의 건 수를 센다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_kv = _csvRdd.map(lambda x: (x[41], 1))\n",
    "_attack = _kv.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 231),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 280790),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 107201),\n",
       " (u'normal.', 97278),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 1040),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 1247),\n",
       " (u'satan.', 1589)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attack.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "_normalRdd=_csvRdd.filter(lambda x: x[41]==\"normal.\")\n",
    "_attackRdd=_csvRdd.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n",
      "396743\n"
     ]
    }
   ],
   "source": [
    "print _normalRdd.count()\n",
    "print _attackRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* combineByKey(x, y, z)\n",
    "    * Combiner function: x\n",
    "        * key-value에서 value로 combine하려면 (value,1)\n",
    "    * Merge value function: y\n",
    "    * Merge combiners function: z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3.0, 1: 10.0}\n"
     ]
    }
   ],
   "source": [
    "data = spark.sparkContext.parallelize( [(0, 2.), (0, 4.), (1, 0.), (1, 10.), (1, 20.)] )\n",
    "sumCount = data.combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "averageByKey = sumCount.map(lambda (label, (value_sum, count)): (label, value_sum / count))\n",
    "\n",
    "print averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([(2.0, 1), (4.0, 1)], 2)), (1, ([(10.0, 1), (0.0, 1), (20.0, 1)], 3))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "aggregated_counts = (data\n",
    "    .map(lambda kv: (kv, 1))\n",
    "    .reduceByKey(add)\n",
    "    .map(lambda kv: (kv[0][0], (kv[0][1], kv[1])))\n",
    "    .groupByKey()\n",
    "    .mapValues(lambda xs: (list(xs), sum(x[1] for x in xs))))\n",
    "\n",
    "aggregated_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'back.': (2203, 2203),\n",
       " u'buffer_overflow.': (30, 30),\n",
       " u'ftp_write.': (8, 8),\n",
       " u'guess_passwd.': (53, 53),\n",
       " u'imap.': (12, 12),\n",
       " u'ipsweep.': (1247, 1247),\n",
       " u'land.': (21, 21),\n",
       " u'loadmodule.': (9, 9),\n",
       " u'multihop.': (7, 7),\n",
       " u'neptune.': (107201, 107201),\n",
       " u'nmap.': (231, 231),\n",
       " u'normal.': (97278, 97278),\n",
       " u'perl.': (3, 3),\n",
       " u'phf.': (4, 4),\n",
       " u'pod.': (264, 264),\n",
       " u'portsweep.': (1040, 1040),\n",
       " u'rootkit.': (10, 10),\n",
       " u'satan.': (1589, 1589),\n",
       " u'smurf.': (280790, 280790),\n",
       " u'spy.': (2, 2),\n",
       " u'teardrop.': (979, 979),\n",
       " u'warezclient.': (1020, 1020),\n",
       " u'warezmaster.': (20, 20)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts = _kv.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "sum_counts.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* rdd to sql, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_csvRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "#sqlCtx = SQLContext(sc)\n",
    "\n",
    "#_df=sqlCtx.createDataFrame(_rdd)\n",
    "_df=spark.createDataFrame(_csvRdd)\n",
    "\n",
    "_df.registerTempTable(\"_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|protocol| count|\n",
      "+--------+------+\n",
      "|     tcp|190065|\n",
      "|     udp| 20354|\n",
      "|    icmp|283602|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.select(\"protocol\", \"duration\", \"dst_bytes\").groupBy(\"protocol\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|protocol|count|\n",
      "+--------+-----+\n",
      "|     tcp|  139|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.select(\"protocol\", \"duration\", \"dst_bytes\")\\\n",
    "    .filter(_df.duration>1000)\\\n",
    "    .filter(_df.dst_bytes==0)\\\n",
    "    .groupBy(\"protocol\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tcp_interactions = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT duration, dst_bytes FROM _tab\n",
    "    WHERE protocol = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tcp_interactions_out = tcp_interactions.rdd\\\n",
    "    .map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5043, Dest. bytes: 0\n",
      "Duration: 5046, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 42448, Dest. bytes: 0\n",
      "Duration: 40121, Dest. bytes: 0\n",
      "Duration: 31709, Dest. bytes: 0\n",
      "Duration: 30619, Dest. bytes: 0\n",
      "Duration: 22616, Dest. bytes: 0\n",
      "Duration: 21455, Dest. bytes: 0\n",
      "Duration: 13998, Dest. bytes: 0\n",
      "Duration: 12933, Dest. bytes: 0\n"
     ]
    }
   ],
   "source": [
    "for i,ti_out in enumerate(tcp_interactions_out.collect()):\n",
    "    if(i%10==0):\n",
    "        print ti_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: 파일에서 Spark SQL로 데이터 읽기\n",
    "\n",
    "* 1. json파일에서 읽기\n",
    "    * 주의: format이 건별로 저장되어 있슴???\n",
    "* 2. twitter json\n",
    "* 3. url에서 json 읽어오기\n",
    "* 4. csv파일에서 일기\n",
    "* 5. com.databricks.spark.csv\n",
    "    * vim conf/spark-defaults.conf\n",
    "        ```\n",
    "        spark.jars.packages=com.databricks:spark-csv_2.10:1.3.0\n",
    "        ```\n",
    "\n",
    "* sqlContext.jsonRDD()\n",
    "* sqlContext.jsonFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### JSON 파일 읽기\n",
    "\n",
    "* json파일을 읽어서, sql을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.json\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pDF= spark.read.json(\"/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pDF.filter(pDF['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pDF.registerTempTable(\"people\")\n",
    "spark.sql(\"select name from people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* spark.catalog를 사용하여 사용하는 'Table'을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'people', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Twitter JSON을 읽을 경우\n",
    "\n",
    "구분 | 예\n",
    "-------|-------\n",
    "unicode를 사용하면 backslash | \"{\\\"created_at\\\":\\\"Sun Nov 13 00:05:19 +0000 2016\\\"\n",
    "보통 | {\"created_at\":\"Sun Nov 13 00:05:19 +0000 2016\"\n",
    "\n",
    "\n",
    "    * allowBackslashEscapingAnyCharacter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "twitterDF= spark.read.json(os.path.join(\"src\",\"ds_twitter_1_noquote.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- symbols: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- urls: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- favorited: boolean (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |-- in_reply_to_user_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- is_quote_status: boolean (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- default_profile: boolean (nullable = true)\n",
      " |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- favourites_count: long (nullable = true)\n",
      " |    |-- follow_request_sent: boolean (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following: boolean (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |-- has_extended_profile: boolean (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- is_translation_enabled: boolean (nullable = true)\n",
      " |    |-- is_translator: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- notifications: boolean (nullable = true)\n",
      " |    |-- profile_background_color: string (nullable = true)\n",
      " |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |-- profile_image_url: string (nullable = true)\n",
      " |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |-- profile_link_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |-- profile_text_color: string (nullable = true)\n",
      " |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- time_zone: string (nullable = true)\n",
      " |    |-- translator_type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- utc_offset: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           text|\n",
      "+---------------+\n",
      "|Hello 21 160924|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.select('text').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           text|\n",
      "+---------------+\n",
      "|Hello 21 160924|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.registerTempTable(\"twitter\")\n",
    "spark.sql(\"select text from twitter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### JSON frm URL\n",
    "\n",
    "* url에서 데이터 읽으면 string (예: r.iter_lines()하면 문자 1개씩 가져옴)\n",
    "* response를 json으로 읽으면 ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Row로 만들어주어야?\n",
    "    ```\n",
    "    df = sqlContext.createDataFrame([json.loads(line) for line in r.iter_lines()])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/session.py:316: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "wcDF=spark.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.registerTempTable(\"wc\")\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* baby names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "_url=\"https://health.data.ny.gov/api/views/jxy9-yhdk/rows.json?accessType=DOWNLOAD\"\n",
    "_json=requests.get(_url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* json데이터는 meta, data로 구분해서 만들어져 있슴\n",
    "* data는 52252건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'meta', u'data']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145570\n"
     ]
    }
   ],
   "source": [
    "_jsonList=_json['data']\n",
    "print len(_jsonList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " u'5DC7F285-052B-4739-8DC3-62827014A4CD',\n",
       " 1,\n",
       " 1425450997,\n",
       " u'714909',\n",
       " 1425450997,\n",
       " u'714909',\n",
       " u'{\\n}',\n",
       " u'2013',\n",
       " u'GAVIN',\n",
       " u'ST LAWRENCE',\n",
       " u'M',\n",
       " u'9']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_json['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* list to spark dataFrame\n",
    "    * schema를 정하지 않으면 없이 생성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145570"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df=spark.createDataFrame(_json['data'])\n",
    "_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* schema를 정하지 않았으므로 임의로 생성된 속성을 사용하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      " |-- _4: long (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: long (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: string (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      " |-- _13: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+----------+------+----------+------+---+----+-----+-----------+---+---+\n",
      "| _1|                  _2| _3|        _4|    _5|        _6|    _7| _8|  _9|  _10|        _11|_12|_13|\n",
      "+---+--------------------+---+----------+------+----------+------+---+----+-----+-----------+---+---+\n",
      "|  1|5DC7F285-052B-473...|  1|1425450997|714909|1425450997|714909|{\n",
      "}|2013|GAVIN|ST LAWRENCE|  M|  9|\n",
      "| 82|43E9414D-9BE0-456...| 82|1425450997|714909|1425450997|714909|{\n",
      "}|2013|GAVIN|    SUFFOLK|  M| 54|\n",
      "+---+--------------------+---+----------+------+----------+------+---+----+-----+-----------+---+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.filter(_df['_10'] == u'GAVIN').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* select를 사용해보자?\n",
    "    * pivotTable??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   _10|\n",
      "+------+\n",
      "|MILANA|\n",
      "|  JADE|\n",
      "|  ANNA|\n",
      "|HUNTER|\n",
      "|ANJALI|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.registerTempTable(\"babyNames\")\n",
    "spark.sql(\"select distinct(_10) from babyNames\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### read from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.txt\n",
    "Michael, 29\n",
    "Andy, 30\n",
    "Justin, 19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "lines = spark.sparkContext.textFile(\"/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name)\n",
    "for teenName in teenNames.collect():\n",
    "  print(teenName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: string]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "spark.createDataFrame(people,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, 2: int, 3: int, 4: int]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "\n",
    "#sqlContext = SQLContext(sc)\n",
    "#df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "#    .options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "df.show()\n",
    "\n",
    "df.withColumnRenamed('1','label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: Spark SQL Uber csv\n",
    "\n",
    "https://github.com/tmcgrath/spark-with-python-course/blob/master/Spark-SQL-CSV-with-Python.ipynb\n",
    "\n",
    "\n",
    "\n",
    "* fivethirtyeight\n",
    "    * git clone https://github.com/fivethirtyeight/uber-tlc-foil-response.git\n",
    "        daily Uber trip statistics in January and February 2015\n",
    "        ```\n",
    "        dispatching_base_number\tdate\tactive_vehicles\ttrips\n",
    "        B02512\t1/1/2015\t190\t1132\n",
    "        B02765\t1/1/2015\t225\t1765\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_home=os.path.join(os.environ['HOME'],\"Code/git/else/uber-tlc-foil-response\")\n",
    "filePath=os.path.join(data_home,\"Uber-Jan-Feb-FOIL.csv\")\n",
    "\n",
    "_fub = spark.sparkContext.textFile(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'dispatching_base_number,date,active_vehicles,trips'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_fub)\n",
    "_fub.count()\n",
    "_fub.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* csv는 comma seperated 형식이므로, ','로 분리\n",
    "* 첫번째 열에서 key값을 추출한다 (header값 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'B02682', u'B02512', u'dispatching_base_number', u'B02617', u'B02765', u'B02764', u'B02598']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dub = _fub.map(lambda line: line.split(\",\"))\n",
    "\n",
    "type(_dub)\n",
    "\n",
    "_row0keys=_dub.map(lambda row: row[0]).distinct().collect()\n",
    "\n",
    "print _row0keys\n",
    "\n",
    "_dub.filter(lambda row: \"B02512\" in row).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* B02512인 경우, trips가 2000보다 큰 레코드 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'B02512', u'1/30/2015', u'256', u'2016'],\n",
       " [u'B02512', u'2/5/2015', u'264', u'2022'],\n",
       " [u'B02512', u'2/12/2015', u'269', u'2092'],\n",
       " [u'B02512', u'2/13/2015', u'281', u'2408'],\n",
       " [u'B02512', u'2/14/2015', u'236', u'2055'],\n",
       " [u'B02512', u'2/19/2015', u'250', u'2120'],\n",
       " [u'B02512', u'2/20/2015', u'272', u'2380'],\n",
       " [u'B02512', u'2/21/2015', u'238', u'2149'],\n",
       " [u'B02512', u'2/27/2015', u'272', u'2056']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dub.filter(lambda row: \"B02512\" in row).filter(lambda row: int(row[3])>2000).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* header는 속성 명을 가지고 있다. 이를 제외하면 전체 갯수에서 1개를 뺀 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_noheader = _fub.filter(lambda line: \"base\" not in line).map(lambda line:line.split(\",\"))\n",
    "_noheader.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* reduceByKey - key별로 value를 합쳐서 결과 -> 아래는 a,3 b,2\n",
    "```\n",
    "(\"a\", 1)\n",
    "(\"b\", 1)\n",
    "(\"a\", 1)\n",
    "(\"a\", 1)\n",
    "(\"b\", 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'B02682', 662509),\n",
       " (u'B02512', 93786),\n",
       " (u'B02617', 725025),\n",
       " (u'B02765', 193670),\n",
       " (u'B02764', 1914449),\n",
       " (u'B02598', 540791)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_noheader.map(lambda x: (x[0], int(x[3]))).reduceByKey(lambda k,v: k + v).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* saving\n",
    "    ```\n",
    "    rddOfStrings.saveAsTextFile(\"out.txt\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 Dataframe\n",
    "\n",
    "http://www.cs.sfu.ca/CourseCentral/732/ggbaker/spark-sql.html\n",
    "\n",
    "\n",
    "* Data Frame은 DB 테이블\n",
    "    * MLib의 입력 데이터로 사용할 수 있다.\n",
    "        * 입력 데이터는 1) Spark RDDs 또는 2) DataFrame을 사용할 수 있다.\n",
    "        * 기본은 Data Frames (Pandas dataframe) (Spark 3.0 이후 DataFrame API)\n",
    "\n",
    "Pipeline | 설명 | 예\n",
    "----------|----------|----------\n",
    "DataFrame | text, feature vectors, true labels, and predictions.\n",
    "Transformer | DataFrame into another DataFrame | Transformer.transform()\n",
    "Estimator | fit on a DataFrame to produce a TransformerPipeline | Estimator.fit()\n",
    "Pipeline | chains multiple Transformers and Estimators together\n",
    "Parameter | a common API for specifying parameters. | ParamMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 기능\n",
    "\n",
    "기능 | 예제\n",
    "-------|-------\n",
    "json 읽기 | sqlContext.read.json(\"employee.json\")\n",
    "data 보기 | dfs.show()\n",
    "schema | dfs.printSchema()\n",
    "select | dfs.select(\"name\").show()\n",
    "filter | dfs.filter(dfs(\"age\") > 23).show()\n",
    "groupBy | dfs.groupBy(\"age\").count().show()\n",
    "select | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Spar DataFrame vs Pandas 의 비교\n",
    "\n",
    "DataFrame | Spark | Pandas\n",
    "-------|-------|-------\n",
    "csv file | map split(',') | read_csv()\n",
    "| show() | head(), tail()\n",
    "data types | 맞게 추정 | 모두 strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('name', 'height')\n",
    "rows = [Person('kim', 170), Person('lee', 175), Person('lim', 180),]\n",
    "#rowsRdd = sc.parallelize(rows)\n",
    "#rowsDf = sqlCtx.createDataFrame(rowsRdd)\n",
    "\n",
    "rowsDF=spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rowsDF.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rowsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|height|\n",
      "+----+------+\n",
      "| kim|   170|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowsDF.where(rowsDF.height < 175)\\\n",
    "    .select([rowsDF.name, rowsDF.height]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|height|max(height)|\n",
      "+------+-----------+\n",
      "|   170|        170|\n",
      "|   175|        175|\n",
      "|   180|        180|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowsDF.groupby(rowsDF.height).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-13: spark-submit\n",
    "\n",
    "* spark-defaults.conf\n",
    "    * packages 여러개를 넣을 경우 컴마로 분리\n",
    "\n",
    "* spark-submit (self-contained app in quick-start 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/jsl/Code/git/bb/jsl/pyds'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### sql, file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_sql.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_sql.py\n",
    "import pyspark\n",
    "#conf = pyspark.SparkConf().setAppName(\"myAppName1\")\n",
    "#sc   = pyspark.SparkContext(conf=conf)\n",
    "#sc.setLogLevel(\"ERROR\")\n",
    "def doIt():\n",
    "    from operator import add\n",
    "    lines = spark.sparkContext.textFile(\"README.md\")\n",
    "    word_count_bo = lines\\\n",
    "        .flatMap(lambda x: x.split(' '))\\\n",
    "        .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "        .reduceByKey(add)\n",
    "    print word_count_bo.count()\n",
    "\n",
    "    #from pyspark.sql import SQLContext\n",
    "    #sqlCtx = SQLContext(sc)\n",
    "    d = [{'name': 'Alice', 'age': 1}]\n",
    "    print spark.createDataFrame(d).collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* spark-submit을 실행하기 전, 'conf/log4j.properties'를 수정 log level을 ERROR로 설정하였다.\n",
    "```\n",
    "log4j.rootCategory=ERROR, console\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/session.py:316: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n",
      "[Row(age=1, name=u'Alice')]\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_sql.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MongoDB Spark connector\n",
    "\n",
    "* 참고 MongoDB의 Spark 연결 설명 https://docs.mongodb.com/spark-connector/\n",
    "\n",
    "* spark-defaults.conf 수정 (MongoDB<3.2인 경우 spark.mongodb.input.partitioner가 필요하다)\n",
    "```\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.jars.packages=org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MongoDB Python API Basics\n",
    "\n",
    "* MongoDB에 쓰기\n",
    "    * DataFrame을 생성\n",
    "```\n",
    "spark.createDataFrame()\n",
    "```\n",
    "    * DataFrame을 MongoDB로 저장\n",
    "```\n",
    "write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()\n",
    "```\n",
    "\n",
    "* MongoDB을 읽기 (collection을 DataFrame으로)\n",
    "    * database, collection은 spark.mongodb.input.uri로 설정해 놓음\n",
    "    * format은 \"com.mongodb.spark.sql.DefaultSource\"로 정해놓음.\n",
    "```\n",
    "spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_mongo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_mongo.py\n",
    "import pyspark\n",
    "#conf = pyspark.SparkConf().setAppName(\"myAppName1\")\n",
    "#sc   = pyspark.SparkContext(conf=conf)\n",
    "#sc.setLogLevel(\"ERROR\")\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print \"------mongodb write-------\"\n",
    "    myRdd = spark.sparkContext.parallelize([\n",
    "        (\"js\", 150),\n",
    "        (\"Gandalf\", 1000),\n",
    "        (\"Thorin\", 195),\n",
    "        (\"Balin\", 178),\n",
    "        (\"Kili\", 77),\n",
    "        (\"Dwalin\", 169),\n",
    "        (\"Oin\", 167),\n",
    "        (\"Gloin\", 158),\n",
    "        (\"Fili\", 82),\n",
    "        (\"Bombur\", None)\n",
    "    ])\n",
    "    myDf = spark.createDataFrame(myRdd, [\"name\", \"age\"])\n",
    "    print myDf\n",
    "    myDf.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()\n",
    "    print \"---------read-----------\"\n",
    "    df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "    print df.printSchema()\n",
    "    df.registerTempTable(\"myTable\")\n",
    "    myTab = spark.sql(\"SELECT name, age FROM myTable WHERE age >= 100\")\n",
    "    myTab.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.10 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.1.0-spark1.6 in spark-packages\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.10;1.3.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.10/2.0.0/mongo-spark-connector_2.10-2.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.10;2.0.0!mongo-spark-connector_2.10.jar (1015ms)\n",
      ":: resolution report :: resolve 2916ms :: artifacts dl 1029ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.10;1.3.0 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.1.0-spark1.6 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   1   |   1   |   0   ||   6   |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 5 already retrieved (644kB/19ms)\n",
      "---------RESULT-----------\n",
      "------mongodb write-------\n",
      "DataFrame[name: string, age: bigint]\n",
      "---------read-----------\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|     js| 150|\n",
      "|Gandalf|1000|\n",
      "| Thorin| 195|\n",
      "|  Balin| 178|\n",
      "| Dwalin| 169|\n",
      "|    Oin| 167|\n",
      "|  Gloin| 158|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_mongo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 내 사례 mongodb twitter\n",
    "\n",
    "json을 읽을 경우, \n",
    "CardSubwayStatisticsService.row.RIDE_PASGR_NUM\n",
    "```\n",
    "$ mongo\n",
    "> use ds_rest_subwayPassengers_mongo_db\n",
    "switched to db ds_rest_subwayPassengers_mongo_db\n",
    "> show tables\n",
    "db_rest_subway\n",
    "system.indexes\n",
    "> db.db_rest_subway.find().limit(1)\n",
    "{ \"_id\" : ObjectId(\"57fa386ff5e6e94359c033e9\"), \"CardSubwayStatisticsService\" : { \"row\" : [ { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 111275, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"용문\", \"ALIGHT_PASGR_NUM\" : 108878, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 11495, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"원덕\", \"ALIGHT_PASGR_NUM\" : 10964, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 118103, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"양평\", \"ALIGHT_PASGR_NUM\" : 116604, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 10590, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"오빈\", \"ALIGHT_PASGR_NUM\" : 10020, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 26304, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"아신\", \"ALIGHT_PASGR_NUM\" : 26358, \"USE_MON\" : \"201306\" } ], \"RESULT\" : { \"MESSAGE\" : \"정상 처리되었습니다\", \"CODE\" : \"INFO-000\" }, \"list_total_count\" : 530 } }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_twitter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_twitter.py\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "conf.set(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway?readPreference=primaryPreferred\")\n",
    "conf.set(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "#sc = pyspark.SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print sc._conf.getAll()\n",
    "sqlContext = pyspark.sql.SQLContext(sc)\n",
    "print \"---------read-----------\"\n",
    "df = sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "print df.printSchema()\n",
    "df.registerTempTable(\"myTwitter\")\n",
    "myTab = sqlContext.sql(\"SELECT CardSubwayStatisticsService.row.RIDE_PASGR_NUM FROM myTwitter\")\n",
    "print type(myTab)\n",
    "myTab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.1.0-spark1.6 in spark-packages\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;1.1.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      ":: resolution report :: resolve 144ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.1.0-spark1.6 from spark-packages in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;1.1.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "16/10/30 21:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/10/30 21:45:04 WARN Utils: Your hostname, jsl-smu resolves to a loopback address: 127.0.1.1; using 117.16.44.45 instead (on interface eth0)\n",
      "16/10/30 21:45:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "[(u'spark.app.name', u'myAppName'), (u'spark.mongodb.output.uri', u'mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway'), (u'spark.rdd.compress', u'True'), (u'spark.jars', u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'), (u'spark.jars.packages', u'graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0'), (u'spark.serializer.objectStreamReset', u'100'), (u'spark.master', u'local[*]'), (u'spark.submit.deployMode', u'client'), (u'spark.files', u'file:/home/jsl/Code/git/bb/jsl/pyds/src/ds_spark_twitter.py,file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'), (u'spark.submit.pyFiles', u'/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'), (u'spark.mongodb.input.uri', u'mongodb://127.0.0.1/ds_rest_subwayPassengers_mongo_db.db_rest_subway?readPreference=primaryPreferred'), (u'spark.mongodb.input.partitioner', u'MongoPaginateBySizePartitioner')]\n",
      "---------read-----------\n",
      "root\n",
      " |-- CardSubwayStatisticsService: struct (nullable = true)\n",
      " |    |-- row: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- COMMT: string (nullable = true)\n",
      " |    |    |    |-- RIDE_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- WORK_DT: string (nullable = true)\n",
      " |    |    |    |-- LINE_NUM: string (nullable = true)\n",
      " |    |    |    |-- SUB_STA_NM: string (nullable = true)\n",
      " |    |    |    |-- ALIGHT_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- USE_MON: string (nullable = true)\n",
      " |    |-- RESULT: struct (nullable = true)\n",
      " |    |    |-- MESSAGE: string (nullable = true)\n",
      " |    |    |-- CODE: string (nullable = true)\n",
      " |    |-- list_total_count: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n",
      "None\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+\n",
      "|      RIDE_PASGR_NUM|\n",
      "+--------------------+\n",
      "|[111275.0, 11495....|\n",
      "|[30281.0, 10832.0...|\n",
      "|[75553.0, 189783....|\n",
      "|[62789.0, 220544....|\n",
      "|[74486.0, 152381....|\n",
      "|[43418.0, 413533....|\n",
      "|[301856.0, 37978....|\n",
      "|[146909.0, 227066...|\n",
      "|[152275.0, 285263...|\n",
      "|[161298.0, 117720...|\n",
      "|[95996.0, 39150.0...|\n",
      "|[59900.0, 201035....|\n",
      "|[228737.0, 108938...|\n",
      "|[164574.0, 481998...|\n",
      "|[748205.0, 817657...|\n",
      "|[206631.0, 188076...|\n",
      "|[112991.0, 111791...|\n",
      "|[225105.0, 938296...|\n",
      "|[175909.0, 271844...|\n",
      "|[45047.0, 126837....|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/bin/spark-submit src/ds_spark_twitter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* all-in-one 위를 spark-submit아닌 것으로 풀기\n",
    "* sc가 또 생성되지 않도록 주의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'spark.app.name', u'myAppName'), (u'spark.mongodb.input.uri', u'mongodb://127.0.0.1/ds_rest_subwayPassengersDb.db_rest_subwayTable?readPreference=primaryPreferred'), (u'spark.submit.pyFiles', u'/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,/home/jsl/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar,/home/jsl/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,/home/jsl/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'), (u'spark.rdd.compress', u'True'), (u'spark.serializer.objectStreamReset', u'100'), (u'spark.master', u'local[*]'), (u'spark.mongodb.output.uri', u'mongodb://127.0.0.1/ds_rest_subwayPassengersDb.db_rest_subwayTable'), (u'spark.submit.deployMode', u'client'), (u'spark.jars', u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar,file:/home/jsl/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/jsl/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'), (u'spark.files', u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/com.databricks_spark-csv_2.10-1.3.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar,file:/home/jsl/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/jsl/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar'), (u'spark.jars.packages', u'graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0,com.databricks:spark-csv_2.10:1.3.0'), (u'spark.mongodb.input.partitioner', u'MongoPaginateBySizePartitioner')]\n",
      "graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0,com.databricks:spark-csv_2.10:1.3.0\n",
      "---------read-----------\n",
      "root\n",
      " |-- CardSubwayStatisticsService: struct (nullable = true)\n",
      " |    |-- row: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- COMMT: string (nullable = true)\n",
      " |    |    |    |-- RIDE_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- WORK_DT: string (nullable = true)\n",
      " |    |    |    |-- LINE_NUM: string (nullable = true)\n",
      " |    |    |    |-- SUB_STA_NM: string (nullable = true)\n",
      " |    |    |    |-- ALIGHT_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- USE_MON: string (nullable = true)\n",
      " |    |-- RESULT: struct (nullable = true)\n",
      " |    |    |-- MESSAGE: string (nullable = true)\n",
      " |    |    |-- CODE: string (nullable = true)\n",
      " |    |-- list_total_count: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n",
      "None\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+\n",
      "|      RIDE_PASGR_NUM|\n",
      "+--------------------+\n",
      "|[111275.0, 11495....|\n",
      "|[30281.0, 10832.0...|\n",
      "|[75553.0, 189783....|\n",
      "|[62789.0, 220544....|\n",
      "|[74486.0, 152381....|\n",
      "|[43418.0, 413533....|\n",
      "|[301856.0, 37978....|\n",
      "|[146909.0, 227066...|\n",
      "|[152275.0, 285263...|\n",
      "|[161298.0, 117720...|\n",
      "|[95996.0, 39150.0...|\n",
      "|[59900.0, 201035....|\n",
      "|[228737.0, 108938...|\n",
      "|[164574.0, 481998...|\n",
      "|[748205.0, 817657...|\n",
      "|[206631.0, 188076...|\n",
      "|[112991.0, 111791...|\n",
      "|[225105.0, 938296...|\n",
      "|[175909.0, 271844...|\n",
      "|[45047.0, 126837....|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "home=os.getenv(\"HOME\")\n",
    "spark_home=os.path.join(home,\"Downloads/spark-1.6.0-bin-hadoop2.6\")\n",
    "findspark.init(spark_home)\n",
    "\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "conf.set(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengersDb.db_rest_subwayTable?readPreference=primaryPreferred\")\n",
    "conf.set(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/ds_rest_subwayPassengersDb.db_rest_subwayTable\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "#sc = pyspark.SparkContext()\n",
    "print sc._conf.getAll()\n",
    "print sc._conf.get(\"spark.jars.packages\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = pyspark.sql.SQLContext(sc)\n",
    "print \"---------read-----------\"\n",
    "df = sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "print df.printSchema()\n",
    "df.registerTempTable(\"myTwitter\")\n",
    "myTab = sqlContext.sql(\"SELECT CardSubwayStatisticsService.row.RIDE_PASGR_NUM FROM myTwitter\")\n",
    "print type(myTab)\n",
    "myTab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(RIDE_PASGR_NUM=[111275.0, 11495.0, 118103.0, 10590.0, 26304.0])\n",
      "Row(RIDE_PASGR_NUM=[111275.0, 11495.0, 118103.0, 10590.0, 26304.0])\n"
     ]
    }
   ],
   "source": [
    "print myTab.first()\n",
    "print myTab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 더 해보기\n",
    "\n",
    "* Spark MySql\n",
    "```\n",
    "spark.driver.extraLibraryPath\n",
    "os.environ['SPARK_CLASSPATH'] =r\"/home/jsl/sd/lib/mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar\"\n",
    "```\n",
    "\n",
    "* postgresql\n",
    "```[not yet] postgresql\n",
    "test.write.jdbc(\n",
    "    url=\"jdbc:postgresql://localhost:5432/db\", \n",
    "    table=\"test\", \n",
    "    mode=\"overwrite\", \n",
    "    properties={\n",
    "        \"user\":\"root\", \n",
    "        \"password\":\"12345\", \n",
    "        \"driver\":\"org.postgresql.Driver\", \n",
    "        \"client_encoding\":\"utf8\"\n",
    "   }\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
