{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "Last updated 17:15 20161108 word count S-7\n",
    "    * word count by line\n",
    "    * 빅데이터 Giga급 서버에 준비\n",
    "    \n",
    "## 목적\n",
    "\n",
    "* Spark를 사용하여 데이터를 변환할 수 있다.\n",
    "* Spark를 사용하여 통계분석을 할 수 있다.\n",
    "* Spark를 사용하여 추천, 분류, 예측을 분석할 수 있다.\n",
    "\n",
    "## 문제\n",
    "\n",
    "* 문제 S-1: standalone cluster 구성\n",
    "* 문제 S-2: Hello Spark\n",
    "* 문제 S-3: Hello RDD\n",
    "    * fivethirtyeight -> data_home=os.path.join(home,\"Code/git/else/data\")\n",
    "* 문제 S-4: RDD word count\n",
    "    * feature extraction - word vector\n",
    "    * stopwords (nltk.corpus.stopwords)\n",
    "* 문제 S-5: RDD from url\n",
    "    * .toDF (rdd -> dataframe)\n",
    "    * 문제 rdd babynames\n",
    "* 문제 S-6: Spark SQL - SqlContext\n",
    "* 문제 S-7: Spark Data frame\n",
    "    * 문제 Spark SQL json\n",
    "    * 문제 spark sql uber csv\n",
    "* 문제 S-8: Hello Statistics\n",
    "* 문제 S-9: Spark data type\n",
    "* 문제 libsvm format\n",
    "* 문제 S-10: Spark MLib Clustering: twitter _my.json, tf-idf\n",
    "* 문제 S-11: Spark MLib Decision Tree\n",
    "* 문제 S-12: Spark MLib Logistic regression\n",
    "    * https://www.codementor.io/spark/tutorial/spark-mllib-logistic-regression\n",
    "* 문제 S-13: Spark MLib movie recommendation 사례\n",
    "* 문제 S-14: Spark Streaming\n",
    "* 문제 S-15: GraphX\n",
    "* 문제 시각화 Bokeh\n",
    "http://www.blog.pythonlibrary.org/2016/07/27/python-visualization-with-bokeh/\n",
    "\n",
    "* spark-submit (self-contained app in quick-start 참조)\n",
    "\n",
    "* ref\n",
    "    * introduction to big data with apache spark (Berkley Anthony Joseph)\n",
    "    * [spark-sklearn](https://github.com/databricks/spark-sklearn)\n",
    "        ```\n",
    "        pip install spark-sklearn\n",
    "        ```\n",
    "        \n",
    "        * spark-shell\n",
    "        ```\n",
    "        $SPARK_HOME/bin/spark-shell --packages databricks:spark-sklearn:0.2.0\n",
    "        ```\n",
    "        \n",
    "    * 듀크대학 STA663 Statistical Computing and Computation, Spring 2016\n",
    "        * https://github.com/cliburn/sta-663-2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.1 설치\n",
    "\n",
    "* 설치하려는 하둡의 버전을 선택하여, prebuilt distribution을 설치한다.\n",
    "* [Spark 다운로드](https://spark.apache.org/downloads.html)\n",
    "    * spark 1.6 hadoop2.6 \n",
    "        ```\n",
    "        tar xvfz spark-1.x.x-bin-hadoop2.x.tgz\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.2 Spark 소개\n",
    "\n",
    "* 버전\n",
    "    * 2009년 UC Berkeley, Matei Zaharia 박사과정 하면서 개뱔\n",
    "    * 2010년 BSD 라이센스 오픈소스로 전환.\n",
    "    * 2013년 Apache 2.0 license로 전환\n",
    "    * 현재 개발자가 Databricks를 설립해서 관리\n",
    "\n",
    "* 왜 Spark를 배워야 하는가?\n",
    "    * REPL도 가능해서 배우기 쉽다. 쉘 환경이 있어 편리하다. Standalone으로 시작할 수 있다.\n",
    "    * 빅데이터를 빠르게 map reduce 할 수 있다.\n",
    "    * Machine learning lib를 가지고 있다.\n",
    "\n",
    "* 구조\n",
    "    * Spark Core\n",
    "    * 분산 클러스터 컴퓨팅 프레임워크로서, API를 사용해서 데이터추출, 변환, 기계학습, 그래프분석을 할 수 있다.\n",
    "\n",
    "구분 | 구성 | 설명\n",
    "-------|-------|-------\n",
    "Spark engine | Spark Core |\n",
    "Spark Applicaiton Frameworks | Spark SQL | DataFrames\n",
    "| Spark Streaming | \n",
    "| MLlib | 머신러닝 (참조 scikit-learn)\n",
    "|GraphX | 그래프 분석\n",
    "\n",
    "\n",
    "    * 빅데이터를 처리하기 위해 만들어져 있고, Hadoop과 달리 메모리에서 처리하기 때문에 빠름 (pipeline). Spark는 RDD를 통해 Hadoop을 사용할 수 있다.\n",
    "    * Scala로 개발되어 jvm에서 실행. 그러나 Scala, Java, Python, R 여러 언어를 섞어서 할 수 있는 환경을 제공 (polyglot)\n",
    "\n",
    "구분 | Spark | Hadoop\n",
    "-------|-------|-------\n",
    "사용 목적 | 데이터 분석 | 데이터 분산 처리\n",
    "파일 시스템 | 자체 파일 시스템이 없슴. hdfs, db, csv등을 사용 | hdfs\n",
    "속도 | 파이프라인을 사용하므로 빠름 | 보다 느림\n",
    "\n",
    "* batch, streaming, iterative, interactive 4가지 방식으로 실행할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 문제 S-1: standalone cluster 구성\n",
    "\n",
    "* 클러스터를 구성하지 않으면 클러스터 없이 운영 - curl로 7077, 8080확인해도 없음 -> NO cluster!\n",
    "\n",
    "sbin디렉토리의 shell | 설명\n",
    "----------|----------\n",
    "start-master.sh, stop-master.sh | 마스터를 시작 (종료)\n",
    "start-slaves.sh, stop-slaves.sh | 각 노드의 슬레이브를 시작 (종료)\n",
    "start-all.sh, stop-all.sh | 마스터, 슬레이브를 모두 시작 (종료)\n",
    "\n",
    "* 클러스터 종류\n",
    "    * Spark-Standalone – Spark workers are registered with spark master\n",
    "    * Yarn – Spark workers are registered with YARN Cluster manager.\n",
    "    * Mesos – Spark workers are registered with Mesos.\n",
    "\n",
    "*  Spark-Standalone\n",
    "    * SPARK_HOME은 /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/\n",
    "    * 단계1 JAVA_HOME을 설정\n",
    "        * JAVA_HOME을 설정한다.\n",
    "            * automatically set symlink to java binary /usr/bin/java\n",
    "            * JAVA_HOME을 설정하려면 /etc/environment에 하는 것이 좋다.\n",
    "            ```\n",
    "            $ echo $JAVA_HOME\n",
    "            $ update-alternatives --config java\n",
    "            ```\n",
    "\n",
    "    * 단계2: master 실행\n",
    "        * $SPARK_HOME/conf/spark-env.sh에 master ip설정\n",
    "            ```\n",
    "            SPARK_MASTER_IP=\n",
    "            ```\n",
    "\n",
    "        * 실행\n",
    "            ```\n",
    "            $ sh $SPARK_HOME/sbin/start-master.sh\n",
    "            ```\n",
    "            * spark://IPADRESS_OF_YOUR_MASTER_SYSTEM:7077\n",
    "            * 기본 port는 7077 (web UI는 localhost:8080)\n",
    "\n",
    "    * 단계3: slave 실행 (worker라고 함)\n",
    "        ```\n",
    "        $ sh $SPARK_HOME/sbin//start-slave.sh spark://IPADRESS_OF_YOUR_MASTER_SYSTEM:7077\n",
    "        ```\n",
    "\n",
    "* 환경 구성\n",
    "    * Client: spark shell, pyspark shell\n",
    "    * Cluster\n",
    "        * Cluster 1:\n",
    "            * Spark master / Spark worker\n",
    "            * hdfs namenode / datanode\n",
    "        * Cluster n:\n",
    "            * Spark worker\n",
    "            * hdfs datanode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-2: Hello Spark\n",
    "\n",
    "### spark-submit\n",
    "* spark 프로그램을 일괄 실행\n",
    "\n",
    "### insteractive shell\n",
    "\n",
    "* scala\n",
    "    ```\n",
    "    ./bin/spark-shell\n",
    "    scala>\n",
    "    ```\n",
    "\n",
    "* python\n",
    "    ```\n",
    "    spark-1.6.0-bin-hadoop2.6/bin$ pyspark\n",
    "    Welcome to\n",
    "          ____              __\n",
    "         / __/__  ___ _____/ /__\n",
    "        _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "       /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
    "          /_/\n",
    "\n",
    "    Using Python version 2.7.12 (default, Jul  1 2016 15:12:24)\n",
    "    SparkContext available as sc, HiveContext available as sqlContext.\n",
    "    >>> sc.version\n",
    "    u'1.6.0'\n",
    "    >>> text=sc.textFile(\"derby.log\");\n",
    "    >>>\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SparkContext\n",
    "    * Spark서버(클러스터)에 대한 클라이언트와 같은 역할로, 반드시 있어야 한다.\n",
    "    * 클러스터를 어떻게 사용할 것인지 정하는 것 -> cluster manager에서 system resource를 할당받음 (cpu, memory, machine)\n",
    "    * Python의 SparkContext는 jar를 분산환경에서 사용하게 되므로 주의 (Scala, Java와 다름)\n",
    "        * pyFiles에 사용할 (의존적인) 라이브러리를 넣는다 (또는 사용할 라이브러리가 없으면 빈 파일로 둔다).\n",
    "\n",
    "    * Cannot run multiple SparkContexts at once;\n",
    "        * sc가 이미 있는 경우\n",
    "        ```\n",
    "        from pyspark import SparkContext\n",
    "        sc = SparkContext(\"master\",\"my python app\", sparkHome=\"sparkhome\",pyFiles=\"placeholderdeps.zip\")\n",
    "        ```\n",
    "\n",
    "* SparkConf\n",
    "    * spark-defaults.conf와 같은 파일의 값을 읽어서 설정\n",
    "    ```\n",
    "    scala> sc.getConf.getOption(\"spark.local.dir\")\n",
    "    res0: Option[String] = None\n",
    "\n",
    "    scala> sc.getConf.getOption(\"spark.app.name\")\n",
    "    res1: Option[String] = Some(Spark shell)\n",
    "\n",
    "    scala> sc.getConf.get(\"spark.master\")\n",
    "    res2: String = local[*]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark on IPython Notebook\n",
    "\n",
    "* ipython notebook에서 pyspark를 사용\n",
    "* kernel을 만들지 않고, findspark를 사용한다.\n",
    "* SPARK_HOME을 설정해서 사용한다\n",
    "    * 현재 ~/Downloads/spark-1.6.0-bin-hadoop2.6\n",
    "* kernel을 사용하면 아래와 같이 한다.\n",
    "    ```\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    conf = SparkConf().setAppName(\"jsl\").setMaster(\"local\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "home=os.getenv(\"HOME\")\n",
    "spark_home=os.path.join(home,\"Downloads/spark-1.6.0-bin-hadoop2.6\")\n",
    "findspark.init(spark_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SparkConf()에서 설정한 값을 읽어 SparkContext sc를 생성한다.\n",
    "    * spark.master와 spark.app.name 필수적으로 설정해야 한다.\n",
    "* standalone 실행에서의 spark master\n",
    "    * local로 설정되고, 기본은 필요한만큼 쓰레드를 생성한다 (local[*])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f511e6939d0>\n"
     ]
    }
   ],
   "source": [
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 설정을 읽어 온다 (conf디렉토리 아래)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.get(\"spark.jars.packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.app.name', u'myAppName'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.jars',\n",
       "  u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'),\n",
       " (u'spark.jars.packages',\n",
       "  u'graphframes:graphframes:0.1.0-spark1.6,org.mongodb.spark:mongo-spark-connector_2.10:1.1.0'),\n",
       " (u'spark.files',\n",
       "  u'file:/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,file:/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,file:/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.master', u'local[*]'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.submit.pyFiles',\n",
       "  u'/home/jsl/.ivy2/jars/graphframes_graphframes-0.1.0-spark1.6.jar,/home/jsl/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.10-1.1.0.jar,/home/jsl/.ivy2/jars/org.mongodb_mongo-java-driver-3.2.2.jar'),\n",
       " (u'spark.mongodb.input.partitioner', u'MongoPaginateBySizePartitioner')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-3: Hello RDD\n",
    "\n",
    "* RDD (Resilient Distributed Dataset)\n",
    "    * Resilient - fault tolerent\n",
    "    * Distributed - multiple nodes in a clusters\n",
    "    * Dataset - \n",
    "* 분산 데이터 자료\n",
    "* HDFS 파일을 처리할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark를 사용하기 전, python 함수 사용하기\n",
    "\n",
    "* map, reduce, filter\n",
    "    * 함수의 인자는 2개가 필요하다 (함수, 데이터).\n",
    "\n",
    "함수 | 설명 | 예\n",
    "-------|-------|-------\n",
    "map() | 각 데이터 요소에 함수를 적용해서 list를 반환 | map(fn,data)\n",
    "filter() | 각 데이터 요소에 함수의 결과 True를 선택해서 반환 | filter(fn, data)\n",
    "reduce() | 각 데이터 요소에 함수를 적용해서 list를 반환 | reduce(fn, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일반적인 함수로 처리하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* map()을 사용하는 경우\n",
    "    * map에 인자로 넘겨주는 함수는 return을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, celsius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lambda함수를 사용하는 경우\n",
    "    * lambda는 무명의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5)*c + 32, celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자열에 map()을 적용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hello World'\n",
    "words = sentence.split()\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자열을 사용하면, 각 단어를 split()한다.\n",
    "* list를 사용하면, 각 요소를 split()한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello World\"\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"Hello World\"]\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, range(1,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD\n",
    "\n",
    "* 3단계\n",
    "    * 1단계: 읽기 - 2가지 방식: \n",
    "        * 외부에서 읽기 \n",
    "        ```\n",
    "        sc.textFile()\n",
    "        ```\n",
    "        \n",
    "        * 내부에서 읽기 parallelizing a collection\n",
    "        ```\n",
    "        sc.parallelize()\n",
    "        ```\n",
    "        \n",
    "    * 2단계: 변환 transformations - lazy도 가능: RDD => RDD or seq(RDD)\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "map(fn) | 요소별로 fn을 적용해서 결과 rdd 돌려줌 | map\n",
    "filter(fn) | 요소별로 선별하여 fn을 적용해서 결과 rdd 돌려줌 | filter(lambda line: \"Spark\" in line)\n",
    "flatMap(fn) | 요소별로 fn을 적용하고, flat해서 결과 rdd 돌려줌 | .flatMap(lambda x: x.split(' '))\n",
    "groupByKey() | |\n",
    "\n",
    "\n",
    "    * 3단계: actions: RDD => a value (e.g., python list)\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "reduce(fn) | 요소별로 fn을 사용해서 줄여서 결과 list를 돌려줌 |\n",
    "collect() | 모든 요소를 결과 list로 돌려줌 |\n",
    "count() | 요소의 갯수를 결과 list로 돌려줌 |\n",
    "countByKey() | |\n",
    "foreach(fn) | |\n",
    "\n",
    "\n",
    "* saving\n",
    "```\n",
    "rddOfStrings.saveAsTextFile(\"out.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 파일에서 읽기\n",
    "    ```\n",
    "    textFile()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"data/ds_spark_wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wikipedia'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* map()함수로 단어 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=textFile.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.'],\n",
       " [u'Originally',\n",
       "  u'developed',\n",
       "  u'at',\n",
       "  u'the',\n",
       "  u'University',\n",
       "  u'of',\n",
       "  u'California,',\n",
       "  u\"Berkeley's\",\n",
       "  u'AMPLab,'],\n",
       " [u'the',\n",
       "  u'Spark',\n",
       "  u'codebase',\n",
       "  u'was',\n",
       "  u'later',\n",
       "  u'donated',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'Apache',\n",
       "  u'Software',\n",
       "  u'Foundation,'],\n",
       " [u'which', u'has', u'maintained', u'it', u'since.'],\n",
       " [u'Spark',\n",
       "  u'provides',\n",
       "  u'an',\n",
       "  u'interface',\n",
       "  u'for',\n",
       "  u'programming',\n",
       "  u'entire',\n",
       "  u'clusters',\n",
       "  u'with'],\n",
       " [u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 문장의 철자 갯수를 센다.\n",
    "    * 첫 문장 'Wikipedia'는 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_sparkLine=textFile.filter(lambda line: \"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print _sparkLine.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한글은 앞에 u를 붙여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_line = textFile.filter(lambda line: u\"스파크\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "print _line.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parallelize() 사용하기\n",
    "    * list에서 읽어, rdd로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_aList=[1,2,3]\n",
    "rdd = sc.parallelize(_aList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* map(), collect() 사용해서 square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "print squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장 처리하기\n",
    "* 단어를 교체하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "a=[\"this is\",\"a line\"]\n",
    "_rdd=sc.parallelize(a)\n",
    "\n",
    "words=_rdd.map(lambda x:x.split())\n",
    "print words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_upper=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "_upper.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 첫 글자를 대문자로 만들어서 출력해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: x[0].upper())\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: [i.upper() for i in x])\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transformation(map()), action(collect()) 함수를 한꺼번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "pluralRDD =words.map(lambda x: [i.upper() for i in x]).collect()\n",
    "print pluralRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = words\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print wordsLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* create RDD from CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp_file = sc.textFile(\"./data/ds_spark_2cols.csv\")\n",
    "numbers_rdd = inp_file.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1'],\n",
       " [u'14', u' 19'],\n",
       " [u'46', u' 1'],\n",
       " [u'10', u' 34'],\n",
       " [u'28', u' 3'],\n",
       " [u'48', u' 1']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* fivethirtyeight\n",
    "    * git clone https://github.com/fivethirtyeight/uber-tlc-foil-response.git\n",
    "        daily Uber trip statistics in January and February 2015\n",
    "        ```\n",
    "        dispatching_base_number\tdate\tactive_vehicles\ttrips\n",
    "        B02512\t1/1/2015\t190\t1132\n",
    "        B02765\t1/1/2015\t225\t1765\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_home=os.path.join(home,\"Code/git/else/uber-tlc-foil-response\")\n",
    "filePath=os.path.join(data_home,\"Uber-Jan-Feb-FOIL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_fub = sc.textFile(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_fub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_fub.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'dispatching_base_number,date,active_vehicles,trips'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_fub.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* csv는 comma seperated 형식이므로, ','로 분리\n",
    "* 첫번째 열에서 key값을 추출한다 (header값 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_dub = _fub.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_dub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_row0keys=_dub.map(lambda row: row[0]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'B02617', u'B02682', u'B02598', u'B02765', u'B02512', u'dispatching_base_number', u'B02764']\n"
     ]
    }
   ],
   "source": [
    "print _row0keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dub.filter(lambda row: \"B02512\" in row).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* B02512인 경우, trips가 2000보다 큰 레코드 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'B02512', u'1/30/2015', u'256', u'2016'],\n",
       " [u'B02512', u'2/5/2015', u'264', u'2022'],\n",
       " [u'B02512', u'2/12/2015', u'269', u'2092'],\n",
       " [u'B02512', u'2/13/2015', u'281', u'2408'],\n",
       " [u'B02512', u'2/14/2015', u'236', u'2055'],\n",
       " [u'B02512', u'2/19/2015', u'250', u'2120'],\n",
       " [u'B02512', u'2/20/2015', u'272', u'2380'],\n",
       " [u'B02512', u'2/21/2015', u'238', u'2149'],\n",
       " [u'B02512', u'2/27/2015', u'272', u'2056']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dub.filter(lambda row: \"B02512\" in row).filter(lambda row: int(row[3])>2000).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* header는 속성 명을 가지고 있다. 이를 제외하면 전체 갯수에서 1개를 뺀 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_noheader = _fub.filter(lambda line: \"base\" not in line).map(lambda line:line.split(\",\"))\n",
    "_noheader.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduceByKey - key별로 value를 합쳐서 결과 -> 아래는 a,3 b,2\n",
    "```\n",
    "(\"a\", 1)\n",
    "(\"b\", 1)\n",
    "(\"a\", 1)\n",
    "(\"a\", 1)\n",
    "(\"b\", 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'B02617', 725025),\n",
       " (u'B02682', 662509),\n",
       " (u'B02598', 540791),\n",
       " (u'B02765', 193670),\n",
       " (u'B02512', 93786),\n",
       " (u'B02764', 1914449)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_noheader.map(lambda x: (x[0], int(x[3]))).reduceByKey(lambda k,v: k + v).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-4: word count\n",
    "\n",
    "* RDD API를 사용해서 단어를 셀 수 있다 (map, reduce 등).\n",
    "* DataFrame API를 사용하려면 MLib Pipleline을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ds_spark_wiki.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/ds_spark_wiki.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "words=textFile.map(mySplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in words.collect():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lambda함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=textFile.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체를 묶어서 word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"data/ds_spark_wiki.txt\")\n",
    "word_count_bo = lines\\\n",
    "    .flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'an',\n",
       " u'open',\n",
       " u'source',\n",
       " u'cluster',\n",
       " u'computing',\n",
       " u'framework.',\n",
       " u'\\uc544\\ud30c\\uce58',\n",
       " u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       " u'\\uc624\\ud508',\n",
       " u'\\uc18c\\uc2a4',\n",
       " u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       " u'\\ucef4\\ud4e8\\ud305',\n",
       " u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Originally',\n",
       " u'developed',\n",
       " u'at',\n",
       " u'the',\n",
       " u'University',\n",
       " u'of',\n",
       " u'California,',\n",
       " u\"Berkeley's\",\n",
       " u'AMPLab,',\n",
       " u'the',\n",
       " u'Spark',\n",
       " u'codebase',\n",
       " u'was',\n",
       " u'later',\n",
       " u'donated',\n",
       " u'to',\n",
       " u'the',\n",
       " u'Apache',\n",
       " u'Software',\n",
       " u'Foundation,',\n",
       " u'which',\n",
       " u'has',\n",
       " u'maintained',\n",
       " u'it',\n",
       " u'since.',\n",
       " u'Spark',\n",
       " u'provides',\n",
       " u'an',\n",
       " u'interface',\n",
       " u'for',\n",
       " u'programming',\n",
       " u'entire',\n",
       " u'clusters',\n",
       " u'with',\n",
       " u'implicit',\n",
       " u'data',\n",
       " u'parallelism',\n",
       " u'and',\n",
       " u'fault-tolerance.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_bo.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어를 세어서 tuple로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "lines = sc.textFile(\"data/ds_spark_wiki.txt\")\n",
    "word_count_bo = lines\\\n",
    "    .flatMap(lambda x: x.split(' '))\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "    .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_bo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'and', 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_bo.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'and', 1)\n",
      "(u'\\uc18c\\uc2a4', 1)\n",
      "(u'is', 1)\n",
      "(u'interface', 1)\n",
      "(u'maintained', 1)\n",
      "(u'donated', 1)\n",
      "(u'\\ucef4\\ud4e8\\ud305', 1)\n",
      "(u'open', 1)\n",
      "(u'for', 1)\n",
      "(u'\\ud074\\ub7ec\\uc2a4\\ud130', 1)\n",
      "(u'with', 1)\n",
      "(u'wikipedia', 1)\n",
      "(u'provides', 1)\n",
      "(u'was', 1)\n",
      "(u'which', 1)\n",
      "(u'codebase', 1)\n",
      "(u'california', 1)\n",
      "(u'apache', 2)\n",
      "(u'spark', 3)\n",
      "(u'data', 1)\n",
      "(u'\\uc624\\ud508', 1)\n",
      "(u'university', 1)\n",
      "(u'programming', 1)\n",
      "(u'\\uc2a4\\ud30c\\ud06c\\ub294', 1)\n",
      "(u'the', 3)\n",
      "(u'originally', 1)\n",
      "(u'computing', 1)\n",
      "(u'developed', 1)\n",
      "(u'it', 1)\n",
      "(u'an', 2)\n"
     ]
    }
   ],
   "source": [
    "for x in word_count_bo.take(30):\n",
    "    print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-5: RDD from url\n",
    "\n",
    "* kaggle\n",
    "    * https://www.kaggle.com/kaggle/us-baby-names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Code/git/bb/jsl/pyds/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "_datadir='data'\n",
    "datapath = os.path.join(os.getcwd(),_datadir)\n",
    "print datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "\n",
    "_fname=os.path.join(datapath,'kddcup.data_10_percent.gz')\n",
    "if(not os.path.exists(_fname)):\n",
    "    print \"%s data does not exist! retrieving..\" % _fname\n",
    "    _f=urllib.urlretrieve(_url,_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Code/git/bb/jsl/pyds/data/kddcup.data_10_percent.gz\n"
     ]
    }
   ],
   "source": [
    "print _fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_data = sc.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터에 'normal.'이 포함된 건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n"
     ]
    }
   ],
   "source": [
    "_normal_data = _data.filter(lambda x: 'normal.' in x)\n",
    "print _normal_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.'], [u'0', u'tcp', u'http', u'SF', u'239', u'486', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'19', u'19', u'1.00', u'0.00', u'0.05', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.'], [u'0', u'tcp', u'http', u'SF', u'235', u'1337', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'29', u'29', u'1.00', u'0.00', u'0.03', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.'], [u'0', u'tcp', u'http', u'SF', u'219', u'1337', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'6', u'6', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'39', u'39', u'1.00', u'0.00', u'0.03', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.'], [u'0', u'tcp', u'http', u'SF', u'217', u'2032', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'6', u'6', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'49', u'49', u'1.00', u'0.00', u'0.02', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']]\n"
     ]
    }
   ],
   "source": [
    "_split_data=_data.map(lambda x: x.split(','))\n",
    "print _split_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rdd to sql, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv_data = _data.map(lambda l: l.split(\",\"))\n",
    "_row_data = _csv_data.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol_type=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "_df=sqlCtx.createDataFrame(_row_data)\n",
    "_df.registerTempTable(\"_my\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").groupBy(\"protocol_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").filter(_df.duration>1000).filter(_df.dst_bytes==0).groupBy(\"protocol_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions = sqlCtx.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM _my WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")\n",
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcp_interactions_out = tcp_interactions.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))\n",
    "for ti_out in tcp_interactions_out.collect():\n",
    "  print ti_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-8: Hello Statistics\n",
    "\n",
    "* mllib.stat.Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.841344746068543 \n",
      "pValue = 5.06089025353873E-6 \n",
      "Very strong presumption against null hypothesis: Sample follows theoretical distribution.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "parallelData = sc.parallelize([1.0, 2.0, 5.0, 4.0, 3.0, 3.3, 5.5])\n",
    "\n",
    "# run a KS test for the sample versus a standard normal distribution\n",
    "testResult = Statistics.kolmogorovSmirnovTest(parallelData, \"norm\", 0, 1)\n",
    "print(testResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, randn\n",
    " # Create a DataFrame with one int column and 10 rows.\n",
    "df = sqlCtx.range(0, 10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0|0.41371264720975787|  0.5888539012978773|\n",
      "|  1| 0.7311719281896606|  0.8645537008427937|\n",
      "|  2| 0.1982919638208397| 0.06157382353970104|\n",
      "|  3|0.12714181165849525|  0.3623040918178586|\n",
      "|  4| 0.7604318153406678|-0.49575204523675975|\n",
      "|  5|0.12030715258495939|  1.0854146699817222|\n",
      "|  6|0.12131363910425985| -0.5284523629183004|\n",
      "|  7|0.44292918521277047| -0.4798519469521663|\n",
      "|  8| 0.8898784253886249| -0.8820294772950535|\n",
      "|  9|0.03650707717266999| -2.1591956435415334|\n",
      "+---+-------------------+--------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|                id|\n",
      "+-------+------------------+\n",
      "|  count|                10|\n",
      "|   mean|               4.5|\n",
      "| stddev|3.0276503540974917|\n",
      "|    min|                 0|\n",
      "|    max|                 9|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", rand(seed=10).alias(\"uniform\"), randn(seed=27).alias(\"normal\")).show()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.308427454327\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "df = sqlCtx.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "print df.stat.corr('rand1', 'rand2')\n",
    "print df.stat.corr('id', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| name|   item|\n",
      "+-----+-------+\n",
      "|Alice|   milk|\n",
      "|  Bob|  bread|\n",
      "| Mike| butter|\n",
      "|Alice| apples|\n",
      "|  Bob|oranges|\n",
      "| Mike|   milk|\n",
      "|Alice|  bread|\n",
      "|  Bob| butter|\n",
      "| Mike| apples|\n",
      "|Alice|oranges|\n",
      "+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"Alice\", \"Bob\", \"Mike\"]\n",
    "items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\n",
    "df = sqlCtx.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = sqlCtx.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\n",
    "print df.show(10)\n",
    "freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
