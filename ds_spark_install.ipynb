{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "\n",
    "## 설치\n",
    "\n",
    "### prerequisites\n",
    "\n",
    "* jdk\n",
    "* python 2.7\n",
    "* scala\n",
    "\n",
    "### 설치 방법\n",
    "\n",
    "* 방법1: prebuilt distribution\n",
    "    * HDFS를 사용하려면, 하둡버전을 정해서 설치파일을 내려받고 압축을 풀어준다\n",
    "    (Hadoop없는 설치하면 아래 오류 참조)\n",
    "    * go to \"https://spark.apache.org/downloads.html\" and download spark-1.x.x-bin-hadoop2.x.tgz    \n",
    "    ```\n",
    "    $ tar xvfz spark-1.x.x-bin-hadoop2.x.tgz\n",
    "    ```\n",
    "\n",
    "* 방법2: source code를 github에서 내려받아서 컴파일\n",
    "\n",
    "* 방법3: apt로 설치하는 경우\n",
    "    * spark에 필요한 shell등을 설치하지 않는 듯하다. 따라서 위 tar를 다운로드 설치한다.\n",
    "    * when install from apt (available as of Ubuntu 16.04)\n",
    "        ```\n",
    "        sudo apt-get install scala\n",
    "        sudo apt-get install spark\n",
    "        ```\n",
    "\n",
    "### 설치 확인\n",
    "\n",
    "* 방법1로 spark-shell, pyspark를 실행하는데 문제가 없다. 다음 설정은 필요에 따라 해준다.\n",
    "\n",
    "* 경로설정을 해주지 않았으면,\n",
    "```\n",
    "$ cd ~/Downloads/spark-1.6.0-bin-hadoop2.6/bin\n",
    "$ spark-shell\n",
    "```\n",
    "\n",
    "* 경로설정을 하는 경우\n",
    "```\n",
    "export SPARK_HOME=$HOME/Downloads/spark-1.6.0-bin-hadoop2.6\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_91\"\r\n",
      "OpenJDK Runtime Environment (build 1.8.0_91-8u91-b14-3ubuntu1~16.04.1-b14)\r\n",
      "OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala code runner version 2.11.6 -- Copyright 2002-2013, LAMP/EPFL\r\n"
     ]
    }
   ],
   "source": [
    "!scala -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오류\n",
    "\n",
    "\n",
    "* SparkContext: Error initializing SparkContext\n",
    "    * /etc/hostname이 지금 설정과 다른 경우의 오류\n",
    "    * 변경하려면\n",
    "        ```\n",
    "        $ scutil --set HostName jsl.com\n",
    "        ```\n",
    "        \n",
    "    * 확인하려면   \n",
    "        ```\n",
    "        $ hostname\n",
    "        ```\n",
    "\n",
    "* cannot import name accumulators\n",
    "    * bin/pyspark 명령어를 보면  path 설정 나옴\n",
    "    * 일단 시작하면 path추가 안되는 모양???\n",
    "    * findspark를 설치??\n",
    "        * https://github.com/minrk/findspark/blob/master/findspark.py\n",
    "\n",
    "* 하둡 아닌 것을 내려받아 설치한 경우\n",
    "    * download and tar to ~/Downloads/spark-x.x.x/\n",
    "    * when running ~/Downloads/spark-1.6.1/spark-shell\n",
    "        * error Failed to find Spark assembly spark\n",
    "            * so need to install sbt\n",
    "            ```\n",
    "            download tar from http://www.scala-sbt.org/\n",
    "            tar -xvf sbt.tar\n",
    "            ```\n",
    "            \n",
    "            * this problem went away with the one prebuilt with hadoop\n",
    "\n",
    "\n",
    "* 실행 오류\n",
    "    * java.net.BindException: Address already in use Address already in use\n",
    "    ```\n",
    "    ps -aux | grep spark\n",
    "    kill -9 3370\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython 노트북에서 실행하려면\n",
    "\n",
    "* 방법1: 제일 쉬운 findspark 라이브러리를 설치\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "home=os.getenv(\"HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(os.path.join(home,\"Downloads/spark-1.6.0-bin-hadoop2.6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f11c539ef50>\n"
     ]
    }
   ],
   "source": [
    "print sc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.0) with graphFrames",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
