{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "## 상태\n",
    " \n",
    "* 2016-07-11월\n",
    "    * [ok] prebuilt 설치 - spark1.6 for hadoop2.0\n",
    "    * [ok] ipython notebook -> findspark를 사용 (아래 방법 1)\n",
    "    * [mac ok] ipython notebook kernel (아래 방법 2)\n",
    "    * [ok] IPYTON_OPTS (아래 방법 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치\n",
    "\n",
    "### prerequisites\n",
    "\n",
    "* jdk\n",
    "* scala\n",
    "* python 2.7\n",
    "* py4j (선택적, pip install py4j하면 PYTHONPATH에 py4j가 불필요)\n",
    "\n",
    "### 설치 방법\n",
    "\n",
    "* 방법1: prebuilt distribution\n",
    "    * HDFS를 사용하려면, 하둡버전을 정해서 설치파일을 내려받고 압축을 풀어준다\n",
    "    (Hadoop없는 설치하면 오류. 아래 참조)\n",
    "    * 다운로드하고 압축을 풀어준다 \"https://spark.apache.org/downloads.html\"   \n",
    "    ```\n",
    "    $ tar xvfz spark-1.x.x-bin-hadoop2.x.tgz\n",
    "    ```\n",
    "\n",
    "* 방법2: source code를 github에서 내려받아서 컴파일\n",
    "\n",
    "* 방법3: apt로 설치하는 경우\n",
    "    * spark 실행에 필요한 shell을 설치하지 않는 듯하다. 위 tar를 다운로드 설치하는 것으로 한다.\n",
    "    * when install from apt (available as of Ubuntu 16.04)\n",
    "        ```\n",
    "        sudo apt-get install scala\n",
    "        sudo apt-get install spark\n",
    "        ```\n",
    "\n",
    "### 설치 확인\n",
    "\n",
    "* 방법1로 spark-shell, pyspark를 실행하는데 문제가 없다.\n",
    "\n",
    "    * 경로설정을 해주지 않았으면,\n",
    "    ```\n",
    "    $ cd ~/Downloads/spark-1.6.0-bin-hadoop2.6/bin\n",
    "    $ spark-shell\n",
    "    ```\n",
    "\n",
    "    * 경로설정을 하는 경우\n",
    "    ```\n",
    "    export SPARK_HOME=$HOME/Downloads/spark-1.6.0-bin-hadoop2.6\n",
    "    export PATH=$SPARK_HOME/bin:$PATH\n",
    "    ```\n",
    "    \n",
    "    * 환경변수 설정 (bashrc)\n",
    "    ```\n",
    "    export SPARK_HOME=\"$HOME/Downloads/spark-1.6.0-bin-hadoop2.6\"\n",
    "    export PYSPARK_SUBMIT_ARGS=\"--packages graphframes:graphframes:0.1.0-spark1.6 --master spark://127.0.0.1:7077 pyspark-shell\"\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_91\"\r\n",
      "OpenJDK Runtime Environment (build 1.8.0_91-8u91-b14-3ubuntu1~16.04.1-b14)\r\n",
      "OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala code runner version 2.11.6 -- Copyright 2002-2013, LAMP/EPFL\r\n"
     ]
    }
   ],
   "source": [
    "!scala -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오류\n",
    "\n",
    "* SparkContext: Error initializing SparkContext\n",
    "    * /etc/hostname이 지금 설정과 다른 경우의 오류\n",
    "    * 변경하려면\n",
    "        ```\n",
    "        $ scutil --set HostName jsl.com\n",
    "        ```\n",
    "        \n",
    "    * 확인하려면   \n",
    "        ```\n",
    "        $ hostname\n",
    "        ```\n",
    "\n",
    "* cannot import name accumulators\n",
    "    * bin/pyspark 명령어를 보면  path 설정 나옴\n",
    "    * 일단 시작하면 path추가 안되는 모양???\n",
    "    * findspark를 설치??\n",
    "        * https://github.com/minrk/findspark/blob/master/findspark.py\n",
    "\n",
    "* 하둡 아닌 것을 내려받아 설치한 경우\n",
    "    * download and tar to ~/Downloads/spark-x.x.x/\n",
    "    * when running ~/Downloads/spark-1.6.1/spark-shell\n",
    "        * error Failed to find Spark assembly spark\n",
    "            * so need to install sbt\n",
    "            ```\n",
    "            download tar from http://www.scala-sbt.org/\n",
    "            tar -xvf sbt.tar\n",
    "            ```\n",
    "            \n",
    "            * this problem went away with the one prebuilt with hadoop\n",
    "\n",
    "\n",
    "* 실행 오류\n",
    "    * java.net.BindException: Address already in use Address already in use\n",
    "    ```\n",
    "    ps -aux | grep spark\n",
    "    kill -9 3370\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython shell을 사용하려면\n",
    "\n",
    "* pyspark는 python shell을 사용한다. ipython shell을 실행하려면:\n",
    "\n",
    "* 방법 1: 단순한 방식\n",
    "```\n",
    " $ export PYSPARK_DRIVER_PYTHON=ipython\n",
    " $ ./bin/pyspark 이러면 ipython를 shell로 사용하게 된다. \n",
    " 또는\n",
    " $ PYSPARK_PYTHON=python2.7 ./bin/pyspark\n",
    "```\n",
    "\n",
    "* 방법2: profile로 환경 설정하기\n",
    "    * ipython profile 생성. 아래 명령을 실행하면, 생성된 파일을 수정한다.\n",
    "        ```\n",
    "        ipython profile create pyspark\n",
    "        ```\n",
    "\n",
    "    * profile startup 파일 생성\n",
    "        ```\n",
    "        vim ~/.ipython/profile_pyspark/startup/00-pyspark-setup.py \n",
    "        ```\n",
    "\n",
    "        ```\n",
    "        import os\n",
    "        import sys\n",
    "\n",
    "        if 'SPARK_HOME' not in os.environ:\n",
    "            os.environ['SPARK_HOME'] = '/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6'\n",
    "        SPARK_HOME = os.environ.get('SPARK_HOME', None)\n",
    "        sys.path.insert(0, SPARK_HOME + \"/python\")\n",
    "\n",
    "        sys.path.insert(0, os.path.join(SPARK_HOME, \"python/lib/py4j-0.9-src.zip\")\n",
    "        execfile(os.path.join(SPARK_HOME, 'python/pyspark/shell.py'))\n",
    "        ```\n",
    "\n",
    "    * ipython shell 실행\n",
    "        ```\n",
    "        $ ipython console --profile=pyspark\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython 노트북에서 실행하려면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 방법1: 그냥 ipython notebook을 실행하고, 셀에 다음 내용을 입력한다.\n",
    "    ```\n",
    "    import os\n",
    "    import sys \n",
    "    os.environ[\"SPARK_HOME\"]='/Users/media/Downloads/spark-2.1.0-bin-hadoop2.7'\n",
    "    os.environ[\"PYLIB\"]=os.environ[\"SPARK_HOME\"]+'/python/lib'\n",
    "    sys.path.insert(0,os.environ[\"PYLIB\"]+'/py4j-0.10.4-src.zip')\n",
    "    sys.path.insert(0,os.environ[\"PYLIB\"]+'/pyspark.zip')\n",
    "    ```\n",
    "\n",
    "    * 그리고 sc를 만들어 주면 된다.\n",
    "    ```\n",
    "    import pyspark\n",
    "    conf=pyspark.SparkConf()\n",
    "    conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "    sc.version\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 방법2: findspark 라이브러리를 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "home=os.getenv(\"HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(os.path.join(home,\"Downloads/spark-1.6.0-bin-hadoop2.6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f11c539ef50>\n"
     ]
    }
   ],
   "source": [
    "print sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 방법3: ipython notebook kernel을 설정 (mac ok 20160902 fri)\n",
    "\n",
    "    * [안해도ok package사용] lib에 graphframes.jar 다운로드 저장. package 사용.\n",
    "    * [안해도ok] 설정변경하려면, 'conf/spark-env.sh'을 변경하고 \"PYSPARK_SUBMIT_ARGS\"에 추가 --master spark://127.0.0.1:7077\n",
    "    * [안해도ok] pip install py4j하면 PYTHONPATH에 py4j가 불필요\n",
    "    * kernel.json 작성 - 작성 후, ipython notebook을 실행하면 kernel에 pySpark가 올라옴.\n",
    "    ```        \n",
    "    $ mkdir -p ~/.ipython/kernels/pyspark\n",
    "    $ vim ~/.ipython/kernels/pyspark/kernel.json\n",
    "    ```\n",
    "\n",
    "    * kernel.json은 아래 내용을 수정해서 저장\n",
    "```\n",
    "{\n",
    "    \"display_name\": \"pySpark (Spark 1.6.0) with graphFrames\",\n",
    "    \"language\": \"python\",\n",
    "    \"argv\": [\n",
    "        \"/usr/bin/python2.7\",\n",
    "        \"-m\",\n",
    "        \"ipykernel\",\n",
    "        \"-f\",\n",
    "        \"{connection_file}\"\n",
    "    ],  \n",
    "    \"env\": {\n",
    "        \"SPARK_HOME\": \"/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6\",\n",
    "        \"PYTHONPATH\": \"/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6/python:/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip\",\n",
    "        \"PYTHONSTARTUP\": \"/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6/python/pyspark/shell.py\",\n",
    "        \"PYSPARK_SUBMIT_ARGS\": \"--packages graphframes:graphframes:0.1.0-spark1.6 --master spark://127.0.0.1:7077 pyspark-shell\",\n",
    "        \"SPARK_DRIVER_MEMORY\":\"10G\"\n",
    "     }   \n",
    "}\n",
    "```\n",
    "\n",
    "* 방법4\n",
    "    * 명령창에서 다음 명령어\n",
    "        ```\n",
    "        $ IPYTHON_OPTS=\"notebook\" ~/Downloads/spark-1.6.0-bin-hadoop2.6/bin/pyspark\n",
    "        ```\n",
    "        \n",
    "        또는\n",
    "        ```\n",
    "        $ IPYTHON_OPTS=\"notebook --pylab inline\" ~/Downloads/spark-1.6.0-bin-hadoop2.6/bin/pyspark\n",
    "        ```\n",
    "        \n",
    "    * 그리고 브라우저에서\n",
    "    ```\n",
    "    import pyspark\n",
    "    sc.stop() # Cannot run multiple SparkContexts at once\n",
    "    sc = pyspark.SparkContext(appName=\"myAppName\")\n",
    "    print sc\n",
    "    ```\n",
    "    * 다음 명령어는 오류 -> 7077 can not be reached\n",
    "    ```\n",
    "    MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"6G\" IPYTHON_OPTS=\"notebook\" ~/Downloads/spark-1.6.0-bin-hadoop2.6/bin/pyspark\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jar를 추가\n",
    "\n",
    "* Scala, Java보다 python에서 추가하는 것은 쉽지 않다.\n",
    "* Add jar to pyspark when using notebook\n",
    "http://stackoverflow.com/questions/31677345/add-jar-to-pyspark-when-using-notebook?rq=1\n",
    "\n",
    "### Spark package 검색\n",
    "\n",
    "```\n",
    "https://spark-packages.org/\n",
    "```\n",
    "\n",
    "### 방법\n",
    "\n",
    "* [ok] pyspark --jars\n",
    "    ```\n",
    "    $ bin/pyspark --py-files lib/graphframes.jar --jars lib/graphframes.jar \n",
    "    Welcome to\n",
    "          ____              __\n",
    "         / __/__  ___ _____/ /__\n",
    "        _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "       /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
    "          /_/\n",
    "\n",
    "    Using Python version 2.7.12 (default, Jul  1 2016 15:12:24)\n",
    "    SparkContext available as sc, HiveContext available as sqlContext.\n",
    "    >>> from graphframes import *\n",
    "    ```\n",
    "\n",
    "* packages \n",
    "    * [ok] spark-submit --packages\n",
    "        ```\n",
    "        ./bin/spark-submit src/ds_spark_dataframe.py \\\n",
    "           --packages graphframes:graphframes:0.1.0-spark1.6\n",
    "        ```\n",
    "\n",
    "    * [ok] spark-defaults.conf\n",
    "\n",
    "config file | 설명\n",
    "----------|----------\n",
    "spark-default.conf | system properties\n",
    "spark-env.conf | \n",
    "\n",
    "        * 예:\n",
    "        ```\n",
    "        spark.jars.packages=graphframes:graphframes:0.1.0-spark1.6\n",
    "        ```\n",
    "\n",
    "        * 또는 import jars to both driver AND executor. 와일드카드 '*'도 가능함\n",
    "        ```\n",
    "        spark.driver.extraClassPath /path/to/my.jar\n",
    "        spark.executor.extraClassPath /path/to/my.jar\n",
    "        ```\n",
    "        \n",
    "    * [ok] SparkConf (spark-defaults.conf에 packages 추가한 후)\n",
    "        ```\n",
    "        conf=pyspark.SparkConf()\n",
    "        conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "        conf.set(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/test.myCollection2\")\n",
    "        conf.set(\"spark.mongodb.output.uri\",\"mongodb://127.0.0.1/test.myCollection2\")\n",
    "        sc = pyspark.SparkContext(conf=conf)\n",
    "        ```\n",
    "\n",
    "* [not yet] addJar()\n",
    "    ```\n",
    "    SparkContext.addJar(...) method\n",
    "    SparkContext.addFile(...) method\n",
    "    ```\n",
    "    \n",
    "* [not yet] loading jars\n",
    "    ```\n",
    "    from py4j.java_gateway import java_import\n",
    "    ```\n",
    "\n",
    "* [nok] os.environ - submit_args\n",
    "    * IPyton Notebook에서 잘 안되는 듯 class-not-found errors. python kernel이 일단 실행되고 나면, 동적으로 추가하는 것이 안되는 듯. (subprocess.Popen은 env값을 가져올 수 있지 않나??)\n",
    "    ```\n",
    "    import os\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.2.0-spark2.0-s_2.11'\n",
    "    ```\n",
    "* SPARK_CLASSPATH - spark-env.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.0) with graphFrames",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
